=======================================
######### AWS STORAGE SERVICE #########

## AWS S3
- It is Simple Storage service 
- It is a scalable, high-speed, low-cost web-based service designed for online backup and archiving of data and application programs. 
- It allows to upload, store, and download any type of files up to 5 TB in size

## AWS Elastic File System
- Fully managed, scalable, and sharable storage among thousands of EC2 instances.

## Glacier
- Secure, durable, and extremely low-cost solutions for backup and archiving.

## Storage Gateway
- Seamlessly connect on-premise applications or services with the AWS cloud storage.

## 
=======================================
##### SIMPLE STORAGE SERVICE(S3) ######

> S3 is one of the first services that has been produced by aws.
> S3 provides developers and IT teams with secure, durable, highly scalable object storage.
- It is easy to use with a simple web services interface to store and retrieve any amount of data from anywhere on the web.

## What is S3?
> S3 is a safe place to store the files.
- It is Object-based storage, i.e., you can store the images, word files, pdf files, etc.
- The files which are stored in S3 can be from 0 Bytes to 5 TB (i.e unlimited storage)

> Files are stored in Bucket. A bucket is like a folder available in S3 that stores the files.

> S3 is a universal namespace, i.e., the names must be unique globally. Bucket contains a DNS address. Therefore, the bucket must contain a unique name to generate a unique DNS address.
If you create a bucket, URL look like:
https://s3-eu-west-1.amazonaws.com/acloudguru 


> If you upload a file to S3 bucket, then you will receive an HTTP 200 code means that the uploading of a file is successful.

## ADVANTAGES OF AMAZON S3

1. Create Buckets: Firstly, we create a bucket and provide a name to the bucket. Buckets are the containers in S3 that stores the data. Buckets must have a unique name to generate a unique DNS address.

2. Storing data in buckets: Bucket can be used to store an infinite amount of data. You can upload the files as much you want into an Amazon S3 bucket, i.e., there is no maximum limit to store the files. Each object can contain upto 5 TB of data. Each object can be stored and retrieved by using a unique developer assigned-key.

3. Download data: You can also download your data from a bucket and can also give permission to others to download the same data. You can download the data at any time whenever you want.

4. Permissions: 
- You can also grant or deny access to others who want to download or upload the data from your Amazon S3 bucket. Authentication mechanism keeps the data secure from unauthorized access.

5. Standard interfaces: S3 is used with the standard interfaces REST and SOAP interfaces which are designed in such a way that they can work with any development toolkit.

6. Security: Amazon S3 offers security features by protecting unauthorized users from accessing your data.

### S3 is a simple key-value store
S3 is object-based. Objects consist of the following:

1. Key: It is simply the name of the object. For example, hello.txt, spreadsheet.xlsx, etc. You can use the key to retrieve the object.

2. Value: It is simply the data which is made up of a sequence of bytes. It is actually a data inside the file.

3. Version ID: Version ID uniquely identifies the object. It is a string generated by S3 when you add an object to the S3 bucket.

4. Metadata: It is the data about data that you are storing. A set of a name-value pair with which you can store the information regarding an object. Metadata can be assigned to the objects in Amazon S3 bucket.

5. Subresources: Subresource mechanism is used to store object-specific information.

6. Access control information: You can put the permissions individually on your files.

===================================
### Block versus object storage ###

## BLOCK STORAGE
> Block storage is SUITABLE FOR TRANSACTIONAL DATABASES, random read/write loads, and structured database storage

> Block storage divides the data to be stored in evenly sized blocks (data chunks), for instance, a file can be split into evenly sized blocks before it is stored 

> Data blocks stored in block storage would NOT CONTAIN METADATA (date created, date modified, content type...etc)

> Block storage only keeps the address(index) where the data blocks are stored

## OBJECT STORAGE
> It stores the files as a whole and does not divide them

> In object storage, an object is:
- A file/data itself
- Its metadata (data created, modified, security attributes, content type.etc)
- Object's global unique ID

> The Object Global Unique ID, Is a unique identifier for the object (can be the object name itself), and it must be unique such that it can be retrieved disregarding where its physical storage location is

> Examples for Objects,
- Photos, Videos, Music, Static Web Content, Data backups (snapshots), Archival images
- ANY DATA THAT CAN BE INCREMENTALY UPDATED and will not have a lot of writes/updates

> OBJECT STORAGE CAN GUARANTEE HIGH AVAILABILITY AND DURABILITY 
- data copies are stored on multiple, geographical distributed location 
- object storage cannot be mounted on a drive or directory, directly to an EC2 instance 
- object storage is a perfect solution for data growth storage problem
=======================================
###### DATA CONSISTENCY MODEL ####### v2
> Data consistency is relevant when we are considering copies of the same data object store over distributed system

> when the copies of data(stored on different systems) are read at the same time from different nodes, consistency level referred to how consistent will they be returned data (from the read), is it going to be 100% the same or slightly different?

####### STRONG CONSISTENCY ####
> Sometime also referred as immediate consistency

> Read from different data stores for listen data returns the exact same information
> Any update made to the date object in any storage node will be propagated and updated on all other storage nodes before the data is made available for read by clients
> Requires a locking mechanism to block read until the data is propagated and updated on all nodes
> Is good for transactional database and real time system with consistent writes
> Has limited scalability and reduced availability

#### EVENTUAL CONSISTENCY #####
> Read from different data stores for the same data result different returns
> There is no locking mechanism, if data is updated to an object, an immediate read from different note will not return the same data
- with time and as the changes/updates get propagated and updated on all other storage note, the reads will be will eventually consistent

> Eventually consistent can virtually provide unlimited 
- scalability 
- availability 
- data durability

#### S3 CONSISTENCY LEVEL
> S3 Provides:
- READ-AFTER-WRITE (IMMEDIATE OR STRONG) CONSISTENCY of PUTS of new objects (new object loads to S3)

> EVENTUAL CONSISTENCY FOR OVERWRITE data PUTS and DELETES (for Changes/updates to existing Objects in S3)

> Updates to an Object are atomic, i.e when you run a PUT for an object then you read (GET) that object, you will either get the updated object or the old one (before the update), you will NEVER GET PARTIAL OR CORRUPT DATA
=======================================
########### S3 ############## v3

> S3 is a storage for the internet. It has a simple web service interface for simple storing and retrieval of any amount of data, anytime, from anywhere on the internet 
> it is object based storage and not a block storage
> S3 has a DISTRIBUTED DATA-STORE ARCHITECTURE where object are redundant stored in multiple location

#### S3 BUCKETS ####
> Data is stored in buckets 
- It can be viewed as a flat container for objects that DOES NOT PROVIDE HIERARCHICAL OF OBJECT
- We can use object key(name) to mimic folders in a bucket when using the AWS console

> You can store unlimited object in your bucket but an object cannot exceed 5TB
> CAN CREATE FOLDER IN YOUR BUCKET (available through console only)
> You cannot create nested buckets (a bucket inside another)
> Bucket ownership is not transferable
> It is recommended to access S3 through SDK or API (console internally use API too)
> An S3 bucket is region specific
> You can have upto 100 buckets (soft limit) per account

> An S3 bucket has properties including 
- access permission 
- versioning status 
- storage classes

#### S3 BUCKET NAMING RULE #####
> S3 bucket names(keys) are globally unique across all region 
- bucket name cannot be changed after they created 
- if a bucket is deleted its name become available again to you or another account to use 
- bucket names must be at least 3 or no more than 63 character long 

> bucket names are part of the URL is to access bucket

> Bucket name must be a series of one or more label(mymain.bucket)
- Adjacent label are separated by a single period 
- bucket name can contain lowercase letter, numbers and hyphen
- each label must start and end with a lowercase letter or a number

> Bucket name must not be formatted as an IP address
=======================================
###### S3 buckets sub resources ####### v4
> Amazon S3 supports various option for you to configure your bucket 
- Amazon S3 support subject sources for you to store and manage the bucket configuration information 
- using the Amazon S3 API you can create and manage these resources 
- you can also use the console or AWS SDK

> By default a bucket its object and related sab resources are all private that is by default only the owner has access to the bucket and storage object

> Sub-resources for S3 bucket includes:
- LIFECYCLE : to decide and object lifecycle management
- WEBSITE : to hold configuration related to static website hosted in it
- VERSIONING: keep object version as it change
- Access control list(ACLs) 
- bucket policies

## S3 bucket DNS names
> The name is simply two parts 
- bucket region endpoints/bucketname
EX: https://s3-eu-west-1.amazonaws.com/cloudbucket1

> S3 buckets region
> For better performance low latency help to minimise cost, create the S3 bucket closer to your client DC are source of data to be stored
=======================================
############ S3 objects ############### v5

> An object size stored in S3 bucket can be 0 byte upto 5tb
> Each object is stored and retrieved by unique key
> An object is a 330 uniquely identified and address through
> Service centre point bucketname object key optional ITI object version
> Object store in S3 bucket in a region will never leave that region
-  unless you specify move them to another reason or enable cross region replication
> S3 PROVIDE HIGH DATA DURABILITY, object are redundantly stored on multiple devices across multiple facility in Amazon S3 region where the bucket exist

## S3 object sub resources
1. Access Control List :
- to define guarantee and permission granted to the object

2. Torrent 
- used by S3 in case bittorrent tries to download the object
=======================================
#### S3 RESOURCES AND SUB RESOURCES ### v6

> Bucket and object are primary S3 resources 
- each has its own sub resources

> Buckets sub-resources are lifecycle 
- Lifecycle, website, versioning, ACL and policies, Cross origin Resource sharing(CORS) and logging(bucket access log)

> Object sub-resources are 
- ACL and restore(restoring an archive)

> Operations on S3 are either bucket level operation or object label operation

## S3 RESOURCE OWNER

> By default all Amazon S3 resources are private 
- only a resource owner can access the resources

> resource owner refer to the account that create the resources.
- for example account that you use to create buckets and objects owns those resources

> If you create an AWS IAM user in your AWS account you are the parent owner 
- if the IAM USER UPLOAD AN OBJECT, THE PARENT ACCOUNT OWNS THE OBJECT

> A bucket owner can grant cross-account permissions to another AWS account(or user in another account) upload object. 
- AWS account that upload the object own them

> The bucket owner does not have permission on the object that other account own with the following exception
- The bucket owner pays the bill. He can deny access to any object regardless of who owns them
- The bucket owner can delete any object in the bucket, regardless of who owns them.
- The bucket owner  can archive any objects or restore archive object regardless of who owns them

## Managing access to resources(access policy option)
> managing access refers to granting other (AWS account and user) permission to perform the resources operations by writing an access policy

> You can grant S3 bucket/object permission to 
- individual user 
- AWS accounts 
- make the resource public, grant permission to everyone (also referred as anonymous access) 
- or to all authenticated user (user with AWS credentials)
=======================================
######### S3 access policy ############ v7

> Access policy describe who has access to what. 
- You can associate and access policy with a resource (bucket/object) OR user

>  Amazon S3 access policy are as follow: 
1. Resource based Policy
2. User Access Policy 

#### 1. S3 RESOURCE BASED POLICY ####
> bucket policy and access control list are resource-based because you attached them to your Amazon S3 resources

## 1a. ACL-BASED ACCESS POLICY (buckets and objects ACLs)
- each BUCKET AND OBJECT CAN HAVE AN ACL associated with it

> An ACL is a list of grants i.e. identifying guarantee(who) and permission granted(what)
> ACL GRANT BASIC READ/WRITE PERMISSION to other AWS account

##1b. BUCKET POLICY 
> FOR BUCKET AND OBJECT, we can add a bucket policy to grant other AWS account or IAM user permission 
- any object permission apply only to the object that the bucket owner creates 
- bucket policy supplement and in many cases replace ACL based access policy

###### 2. S3 USER ACCESS POLICY #######

> You can use AWS IAM to manage access to your Amazon S3 resources

> Using IAM, you can create IAM user, group and role in your account and attach access policy to them granting them access to AWS resources including Amazon S3

> You cannot grant anonymous permissions in any IAM user policy because the policy is attached to a user

> User policy can grant permission to a bucket and object in it

## How does S3 evaluate (allow/deny) a request for an S3 resource operation (Bucket or Object Operation)

> S3 evaluates:

User Context:
> S3 checks whether the user attached policies allows the request, so basically whether the parent account allows the operation to the IAM user or not
- If the user is the root account, this validation is skipped

Bucket Context:
- S3 validates whether the bucket owner (can be the same parent account or another AWS account) has allowed the requested operation to this user

- Bucket policy, Bucket ACL, and Object (if Object operation) are All checked

Notes:
> If the parent AWS account owns the resource (bucket or object), it can grant resource permissions to its IAM user by using either the user policy or the resource policy.

> If the bucket and object owner is the same,
- Access to the object can be granted in the bucket policy, which is evaluated at the bucket context.

> If the owners are different,
- The object owners must use an object ACL to grant permissions.

> If the AWS account that owns the object is also the parent account to which the IAM user belongs, it can configure object permissions in a user policy, which is evaluated at the user context.
=======================================
####### Bucket ACL permissions ######## v8

> Amazon S3 access control list(ACL) enable you to manage access to bucket and object

> Each bucket and object has an ACL attached to it as a sub-resources
- It DEFINE WHICH ACCOUNT(GRANTEE) OR PRE-DEFINED S3 GROUPS ARE GRANTED ACCESS AND THE TYPE OF ACCESS
- You cannot provide permissions to the individual IAM user
- Grantee accounts (cross account access) can then delegate access provided by other account to their individual user

> When you create a bucket or an object, Amazon S3 create a default ACL that grants the resources owner full control over the resources

### AWS S3 predefine groups
> Amazon S3 has a set of predefined groups that are

1. Authenticated user group 
- this group represent all AWS account. 
- Access permission to this group allow any AWS account to access the resources. 
- However all request must be signed(authenticated) 
- when you grant access to the authenticated user group any AWS authenticated user in the world can access our resources

2. All user group
> access permission to this group allows anyone in the world access to the resources 
- the request can be signed(authenticated) or unsigned(anonymous)
- unsigned request omit the authentication header in the request 
- AWS highly recommend that you never grant the 'all user group' WRITE, WRITE_ACP OR FULL_CONTROL permission

3. Log delivery group
- Write permission on a bucket enable this group to write server access logs
=======================================
########## v9

## When to use Object ACLs To manage Object Permissions

> An OBJECT ACL IS THE ONLY WAY TO MANAGE ACCESS TO OBJECTS NOT OWNED BY THE BUCKET OWNER
- This can happen when the bucket owner authorizes another account to upload objects to the bucket, but still the Bucket owner will not own the objects
- However, the bucket owner can deny, through the bucket policy, access to the object, even if the object ACL allows it
- The bucket owner can delete any object regardless whether it owns it or not

> Object ACLs is require to managing granular permissions at the object level because
- Bucket policies are limited to 20KB in size and won't be practical to be used for this

> Object ACLs are limited to 100 granted permissions per ACL


## When to use Bucket ACLS
> The only recommended use case for the bucket ACL is to grant write permission to the Amazon S3 Log Delivery group to write access log objects to your bucket

- If you want Amazon S3 to deliver access logs to your bucket, you will need to grant write permission on the bucket to the Log Delivery group.

> You can use bucket and Object ACLs to grant cross-account permissions to other accounts, but ACLS support only a finite set of permission, these don't include all Amazon S3 permissions.

######## ACL LIMITATIONS ###########
> You can use ACLs to grant basic read/write permissions to other AWS accounts.

> There are limits to managing permissions using ACLS.
- For example, you can grant permissions only to other AWS accounts; you cannot grant permissions to users in your account.

> You cannot grant conditional permissions, nor can you explicitly deny permissions. ACLs are suitable for specific scenarios.

=======================================
############ v10

## When to use Bucket Policies

> If an AWS account that owns a bucket wants to grant permission to users in its account, it can use either a bucket policy or a user policy.

> If the AWS account that owns the object also owns the bucket, then it can write a bucket policy to manage the object permissions.

> If you want to manage cross-account permissions for all Amazon S3 permissions 
- For Cross account permissions, when you use to provide another account full S3 permissions, you can only do that via Bucket policies

## When to use User Policies
> User policies support granting permission for all Amazon S3 operations

> The user policies are for managing permissions for users in your account.

> If the AWS account that owns the object wants to grant permission to a user in its account, it can use a user policy.

> An IAM uset must have permissions from the parent account to which it belongs, and from the AWS account that owns the resource the user wants to access. 
The permissions can be granted as follows:
- Permission from the parent account : The parent account can grant permissions to its user by attaching a user policy.
- Permission from the resource owner : The resource owner can grant permission to either the IAM user (using a bucket policy) or the parent account (using a bucket policy, bucket ACL, or object ACL).

## PERMISSIONS DELEGATION
>> If an AWS acount owns a resource, it can grant those permissions to another AWS account.
- account can then delegate those permissions, or a subset of them, to users in the account. This is referred to as permission delegation.
- An account that receives permissions from another account cannot delegate permission cross-account to another AWS account

=======================================
######## S3 BUCKET VERSIONING ######### v11

> Bucket versioning is a S3 bucket sub-resource, used to protect against accidental object/data deletion or overwrites

> Versioning can also be used for data retention and archive
> once you enable Bucket versioning on a bucket, it can not be disabled, however, it can be suspended

> When enabled, bucket versioning will protect existing and new objects, and maintains their versions as they are updated (edited, written to...)
- Updating objects refers to PUT, POST, COPY, DELETE actions on objects

> By default an HTTP GET retrieves the most recent version
> Only S3 bucket owner can permanently delete objects once versioning is enabled

###### S3 Delete markers #####
> When versioning is enabled, and you try to delete an object, a DELETE marker is placed on the object
- You can still view the object and the delete marker

> If you reconsider deleting the object, you can delete the "DELETE Marker", and the object will be available again

> You will be charged for all S3 storage costs for all Object versions stored
- You can use versioning with S3 lifecycle policies to delete older versions, OR, you can move them to a cheaper S3 storage (or Glacier)

> Bucket versioning states:
- Enabled
- Suspended
- Un-versioned

> Versioning applies to all objects in a bucket and not partially applied
> Objects existing before enabling versioning will have a version ID or "NULL"
> If you have a bucket that is already versioned, then you suspend versioning, existing objects and their versions remain as is
- However they will not be updated/versioned further with future updates,

## Suspended Bucket Versioning
> While the bucket versioning is suspended:
- New objects (uploaded after suspension), they will have a version ID "null"
• If the same object key (name) is used to store another object, It will override the existing one (complete object overwrite)

- Objects with versions that existed before the versioning suspenion; 
- a new object with the same object key (name) will replace the curreht/latést version, but will have an ID "null", and becomes the new current version
- a new object with the same key (name) will overwrite the one with the "null" ID

> An object deletion in a suspended versloning bucket, will only delete the object(s) with ID "null"

> The bucket owner canpermanently delete any versions

## Bucket Versioning- MFA Delete
> Multi Factor Authentication (MFA) Delete js a versioning capability that adds another level of security in case your account is compromised

> This adds another layer of security for the following:
- Changing your bucket's versioning state
- Permanently deleting an object version

> MFA Delete requires:
- Your security credentials
- The code displayed on an approved physical or SW-based authentication device
> This provides maximum protection to your objects

[9:35 AM, 1/23/2020] Manish Agrawal: Multipart Upload
Is used to upload an object (objects) in parts
Parts are uploaded independently and in parallel, in any order
It is recommended for objects sizes of 100MB or larger
However, you can use for object sizes starting from 5MB up to 5TB
You must use it for Objects larger than 5GB
This is done through the S3 Multipart upload API
[9:35 AM, 1/23/2020] Manish Agrawal: Copying S3 Objects
The copy operation creates a copy of an object that is already stored in Amazon S3
You can create a copy of your object up to 5 GB in size in a single atomic operation
However, to copy an object greater than 5 GB, you must use the mu
art upload API
You will incur charges if the copy is to a destination that is another AWS region
[9:36 AM, 1/23/2020] Manish Agrawal: Copying S3 Objects
Can be done using AWS SDKS or REST API
ES-315
Use the copy operation to:
Generate additional copies of the objects
Renaming objects (copy to a new name)
Changing the copy's storage class or encrypt it at rest (these can be done via AWS
Console now with the new UI)
$3- and Ss3-RRS
Move objects across AWS locations/region
Change object metadata
Once you upload an objest to S3, you can NOT change some of its metadata, this is
where the copy comes in handy
Pedire col
[9:40 AM, 1/23/2020] Manish Agrawal: Successful Upload Acknowledgement
When you successfully upload an object (a file) to S3, S3 returns a HTTP 200 OK message
for a successful PUT Operation
If uploading an Object and requesting SSE using Customer Provided Keys,and if the PUTPoT
operation is successful
Amazon S3 then returns the HTTP 200K, the encryption algorithm, and MD5 of the
encryption key you specified when uploading the object.
=======================================
[2:39 PM, 1/23/2020] Manish Agrawal: S3 STANDARD Storage Class
> Provides for 99.99% availability
> 11 9's Data Durability ( 99.999999999%)
> Data encrypted in-transit and at rest in S3
> Designed to sustain the concurrent loss of data in two facilities
[2:44 PM, 1/23/2020] Manish Agrawal: ## S3 Infrequent Access Class (S3 – IA)

> For long-lived, less frequently accessed data

> Designed to sustain the concurrent loss
of data in two facilities

> Suitable for backups and older data (old pictures, files)

> 99.9% Availability
> 11  9's data durability
[2:48 PM, 1/23/2020] Manish Agrawal: S3 Infrequent Access Class (S3 – IA)
> Quick access and high performance like S3 Standard

> Data encrypted in-transit and at rest in S3 buckets

> Data must be kept for at least 30 days in this class

> Suitable for objects greater than 128 KB (Not less than that)
[3:14 PM, 1/23/2020] Manish Agrawal: ##S3 Reduced Redundancy Storage ( S3 – RRS)

> Designed to sustain the data loss in one facility

> 99.99% availability
> 99.99% data durability

> Use for non-critical, reproducible data

> Lower level of redundancy
> Use for images and save their thumbnails in S3 standard, transcoded media
[3:22 PM, 1/23/2020] Manish Agrawal: S3 Reduced Redundancy Storage ( S3 - RRS)
> If an S3 RRS object is lost, AWS will return HTTP 405 error when the object is requested for read
> S3 can send a notification in case an object is lost
[3:34 PM, 1/23/2020] Manish Agrawal: ##### Glacier – Archiving Storage 
> an archiving storage for infrequently accessed data
> Achieved objects are not available for real time access, you need to submit a retrieval
request, restore the data first before you can read it (can take few hours),
- Requested archived data will be copied to RRS
- After data is retrieved, you have up to 24 hours to download it

> You can NOT specify Glacier as the Storage Class at the time you create an object

> 11 9's data durability

> No SLA
[4:32 PM, 1/23/2020] Manish Agrawal: Glacier - Archiving Storage
> Is designed to sustain data loss in two facilities
> You need to keep your data for a minimum of 90 days (minimum charge)

> Glacier automatically encrypts data at rest using AES-256-bit symmetric keys and supports
secure transfer of your data (In-transit) over Secure Sockets Layer (SSL)

> Glacier may NOT be available in all AWS regions, check for availability

> Glacier objects are visible and available only through AWS S3 not through AWS Glacier
[4:35 PM, 1/23/2020] Manish Agrawal: Glacier - Archiving Storage
> Glacier redundantly stores your data to multiple facilities, on multiple devices within each facility
synchronously

> Glacier awaits until the multiple facility upload is completed successfully before confirmting a
successful upload to the user

> Glacier does NOT archive object metadata, you need to maintain a client-side database to
maintain this information about the objects 
- Basically which archives hold which objects 

Suitable for:
-Archive
- Media asset archiving,
Health care information archiving,
- Regulatory and compliance archiving,
- A replacement for magnetic tapes
[4:41 PM, 1/23/2020] Manish Agrawal: Glacier - Archiving Storage
> You can upload archives to Glacier of a size:
- As small as 1 Byte
- As large as 40 TB

> Glacier file archives of size 1bytes to 4GB can be done in one shot

> Glacier file archives from 100MB up to 40TB, can be uploaded to Glacier using multi-part
upload


> Uploading archives is Synchronous

> Downloading archives is Asynchronous

> The contents of an archive, once uploaded, can not be updated

## Glacier - Archiving Storage
> You can upload data to Glacier directly from CLI, SDKS, or through APIS

- You can not do that through the AWS Console

> AWS Import/Export Snowball comes in handy when you need to upload large amount of
data to Glacier
usually takes 5-7 days

## Glacier - Archiving Storage
> When transitioning objects to Glacier from other classes (via lifecycle rules),
- Glacier adds 32KB overheads (indexing and archive metadata) to the object
-• The number is 40KB overheads if you are using S3 to upload to Glacier
- You pay for these overheads in addition to your actual data
- The 32KB of overhead associated with each archive accounts för system
indexing/metadata overhead.


> It is recommended to group a large number of small objects, into a smaller number of larger
objects to reduce these overhead charges.
- If you need to access individual archived files, make sure that you group them in a way
that allow this (compression techniques that allows the access to individual files)
• This will enable you to avoid downloading the entire archive to access individual
files

## Glacier - Archiving Storage
> Maintaining Client-Side Archive Metadata
- Amazon Glacier does not support any additional metadata for the archives, except the
archive description (optional)
• You might maintain metadata about the archives on the client-side. The metadata
can include archive name and some other meaningful information about the
archive.
> After you upload an archive, you cannot update its content or its description.
- The only way you can update the archive content or its description is by deleting the
archive and uploading another archive.
- Note that each time you upload an archive, Amazon Glacier returns to you a unigue
archive ID.