=======================================
######### AWS STORAGE SERVICE #########

## AWS S3
- It is Simple Storage service 
- It is a scalable, high-speed, low-cost web-based service designed for online backup and archiving of data and application programs. 
- It allows to upload, store, and download any type of files up to 5 TB in size

## AWS Elastic File System
- Fully managed, scalable, and sharable storage among thousands of EC2 instances.

## Glacier
- Secure, durable, and extremely low-cost solutions for backup and archiving.

## Storage Gateway
- Seamlessly connect on-premise applications or services with the AWS cloud storage.

## 
=======================================
> three different metrics: availability, durability and reliability. 
## What durability is AWS?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## What durability is AWS?
> Durability is used to measure the likelihood of data loss. ... 
> AWS measures durability as a percentage. 
- For example, the S3 Standard Tier is designed for 99.999999999% durability. 
- This means that if you store 100 billion objects in S3, you will lose one object at most.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## What is reliability in AWS?

> The reliability pillar includes the ABILITY OF A SYSTEM TO RECOVER FROM INFRASTRUCTURE OR SERVICE DISRUPTIONS, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## What is Resiliency in AWS?

> The ability for a system to recover from a failure induced by load, attacks, and failures.
=======================================
##### SIMPLE STORAGE SERVICE(S3) ######

> S3 is one of the first services that has been produced by aws.
> S3 provides developers and IT teams with secure, durable, highly scalable object storage.
- It is easy to use with a simple web services interface to store and retrieve any amount of data from anywhere on the web.

## What is S3?
> S3 is a safe place to store the files.
- It is Object-based storage, i.e., you can store the images, word files, pdf files, etc.
- The files which are stored in S3 can be from 0 Bytes to 5 TB (i.e unlimited storage)

> Files are stored in Bucket. A bucket is like a folder available in S3 that stores the files.

> S3 is a universal namespace, i.e., the names must be unique globally. Bucket contains a DNS address. Therefore, the bucket must contain a unique name to generate a unique DNS address.
If you create a bucket, URL look like:
https://s3-eu-west-1.amazonaws.com/acloudguru 


> If you upload a file to S3 bucket, then you will receive an HTTP 200 code means that the uploading of a file is successful.

## ADVANTAGES OF AMAZON S3

1. Create Buckets: Firstly, we create a bucket and provide a name to the bucket. Buckets are the containers in S3 that stores the data. Buckets must have a unique name to generate a unique DNS address.

2. Storing data in buckets: Bucket can be used to store an infinite amount of data. You can upload the files as much you want into an Amazon S3 bucket, i.e., there is no maximum limit to store the files. Each object can contain upto 5 TB of data. Each object can be stored and retrieved by using a unique developer assigned-key.

3. Download data: You can also download your data from a bucket and can also give permission to others to download the same data. You can download the data at any time whenever you want.

4. Permissions: 
- You can also grant or deny access to others who want to download or upload the data from your Amazon S3 bucket. Authentication mechanism keeps the data secure from unauthorized access.

5. Standard interfaces: S3 is used with the standard interfaces REST and SOAP interfaces which are designed in such a way that they can work with any development toolkit.

6. Security: Amazon S3 offers security features by protecting unauthorized users from accessing your data.

### S3 is a simple key-value store
S3 is object-based. Objects consist of the following:

1. Key: It is simply the name of the object. For example, hello.txt, spreadsheet.xlsx, etc. You can use the key to retrieve the object.

2. Value: It is simply the data which is made up of a sequence of bytes. It is actually a data inside the file.

3. Version ID: Version ID uniquely identifies the object. It is a string generated by S3 when you add an object to the S3 bucket.

4. Metadata: It is the data about data that you are storing. A set of a name-value pair with which you can store the information regarding an object. Metadata can be assigned to the objects in Amazon S3 bucket.

5. Subresources: Subresource mechanism is used to store object-specific information.

6. Access control information: You can put the permissions individually on your files.

===================================
### Block versus object storage ###

## BLOCK STORAGE
> Block storage is SUITABLE FOR TRANSACTIONAL DATABASES, random read/write loads, and structured database storage

> Block storage divides the data to be stored in evenly sized blocks (data chunks), for instance, a file can be split into evenly sized blocks before it is stored 

> Data blocks stored in block storage would NOT CONTAIN METADATA (date created, date modified, content type...etc)

> Block storage only keeps the address(index) where the data blocks are stored

## OBJECT STORAGE
> It stores the files as a whole and does not divide them

> In object storage, an object is:
- A file/data itself
- Its metadata (data created, modified, security attributes, content type.etc)
- Object's global unique ID

> The Object Global Unique ID, Is a unique identifier for the object (can be the object name itself), and it must be unique such that it can be retrieved disregarding where its physical storage location is

> Examples for Objects,
- Photos, Videos, Music, Static Web Content, Data backups (snapshots), Archival images
- ANY DATA THAT CAN BE INCREMENTALY UPDATED and will not have a lot of writes/updates

> OBJECT STORAGE CAN GUARANTEE HIGH AVAILABILITY AND DURABILITY 
- data copies are stored on multiple, geographical distributed location 
- object storage cannot be mounted on a drive or directory, directly to an EC2 instance 
- object storage is a perfect solution for data growth storage problem
=======================================
###### DATA CONSISTENCY MODEL ####### v2
> Data consistency is relevant when we are considering copies of the same data object store over distributed system

> when the copies of data(stored on different systems) are read at the same time from different nodes, consistency level referred to how consistent will they be returned data (from the read), is it going to be 100% the same or slightly different?

####### STRONG CONSISTENCY ####
> Sometime also referred as immediate consistency

> Read from different data stores for listen data returns the exact same information
> Any update made to the date object in any storage node will be propagated and updated on all other storage nodes before the data is made available for read by clients
> Requires a locking mechanism to block read until the data is propagated and updated on all nodes
> Is good for transactional database and real time system with consistent writes
> Has limited scalability and reduced availability

#### EVENTUAL CONSISTENCY #####
> Read from different data stores for the same data result different returns
> There is no locking mechanism, if data is updated to an object, an immediate read from different note will not return the same data
- with time and as the changes/updates get propagated and updated on all other storage note, the reads will be will eventually consistent

> Eventually consistent can virtually provide unlimited 
- scalability 
- availability 
- data durability

#### S3 CONSISTENCY LEVEL
> S3 Provides:
- READ-AFTER-WRITE (IMMEDIATE OR STRONG) CONSISTENCY of PUTS of new objects (new object loads to S3)

> EVENTUAL CONSISTENCY FOR OVERWRITE data (for Changes/updates to existing Objects in S3)
- It can be PUTS and DELETES operation 

> Updates to an Object are atomic, i.e when you run a PUT for an object then you read (GET) that object, you will either get the updated object or the old one (before the update), you will NEVER GET PARTIAL OR CORRUPT DATA
=======================================
########### S3 ############## v3

> S3 is a storage for the internet. It has a simple web service interface for simple storing and retrieval of any amount of data, anytime, from anywhere on the internet 
> it is object based storage and not a block storage
> S3 has a DISTRIBUTED DATA-STORE ARCHITECTURE where object are redundant stored in multiple location

#### S3 BUCKETS ####
> Data is stored in buckets 
- It can be viewed as a flat container for objects that DOES NOT PROVIDE HIERARCHICAL OF OBJECT
- We can use object key(name) to mimic folders in a bucket when using the AWS console

> You can store unlimited object in your bucket but an object cannot exceed 5TB
> CAN CREATE FOLDER IN YOUR BUCKET (available through console only)
> You cannot create nested buckets (a bucket inside another)
> Bucket ownership is not transferable
> It is recommended to access S3 through SDK or API (console internally use API too)
> An S3 bucket is region specific
> You can have upto 100 buckets (soft limit) per account

> An S3 bucket has properties including 
- access permission 
- versioning status 
- storage classes

######## S3 BUCKET NAMING RULE #######
> S3 bucket names(keys) are globally unique across all region 
- bucket name cannot be changed after they created 
- if a bucket is deleted its name become available again to you or another account to use 
- bucket names must be at least 3 or no more than 63 character long 

> bucket names are part of the URL is to access bucket

> Bucket name must be a series of one or more label(mymain.bucket)
- Adjacent label are separated by a single period 
- bucket name can contain lowercase letter, numbers and hyphen
- each label must start and end with a lowercase letter or a number

> Bucket name must not be formatted as an IP address
=======================================
###### S3 buckets sub resources ####### v4
> Amazon S3 supports various option for you to configure your bucket 
- Amazon S3 support subject sources for you to store and manage the bucket configuration information 
- using the Amazon S3 API you can create and manage these resources 
- you can also use the console or AWS SDK

> BY DEFAULT A BUCKET ITS OBJECT AND RELATED SUB-RESOURCES ARE ALL PRIVATE 
- i.e. by default only the owner has access to the bucket and storage object

> Sub-resources for S3 bucket includes:
- LIFECYCLE : to decide and object lifecycle management
- WEBSITE : to hold configuration related to static website hosted in it
- VERSIONING: keep object version as it change
- Access control list(ACLs) 
- bucket policies

## S3 bucket DNS names
> The name is simply two parts 
- bucket region endpoints/bucketname
EX: https://s3-eu-west-1.amazonaws.com/cloudbucket1

> S3 buckets region
> For better performance low latency help to minimise cost, create the S3 bucket closer to your client DC are source of data to be stored
=======================================
############ S3 objects ############### v5

> An object size stored in S3 bucket can be 0 byte upto 5tb
> Each object is stored and retrieved by unique key
> An object is a 330 uniquely identified and address through
> Service centre point bucketname object key optional ITI object version
> Object store in S3 bucket in a region will never leave that region
-  unless you specify move them to another reason or enable cross region replication
> S3 PROVIDE HIGH DATA DURABILITY, object are redundantly stored on multiple devices across multiple facility in Amazon S3 region where the bucket exist

## S3 object sub resources
1. Access Control List :
- to define guarantee and permission granted to the object

2. Torrent 
- used by S3 in case bittorrent tries to download the object
=======================================
#### S3 RESOURCES AND SUB-RESOURCES ### v6

> Bucket and object are primary S3 resources 
- each has its own sub resources

> Buckets sub-resources are lifecycle 
- Lifecycle, website, versioning, ACL and policies, Cross origin Resource sharing(CORS) and logging(bucket access log)

> Object sub-resources are 
- ACL and restore(restoring an archive)

> Operations on S3 are either bucket level operation or object label operation

#### S3 RESOURCE OWNER

> By default all Amazon S3 resources are private 
- only a resource owner can access the resources

> resource owner refer to the account that create the resources.
- for example account that you use to create buckets and objects owns those resources

> If you create an AWS IAM user in your AWS account you are the parent owner 
- if the IAM USER UPLOAD AN OBJECT, THE PARENT ACCOUNT OWNS THE OBJECT

> A bucket owner can grant cross-account permissions to another AWS account(or user in another account) upload object. 
- AWS account that upload the object own them

> The bucket owner does not have permission on the object that other account own with the following exception
- The bucket owner pays the bill. He can deny access to any object regardless of who owns them
- The bucket owner can delete any object in the bucket, regardless of who owns them.
- The bucket owner  can archive any objects or restore archive object regardless of who owns them

## Managing access to resources(access policy option)
> managing access refers to granting other (AWS account and user) permission to perform the resources operations by writing an access policy

> You can grant S3 bucket/object permission to 
- individual user 
- AWS accounts 
- make the resource public, grant permission to everyone (also referred as anonymous access) 
- or to all authenticated user (user with AWS credentials)
=======================================
######### S3 access policy ############ v7

> Access policy describe who has access to what. 
- You can associate and access policy with a resource (bucket/object) OR user

>  Amazon S3 access policy are as follow: 
1. Resource based Policy
2. User Access Policy 

#### 1. S3 RESOURCE-BASED POLICY ####
> bucket policy and access control list are resource-based because you attached them to your Amazon S3 resources

## 1a. ACL-BASED ACCESS POLICY (buckets and objects ACLs)
> each BUCKET AND OBJECT CAN HAVE AN ACL associated with it
> An ACL is a list of grants i.e. identifying guarantee(who) and permission granted(what)
> ACL GRANT BASIC READ/WRITE PERMISSION to other AWS account

##1b. BUCKET POLICY 
> FOR BUCKET AND OBJECT, we can add a bucket policy to grant other AWS account or IAM user permission 
- any object permission apply only to the object that the bucket owner creates 
- bucket policy supplement and in many cases replace ACL based access policy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###### 2. S3 USER-ACCESS POLICY #######

> You can use AWS IAM to manage access to your Amazon S3 resources

> Using IAM, we can create IAM user, group and role and attach user-access policy to them granting them access to AWS resources including Amazon S3

> You cannot grant anonymous permissions in any IAM user policy because the policy is attached to a user

> User policy can grant permission to a bucket and object in it

## How does S3 evaluate (allow/deny) a request for an S3 resource operation (Bucket or Object Operation)
> S3 evaluates:

User Context:
> S3 checks whether the user attached policies allows the request, so basically whether the parent account allows the operation to the IAM user or not
- If the user is the root account, this validation is skipped

Bucket Context:
> S3 validates whether the bucket owner (can be the same parent account or another AWS account) has allowed the requested operation to this user
- Bucket policy, Bucket ACL, and Object (if Object operation) are All checked

Notes:
> If the parent AWS account owns the resource (bucket or object), it can grant resource permissions to its IAM user by using either the user policy or the resource policy.

> If the bucket and object owner is the same,
- Access to the object can be granted in the bucket policy, which is evaluated at the bucket context.

> If the owners are different,
- The object owners must use an object ACL to grant permissions.

> If the AWS account that owns the object is also the parent account to which the IAM user belongs, it can configure object permissions in a user policy, which is evaluated at the user context.
=======================================
####### Bucket ACL permissions ######## v8

> Amazon S3 access control list(ACL) enable you to manage access to bucket and object

> Each bucket and object has an ACL attached to it as a sub-resources
- It DEFINE WHICH ACCOUNT(GRANTEE) OR PRE-DEFINED S3 GROUPS ARE GRANTED ACCESS AND THE TYPE OF ACCESS
- You cannot provide permissions to the individual IAM user
- Grantee accounts (cross account access) can then delegate access provided by other account to their individual user

> When you create a bucket or an object, Amazon S3 create a default ACL that grants the resources owner full control over the resources

### AWS S3 predefine groups
> Amazon S3 has a set of predefined groups that are

1. Authenticated user group 
- this group represent all AWS account. 
- Access permission to this group allow any AWS account to access the resources. 
- However all request must be signed(authenticated) 
- when you grant access to the authenticated user group any AWS authenticated user in the world can access our resources

2. All user group
> access permission to this group allows anyone in the world access to the resources 
- the request can be signed(authenticated) or unsigned(anonymous)
- unsigned request omit the authentication header in the request 
- AWS highly recommend that you never grant the 'all user group' WRITE, WRITE_ACP OR FULL_CONTROL permission

3. Log delivery group
- Write permission on a bucket enable this group to write server access logs
=======================================
########## v9

## When to use Object ACLs To manage Object Permissions

> An OBJECT ACL IS THE ONLY WAY TO MANAGE ACCESS TO OBJECTS NOT OWNED BY THE BUCKET OWNER
- This can happen when the bucket owner authorizes another account to upload objects to the bucket, but still the Bucket owner will not own the objects
- However, the bucket owner can deny, through the bucket policy, access to the object, even if the object ACL allows it
- The bucket owner can delete any object regardless whether it owns it or not

> Object ACLs is require to managing granular permissions at the object level because
- Bucket policies are limited to 20KB in size and won't be practical to be used for this

> Object ACLs are limited to 100 granted permissions per ACL


## When to use Bucket ACLS
> The only recommended use case for the bucket ACL is to grant write permission to the Amazon S3 Log Delivery group to write access log objects to your bucket

- If you want Amazon S3 to deliver access logs to your bucket, you will need to grant write permission on the bucket to the Log Delivery group.

> You can use bucket and Object ACLs to grant cross-account permissions to other accounts, but ACLS support only a finite set of permission, these don't include all Amazon S3 permissions.

######## ACL LIMITATIONS ###########
> You can use ACLs to grant basic read/write permissions to other AWS accounts.

> There are limits to managing permissions using ACLS.
- For example, you can grant permissions only to other AWS accounts; you cannot grant permissions to users in your account.

> You cannot grant conditional permissions, nor can you explicitly deny permissions. ACLs are suitable for specific scenarios.

=======================================
############ v10

## WHEN TO USE BUCKET POLICIES
> If an AWS account that owns a bucket wants to grant permission to users in its account, it can use either a bucket policy or a user policy.

> If you want to manage cross-account permissions for all Amazon S3 permissions 
- For Cross account permissions, when you use to provide another account full S3 permissions, you can only do that via Bucket policies

## WHEN TO USE USER POLICIES
> User policies support granting permission for all S3 operations
> The user policies are for managing permissions for users in your account.

> If the AWS account that owns the object wants to grant permission to a user in its account, it can use a user policy.

> An IAM user must have permissions from the parent account to which it belongs, and from the AWS account that owns the resource the user wants to access. 
The permissions can be granted as follows:
- Permission from the parent account : The parent account can grant permissions to its user by attaching a user policy.
- Permission from the resource owner : The resource owner can grant permission to either the IAM user (using a bucket policy) or the parent account (using a bucket policy, bucket ACL, or object ACL).

## PERMISSIONS DELEGATION
>> If an AWS acount owns a resource, it can grant those permissions to another AWS account.
- account can then delegate those permissions, or a subset of them, to users in the account. This is referred to as permission delegation.
- An account that receives permissions from another account cannot delegate permission cross-account to another AWS account

=======================================
######## S3 BUCKET VERSIONING ######### v11

> BUCKET VERSIONING is a S3 bucket sub-resource, USED TO PROTECT AGAINST ACCIDENTAL OBJECT DELETION OR OVERWRITES

> Versioning can also be used for data retention and archive
> once you enable Bucket versioning on a bucket, it can not be disabled, however, it can be suspended

> When enabled, bucket versioning will protect existing and new objects, and maintains their versions as they are updated (edited, written to...)
- Updating objects refers to PUT, POST, COPY, DELETE actions on objects

> By default an HTTP GET retrieves the most recent version
> Only S3 bucket owner can permanently delete objects once versioning is enabled

###### S3 Delete markers #####
> When versioning is enabled, and you try to delete an object, a DELETE marker is placed on the object
- You can still view the object and the delete marker

> If you reconsider deleting the object, you can delete the "DELETE Marker", and the object will be available again

> You will be charged for all S3 storage costs for all Object versions stored
- You can use versioning with S3 lifecycle policies to delete older versions, OR, you can move them to a cheaper S3 storage (or Glacier)

> Bucket versioning states:
- Enabled
- Suspended
- Un-versioned

> Versioning applies to all objects in a bucket and not partially applied
> Objects existing before enabling versioning will have a version ID or "NULL"
> If you have a bucket that is already versioned, then you suspend versioning, existing objects and their versions remain as is
- However they will not be updated/versioned further with future updates,

## Suspended Bucket Versioning
> While the bucket versioning is suspended:
- New objects (uploaded after suspension), they will have a version ID "null"
- If the same object key (name) is used to store another object, It will override the existing one (complete object overwrite)

- Objects with versions that existed before the versioning suspenion; 
- a new object with the same object key (name) will replace the curreht/latést version, but will have an ID "null", and becomes the new current version
- a new object with the same key (name) will overwrite the one with the "null" ID

> An object deletion in a suspended versloning bucket, will only delete the object(s) with ID "null"

> The bucket owner canpermanently delete any versions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Bucket Versioning- MFA Delete 

> Multi Factor Authentication (MFA) Delete is a versioning capability that adds another level of security in case your account is compromised

> This adds another layer of security for the following:
- Changing your bucket's versioning state
- Permanently deleting an object version

> MFA Delete requires:
- Your security credentials
- The code displayed on an approved physical or SW-based authentication device
> This provides maximum protection to your objects
=======================================
######### Multipart Upload ########## v12 
> Is used to upload an object (objects) in parts
- Parts are uploaded independently and in parallel, in any order
- It is recommended for objects sizes of 100MB or larger
- However, we can use for object sizes starting from 5MB up to 5TB
- You MUST USE IT FOR OBJECTS LARGER THAN 5GB
- This is done through the S3 Multipart upload API

## Copying S3 Objects
> The copy operation creates a copy of an object that is already stored in Amazon S3
- we can create a copy of your object up to 5 GB in size in a single atomic operation
- However, to copy an object greater than 5 GB, you must use the multipart upload API
- You will incur charges if the copy is to a destination that is another AWS region

> Can be done using AWS SDKS or REST API

> Use the copy operation to:
- Generate additional copies of the objects
- Renaming objects (copy to a new name)
- Changing the copy's storage class or encrypt it at rest (these can be done via AWS Console now with the new UI)
- Move objects across AWS locations/region
- Change object metadata
- Once you upload an objest to S3, you can NOT change some of its metadata, this is where the copy comes in handy

## Successful Upload Acknowledgement
> When you successfully upload an object (a file) to S3, S3 returns a HTTP 200 OK message for a successful PUT Operation
If uploading an Object and requesting SSE using Customer Provided Keys,and if the PUT operation is successful
Amazon S3 then returns the HTTP 200K, the encryption algorithm, and MD5 of the encryption key you specified when uploading the object.
=======================================
############ S3 STORAGE CLASS ######### v13
1. S3 Standard
2. Standard-IA
3. One Zone-IA 
4. S3-RRS (deprecating) 
5. S3 Intelligent-Tiering (new)

#### S3 STANDARD Storage Class ####
> Provides for 99.99% availability
> 11 9's Data Durability ( 99.999999999%)
> Data encrypted in-transit and at rest in S3
> Designed to sustain the concurrent loss of data in two facilities

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## S3 Infrequent Access Class(S3–IA) ##

> For long-lived, less frequently accessed data
> Designed to sustain the concurrent loss of data in two facilities

> Suitable for backups and older data (old pictures, files)
> 99.9% Availability
> 11  9's data durability

> Quick access and high performance like S3 Standard

> Data encrypted in-transit and at rest in S3 buckets

> Data must be kept for at least 30 days in this class

>> Suitable for objects greater than 128 KB (Not less than that)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## S3 Reduced Redundancy Storage(S3–RRS)

> Designed to sustain the data loss in one facility

> 99.99% availability
>> 99.99% data durability
- It is the only S3 class that does not offer 11 9's durability

> Use for non-critical, reproducible data

> Lower level of redundancy
> Use for images and save their thumbnails in S3 standard, transcoded media

> If an S3 RRS object is lost, AWS will return HTTP 405 error when the object is requested for read
> S3 can send a notification in case an object is lost
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 5. S3 Intelligent Tiering

> It is used when we don't have deep understanding of our data access patterns.

> It incorporates two access tiers: 
1. frequent access and 
2. infrequent access. 

> Both access tiers offer the same low latency as the Standard storage class. 
> For a small monitoring and automation fee, S3 Intelligent-Tiering monitors access patterns and moves objects that have not been accessed for 30 consecutive days to the infrequent access tier.
- If the data is accessed later, it is automatically moved back to the frequent access tier. 
- save money, with no performance impact, no operational overhead, and no retrieval fees.
=======================================
######### S3 LifeCycle Policy ######### v17
> This is a bucket level sub-resource (configuration).
- It can be applied to certain objects in a bucket folder, objects with a specific tag, or objects with a specific prefix

> The purpose is to primarily perform desired actions on contents (or some) of the bucket

> You can configure two actions:
1. Transition actions: 
- In which we can define object transition to another Storage class after certain time period

2. Expiration actions: 
> expiration duration for object is set, then S3 will then delete the expired objects on our behalf
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
### S3 Standard to Standard-IA transition
> Objects less than 128KB will not be transitioned to S3 Standard-IA
> Objects must be stored in S3-IA for at least 30 days

> An object must be in S3-Standard for at least 30 days before it can be transitioned to S3-IA
- The same applies to non-current versions of an object (when versioning is enabled), they must stay in S3-Standard for 30 days before they can be moved to S3-IA
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Moving Objects from Glacier to S3-Standard or S3-IA
> You can NOT use lifecycle policies to move an archived object from Glacier to S3-Standard or S3-IA

> The workaround to this is:
- Restore the archived object from Glacier to RRS
- Copy the Object
  • While copying specify the new storage class of the copy
  • Storage class of the original object remains in Glacier

> You can NOT change an object from any storage Class (including S3 Standard and S3-IA) into RRS
> You can't change from S3-IA to S3-Standard or S3-RRS
=======================================
#### S3 Server Side Encryption (SSE)### v18

> This is primarily about encrypting data at rest in S3 buckets
> There are two main ways to encrypt data stored on S3 buckets:

1. Client side encryption:
- Where the client encrypts data on the Client side, then transfer the encrypted data to S3 buckets ( hence data is encrypted In-transit and at Rest)

2. Server Side Encryption (SSE):
- Data is encrypted by the S3 service before it is saved to S3 storage disks
- Data is decrypted when you download it

> User access to S3 bucket objects is the same way even through it is encrypted

> At any given time, we can apply only 1 encryption type to S3 Object
=======================================
###### S3 - SS Encryption Model ####### v19

> Depending on how we manage encryption keys, there are 3 types of S3 SSE available:
1. Server Side Encryption on S3 (SSE-S3):
- It use S3 managed encryption keys

2. Server Side Encryption using AWS Key Management Service keys(SSE-KMS)
3. Server Side Encryption using Client provided keys (SSE-C)

####### 1. SSE-S3 ########

> Server Side Encryption on S3 using S3 managed encryption keys
> Each object is encrypted by a unique key, then the encryption key itself is encrypted using a master key
> S3 regularly rotates the master key
> Uses AES-256 bits

## PROCESS ## not require for SSA
> KMS GENERATES DATA KEY AND ENCRYPTS IT USING THE MASTER KEY THAT WE SPECIFIED EARLIER;
- KMS then returns this encrypted datakey along with the plaintext datakey to Amazon S3.
- Amazon S3 encrypts the object using the plaintext datakey first , and then stores the encrypted object (along with the encrypted datakey) and deletes the plaintext datakey from memory.
- To retrieve this encrypted object, S3 sends the encrypted datakey to AWS KMS.
- AWS KMS decrypts the datakey using the correct master key and returns the decrypted (plaintext) object key to S3.
- With the plaintext datakey, S3 decrypts the encrypted object and returns it to you.
- Each object is encrypted with a unique datakey,
- This key is encrypted with a periodically rotated key managed by AWS S3.
- SSE-S3 USES 256-BIT ADVANCED ENCRYPTION STANDARD (AES) KEYS FOR BOTH OBJECT AND MASTER KEYS.

> This feature is offered at no additional cost beyond what you pay for using Amazon S3.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##### 2. SSE-KMS 

> Server Side Encryption using AWS-KMS keys, similar to SSE-S3
- KMS uses Customer Master Keys (CMK) to encrypt S3 objects
- we can continue to use automatically created default CMK key for encryption OR, 
- we can select CMK that we created separately using AWS-KMS.

> Creating your own CMK will allow you to create, rotate, disable, and define access controls,
- Also allows you to audit the encryption keys used to protect your data.

> Separate permissions for an envelope key that protects/encrypts your object encryption keys

> Service is chargeable

- You can encrypt the data in S3 by defining an AWS KMS master key within your account that you want to use to encrypt the unique object/data key that will ultimately encrypt your object (data).

- When you upload your object, a request is sent to KMS to create an object key.
- The first time you add an SSE-KMS-encrypted object to a bucket in a region, a default CMK is created automatically.
- This key is used for SSE-KMS encryption unless you select a CMK that you created separately using AWS-KMS.

- Amazon S3 supports bucket policies that you can use if you require SSE for all objects that are stored in your bucket.
- For SSE to be requested in an API callI, the request has to include the "x-amz-server-side-encryption" header requesting SSE
- If you want SSE-KMS then the x-amz-server-side-encryption header has to define SEE-KMS.
"s3:x-amz-server-side-encryption":"aws:kms"

> Creating your own CMK gives you more flexibility, including the ability to create, rotate, disable, and define access controls, and to audit the encryption keys used to protect your data.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##### 3. SSE-C
> Server Side Encryption using Client provided keys
> Client manages the keys, S3 service manages encryption
> AWS does not store the client provided encryption key, so if the client looses the key, they can't access object, başically they loose the data.

- CLIENTS CAN USE THEIR OWN ENCRYPTION KEY WHILE UPLOADING AN OBJECT TO AMAZON S3.
- This encryption key is used by Amazon S3 to encrypt your data using AES-256.
- S3 does not store the key, after the object is encrypted, the encryption key supplied by the client is deleted from the Amazon S3 system that used it to protect the client's data.
- When the client retrieves this object from Amazon S3, they must provide the same encryption key in the request.
- Amazon S3 verifies that the encryption key matches, Decrypts the object, and returns the object to the requester.

=======================================
####### S3 Static Website Hosting ##### v20


> The website S3 endpoint URL is :
<S3bucketname>,S3-website-<AWS-Region>.amazonaws.com

> You need to allow read access to all your content on the bucket (website) content that you want to make available
> Use bucket policy or ACLS for this
> Requester Pays bucket do not work with website endpoint
> Such a request (requester pays) to an S3 website endpoint will return HTTP 403 Access Denied

> It does not support HTTPS (SSL) connections
> Returns an HTML document
> Supports object and bucket level redirects
> Support GET & HEAD requests only on objects
> Supports publicly readable content only
> When no specific content is requested by the HTTP GET or HEAD requests, it returns the index HTML document that you specify in your configuration

## S3 Static Website Hosting - Configurations
> Enable website hosting to your bucket, and specify:
- Index document (default web page) -> a MUST do
- Error document (Instead of the standard HTML 4X error messages) -> Optional
- The index/default web page document can be specified also under each folder (directory) in your bucket to be used as a response to requests that de not specify specific content

## S3 Static Website Hosting – Redirection
> If your Amazon S3 bucket is configured for website hosting, you can redirect requests for an object to another object in the same bucket or to an external URL.
> You can redirect (re-route) all requests (at the bucket level) to another website
> You can do conditional redirection based on object or prefixes in a request
> You can also redirect requests that would return an error
=======================================
############ Pre-Signed URL ########### v21

> PRE-SIGNED URL CAN BE USED TO PROVIDE TEMPORARY ACCESS TO A SPECIFIC OBJECT, to those who do not have AwS credentials,
- Example is customers who bought website subscription, or product subscription online

> By default, all objects are private and only the object owner can access it.

> To share an object you have to either:
- Make the object public, OR
- Generate a pre-signed URL to grant access, for a limited-time, to this object(s), using the object owner's own security credentials

> Expiration date and time must be configured when generating a pre-signed URL,
- The shared object(s) will be accessible by the signed URL through the expiration date/time

> Pre-signed URL for S3 objects can be generated using SDK for Java & .Net, and AWS Explorer for Visual Studio

> Pre-signed URL can be used for downloading or uploading S3 object(s)
=======================================
#### Cross Region Replication(CRR) #### v22

> It is a bucket level replication which enables
- Automatic, Asynchronous copying of Objects across buckets in different AWS regions

> Can be used for:
- Compliance requirements, where we need to store copy of data a distance away
- Provides for low latency access to data in other locations

> You can configure it from AWS Console, CLI, SDK, API

> The replicas will:
- Be exact replicas of the source bucket objects.
- Share the same key names and metadata (Creation time, version ID, ÁCL, Storage Class, User-defined metadata)

> To activate this feature, 
- add a replication configuration to source bucket.
- Destination bucket where you want objects replicated to.
- You can request to replicate all or subset of objects with specific key name prefixes."

> we can configure CRR with S3 Lifecycle management rules

> we can specify a different storage class for object replicas while creating the replication configuration

> S3 will encrypt data in-transit across regions using SSL

> It requires:
- The source and destination buckets are in different AWS regions
- Both source and dest buckets MUST HAVE VERSIONING ENABLED
- Replication can happen to only one destination bucket
- AWS S3 must have permission (IAM Role) to replicate objects from the source bucket to the destination bucket on your behalf.

 Bucket permissions:
> if the object owner is the same as the bucket owner, the bucket owner has full permissions to replicate the object.
- If not, the source bucket owner must have permission to read the object and object ACL
(AWS S3 actions s3:GetObjectVersion and s3:GetObjectVersionACL)

> If you are setting up cross-region replication in a cross-account (Different source & destination buckets are owned by different AWS accounts), the source bucket owner must have permission to replicate objects in the destination bucket.

> After enabling CRR, the following will trigger a replication:
- Every object upload to the source bucket
- Every DELETE to an object in the source buckep
- Any changes to the object, its metadata, or ACL
=======================================
###### S3 CRR - CREATE Operation ###### v23

> Any new objects created after you add a replication configuration, and changes to existing objects

> Objects created with SSE-S3 then 
- The replicated copy will also encrypted using same key

> Amazon S3 replicates only objects in the source bucket for which the bucket owner has permission to read objects and read ACLS.

> Any object ACL updates are replicated,
- There can be some delay before Amazon S3 can bring the two in sync.
- This applies only to objects created after you add a replication configuration to the bucket.

> S3 replicates object tags, if any.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
####### S3 CRR - DELETE Operation #####
> If we delete an object from the source bucket, the CRR behavior is as follows:
> If a DELETE request is made without specifying an object version ID,
- S3 adds a delete marker, which CRR repticates to the destination bucket.

> If a DELETE request specifies a particular object version ID to delete,
- S3 deletes that object version in the source bucket, but it does not replicate the deletion in the destination bucket
- It does not delete the same object version from the destination bucket
- This behavior protects data from malicious deletions.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## S3 CRR – What is NOT replicated

> AWS S3 will not replicate objects that existed before we added replication configuration
- You can use Copy API to copy existing bucket data
- Changes to existing objects that happen after CRR is enabled, are replicated

> Objects created with SSE-C or SSE-KMS are not replicated
- Amazon S3 does not keep the encryption keys you provide after the object is created in the source bucket so it cannot decrypt the object for replication, and therefore it does not replicate the object

> If the object owner is different from the source bucket owner, and bucket owner does not have permissions to the object, AWS S3 will not replicate these objects

> Updates to bucket-level sub-resources are not replicated
- This allows you to have different bucket configurations on the source and destination buckets.

Only customer actions are replicated
> Actions performed by lifecycle configuration are not replicated.
- For example, if lifecycle configuration is enabled only on your source bucket, Amazon S3 creates delete markers for expired objects, but it does not replicate those markers.
- However, you can have the same lifecycle configuration on both the source and destination buckets if you want the same lifecycle actions to happen to both buckets.

> Objects in the source bucket that are replicas, created by another cross-region replication, are not replicated.

## S3 Cross Region Replication - Charges
You are charged for :
- Requests for upload
- Inter-region data transfer
- Storage at the destination bucket
=======================================
## S3 Cross-Origin Resource Sharing (CORS) ## v24

> IT IS A WAY BY WHICH A CLIENT WEB APPLICATION (WEBPAGE OR A DOMAIN) CAN REQUEST RESOURCES FROM ANOTHER WEBPAGE (different domain)
> CORS can be used to allow web applications to access resources on S3 buckets/resources

> EXAMPLE:
- You can host web fonts on S3 bucket, then configure bucket to allow CORS requests for webfonts.
- Other domains (web pages) will issue CORS requests to load webfonts from your S3 bucket

> CORS configuration is an XML document with rules that identify the origins that we will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operation-specific information.
EXAMPLE:
<CORSConfiguration>
 <CORSRule>
   <AllowedOrigin>http://www.example.com</AllowedOrigin>
   <AllowedMethod>PUT</AllowedMethod>
   <AllowedMethod>POST</AllowedMethod>
   <AllowedMethod>DELETE</AllowedMethod>
   <AllowedHeader>*</AllowedHeader>
  <MaxAgeSeconds>3000</MaxAgeSeconds>
  <ExposeHeader>x-amz-server-side-encryption</ExposeHeader>
  <ExposeHeader>x-amz-request-id</ExposeHeader>
  <ExposeHeader>x-amz-id-2</ExposeHeader>
 </CORSRule>
</CORSConfiguration>

> In the CORS configuration, you can specify the following values for the AllowedMethod element: GET, PUT, POST, DELETE and HEAD

https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html
=======================================
######## S3 TRANSFER ACCELERATION ##### v25

> It is USED TO ACCELERATE OBJECT UPLOADS to S3 buckets from users over long distances

> Transfer acceleration is secure as direct upload to S3 over the internet

> It utilizes AWS Cloudfront edge location nearest to the upload source (user), once data arrives at the edge location, it gets routed to the destination S3 bucket over an optimized network path

> Need to Enable Transfer Acceleration on the S3 bucket
- Once enabled, it can only be suspended, it can NOT be disabled
- AFTER ENABLING IT, IT MAY TAKE UP TO 30 MIN before we notice upload speed enhancements

- Point your PUT/GET requests to:
  <bucketname>.S3-accelerate.amazonaws.com (for IPV4)

>> BUCKET NAMES must be DNS compliant and MUST NOT HAVE PERIODS (.) in the bucket name (between bucket name labels)

> Transfer Acceleration is NOT HIPAA Compliant(i.e not use for health data) 

> Using Transfer acceleration incurs a charge
- AWS checks for speed enhancements, and if no enhancement is provided, client does not get charged for using Transfer Acceleration

> No data is saved at Cloudfront edge locations (not cached)
> You can use multipart uploads with Transfer Acceleration

## S3 Transfer Acceleration – Speed Comparison Tool
> You can use it to compare accelerated and non-accelerated upload speeds across amazon S3 regions

> Uses multipart upload from your browser to multiple AWS, regions, and demonstrates speed enhancements
=======================================
########## v26

## S3 Intensive GET/ PUT
> S3 maintains an index of object key names in each region
> Object keys (names) are stored in an order across multiple partitions of this index
> The object key name dictates which partition the key is stored in
> Sequentially named objects are more likely to be saved in the same partition
- At high request rates to access sequentially named objects, S3 will place a higher load on the available I/O of the partition hosting these keys (names) which will impact
performance

## Occasional <=100PLD and <800 GET RPS
>> If bucket access (workload) involves 
- only occasional bursts of 100 PUT/LIST/DELETE RPS, and less than 800 GET RPS, 
- NO FURTHER ACTIONS are required to optimize S3 read/write performance

## Routine >100PLD or 300 GET RPS
>> If your bucket access 
- routinely exceeds 100 PUT/LIST/DELETE RPS or 300 GET RPS, then we need to:
- INTRODUCE PREFIX RANDOMIZATION IN OBJECTS KEY NAMES such that they will be stored in different partitions

## Rapid >300PLD or >800 GET RPS
> For rapidly increasing request rate where 
- bucket receives more than 300 PUT/LIST/DELETE RPS, or more than 800 GET RPS,
- OPEN A SUPPORT CASE WITH AWS, This will allow to prepare for the workload and avoid any temporary limits on your request rate

> For intensive GET requests,
- Introduce prefix randomize
- And use CloudFront distributions to distribute the objects and offload S3
- This will also decrease request charges, since CloudFront will cache and distribute the objects/content

> For steadily increasing request rates, S3 can adjust

=======================================
########## v27

## Billing Charges in S3
> No charge for data transferred from/to EC2 to/from S3 in the same region
- This includes data transferred via Copy command
> Data transfer into S3 is free of charge
> Data transfer out of S3 to EC2 instance in the same region is free
- Data transferred out of S3 to EC2 in other regions is charged_
> Data transfer via Copy to other regions is charged at the Internet data transfer rates
> Data transferred from S3 to Cloudfront is free

## Billing Charges in S3
> You pay for:
- Per GB/Month storage fee
- Data transferred in/out of S3 (as explained in the last slide)
- Upload requests($0.05/1000 requests). Both PUT and GET requests are charged
- Retrieval request fee for S3-IA and Glacier

## Requester Pays
> If Requester pays is enabled on a bucket:
- The bucket owner will only pay for Object Storage fees
- The requester will pay for:
  • Requests to S3 to upload/download objects
  • Data transfer charges

> Buckets with Requester Pays enabled do not allow anonymous access & Do not support BitTorrent

> Requester pays buckets can not be the target for user logging
> You can NOT specify requester pays at the Object level
> Can be enabled from the bucket properties under the AWS Console


=======================================
########## v28

## S3 & Event Notification
> When certain bucket events occur, AWS S3 Can be configured to automatically send notifications to one of the following AWS Services:
- SNS Topic(s)
- SQS queue
- or AWS Lambda function(s)

> This is a bucket level configuration, and you can configure multiple events as required
> You need to configure SNS, SQS, Lambda before configuring S3 event notifications
> No extra charge is incurred for enabling event notifications to your bucket, however, SNS, SQS, Lambda charges apply

> You can set notifications for, Create object, Object delete, Object Delete marker created.. and many other bucket related events


## S3 Monitoring via CloudWatch
> With AWS CloudWatch you can monitor multiple metrics.

> CloudWatch can take actions based on a single metric

> For S3, metrics that can be monitored by CloudWatch include:
- S3 Requests, Bucket Storage, Bucket Size, All requests, HTTP 4XX, 5XX errors among others

## S3 Monitoring via CloudWatch
> You can configure filters to filter CW data, these are called CloudWatch Dimensions
- You can filter by bucket name, storage type, prefix, tag

> Daily CloudWatch, Bucket-level storage metrics are turned on by default at no additional cost.
- 1 minute CloudWatch metric can be configured both at the bucket and object levef
- You can have up to 1000 metric configurations per bucket

## S3 API log monitoring via CloudTrail
> CloudTrail captures all API requests made to S3 API

> By default, AWS CloudTrail logs bucket level actions, however you can configure it to log the object level actions (such as DELETE, GET, PUT, POST object events)

> CloudTrail delivers these logs to a S3 bucket that you configure

> CloudTrail collected information includes: 
- Who made the request
- When it was made
- For what.etc
=======================================
##### GLACIER – ARCHIVING STORAGE ##### v14

> IT IS AN ARCHIVING STORAGE FOR INFREQUENTLY ACCESSED DATA
> Achieved objects are not available for real time access, you need to submit a retrieval request, 
- It restore the data first before you can read it (can take few hours),
- Requested archived data will be copied to RRS
- After data is retrieved, you have up to 24 hours to download it

> You can NOT specify Glacier as the Storage Class at the time you create an object

> 11 9's data durability

> No SLA

> It is designed to sustain data loss in two facilities
> We need to keep your data for a minimum of 90 days (minimum charge)

> Glacier automatically encrypts data at rest using AES-256-bit symmetric keys and supports secure transfer of data (In-transit) over Secure Sockets Layer (SSL)

> Glacier MAY NOT BE AVAILABLE IN ALL AWS REGIONS, check for availability

> Glacier objects are visible and available only through AWS S3 not through AWS Glacier

> Glacier REDUNDANTLY STORES DATA TO MULTIPLE FACILITIES, on multiple devices within each facility synchronously

> Glacier awaits until the multiple facility upload is completed successfully before confirmting a successful upload to the user

> Glacier DOES NOT ARCHIVE OBJECT METADATA, need to maintain a client-side database 

> GLACIER is Suitable for:
- Archive
- Media asset archiving,
- Health care information archiving,
- Regulatory and compliance archiving,
- A replacement for magnetic tapes

> You can upload archives to Glacier of a size:
- As small as 1 Byte
- As large as 40TB

> Glacier file archives of size 1bytes to 4GB can be done in one shot
> Glacier file archives from 100MB up to 40TB, can be uploaded to Glacier using multi-part upload
> Uploading archives is Synchronous
> Downloading archives is Asynchronous

> We can upload data to Glacier directly from CLI, SDKS, or through APIS
- we cannot do through Console

> The contents of an archive, once uploaded, can not be updated

> AWS Import/Export Snowball comes in handy when you need to upload large amount of data to Glacier usually takes 5-7 days

> When transitioning objects to Glacier from other classes (via lifecycle rules),
- Glacier adds 32KB overheads (indexing and archive metadata) to the object
- 40KB overheads for S3 to upload to Glacier (chargeable)


> It is recommended to GROUP LARGE NUMBER OF SMALL OBJECTS, into a smaller number of larger objects to reduce these overhead charges.
- If you need to access individual archived files, make sure that you group them in a way that allow this (compression techniques that allows the access to individual files)
• This will enable you to avoid downloading the entire archive to access individual files

> Maintaining Client-Side Archive Metadata
- Glacier does not support any additional metadata for the archives, except the archive description (optional)
- You might maintain metadata about the archives on the client-side. The metadata can include archive name and some other meaningful information about the archive.

> After upload an archive, you cannot update its content or its description.
- The only way you can update the archive content or its description is by deleting the archive and uploading another archive.

- Note that each time you upload an archive, Amazon Glacier returns to you a unigue archive ID.
=======================================
##### Glacier - Archive Retrieval ##### v15
> You can retrieve data from Glacier in multiple ways:

1. Expedited: 1-5 min
- More expensive
- Use for urgent requests only

2. Standard: 3-5 Hrs
- Less expensive than Expedited
- we get 10GB data retrieval free /month

3. Bulk retrieval: 5-12 Hrs
- Cheapest
- Use to retrieve large amounts up to Petabytes in a day

> You can retrieve parts of an archive
> It is common to group multiple objects and compress them using TAR or ZIP format before archiving them

> Archived Data Class does not change when retrieving data from Glacier, because what gets retrieved in a copy of the data, placed into RRS, not the actual Archive in Glacier

> Is an asynchronous job, and you need to submit/initiate a request for that
- You can NOT use AWS Console for Archive Jobs retrieval
- You can use AWS SNS to be notified when a retrieval job has completed

> When you restore an archive (or objects therein) from Glacier:
- You can specify while initiating the restore how long (default is 24 hours) you need the restored copy to be available to you
- you can also change this after the copy is made available in RRS (you can make it longer or shorter) by re-issuing a restore, AWS will update the same copy's expiration

- A copy of the object is moved into RRS, and the original remains in Glacier
- While the restoration is in progress, the console shows "restoration in progress"
- You pay for both Glacier and RRS storage for the duration the archive copy is available in RRS, after which, you only pay for Glacipr storage

- THE ARCHIEVE STORAGE CLASS REMAINS GLACIER 

## GLACIER BYTE RANGES

> S3 and Glacier support HTTP GET requests with Range in HTTP header to identify a specific byte range you may want to-retrieve from your Glacier archived data, instead of retrieving the entire archive
- This can help you manage the download charges (RRS charges)
- You can also download the portion your object(s) are within (in case the Glacier archive has many objects)
- But this also implies, you must maintain a client database (outside of Glacier), which includes information about the archive, the files contained, and the byte ranges
- This can also help you break a large archive retrieval into multiple jobs 

> Byte range can start as low as 1byte, and ends at increments of 1MByte (archived portion retrieval can end at the archive end, or multiples of 1Mbyte)

=======================================
########## Glacier – Costs ########## v16
> No charge for data transferred between EC2 instances and Glacier in the same region
> If you delete your data from Glacier before for 90 days from when it was archived you will be charged a deletion fee

> When you restore a Glacier archive you pay for :
- The Glacier archive itself
- The request(s)
- The restored copy from Glacier to RRS for the duration you specified in your réstore request

### Glacier - Archiving Storage
> When transitioning objects to Glacier from other classes (via lifecycle rules),
- Glacier adds 32KB overheads (indexing and archive metadata) to the object
- The number is 40KB overheads if you are using S3 to upload to Glacier
- The 32KB of overhead associated with each archive accounts for system indexing/metadata overhead.
- You pay for these overheads in addition to your actual data

> It is recommended to group a large number of small objects, into a smaller number of larger archieve to reduce these overhead charges.

=======================================
######### AWS STORAGE GATEWAY #########


> AWS Storage Gateway is a service that CONNECTS AN ON-PREMISES SOFTWARE APPLIANCE WITH CLOUD-BASED STORAGE TO PROVIDE SEAMLESS AND SECURE INTEGRATION between an organization's on-premises IT environment and AWS's Storage infrastructure. The service enables you to securely store data to the AWS cloud for scalable and cost-effective storage.

> AWS STORAGE GATEWAY'S SOFTWARE APPLIANCE is available for download as a virtual machine (VM) image that you INSTALL ON A HOST IN YOUR DATACENTER.

> Storage gateway supports either VMware ESXI or Microsoft Hyper-V. 
- Once you have installed your gateway associated with your AWS account through the activation process, you can use the AWS management console to create the storage gateway option that is right for you.

##### Types of Storage Gateway #####
1. File Gateway (NFS)
2. Volumes Gateway (ISCSI)
   2a. Stored Volumes
   2b. Cached Volumes
3. Tape Gateway (VTL)

#### 1. File Gateway #####
> Files are stored as object in S3 Buckets, accessed through a Network File System (NFS) mount point. 
- Ownership, permissions and timestamps are durably stored in S3 in the user-metadata of the object associated with the file. 
- Once objects are transferred to S3 they can be managed as native S3 objects, and bucket policies such as versioning, lifecycle management and cross region replication apply directly to objects stored in bucket.

##### 2. Volume Gateway #####
> A volume gateway provides cloud-backed storage volumes that you can mount as Internet Small Computer System Interface (ISCSI) devices from your on-premises application servers. The gateway supports the following volume configurations:

2a. Cached volumes 
- YOU STORE YOUR DATA IN S3 AND RETAIN A COPY OF FREQUENTLY ACCESSED DATA SUBSETS LOCALLY. 
- Cached volumes offer a substantial cost savings on primary storage and minimize the need to scale your storage on-premises. 
- You also retain low-latency access to your frequently accessed data.

2b. Stored volumes 
– If you need low-latency access to your entire dataset, first configure your on-premises gateway to store all your data locally. Then ASYNCHRONOUSLY BACK UP POINT-IN-TIME SNAPSHOTS OF THIS DATA TO AMAZON S3. 
- This configuration provides durable and inexpensive offsite backups that you can recover to your local data center or Amazon EC2. For example, if you need replacement capacity for disaster recovery, you can recover the backups to Amazon EC2.

###### 3. Tape Gateways ######
> With a tape gateway, you can cost-effectively and durably archive backup data in Amazon Glacier. A tape gateway provides a virtual tape infrastructure that scales seamlessly with your business needs and eliminates the operational burden of provisioning, scaling, and maintaining a physical tape infrastructure.

> You can run AWS Storage Gateway either on-premises as a VM appliance, or in AWS as an Amazon Elastic Compute Cloud (Amazon EC2) instance. You deploy your gateway on an EC2 instance to provision ISCSI storage volumes in AWS. Gateways hosted on EC2 instances can be used for disaster recovery, data mirroring, and providing storage for applications hosted on Amazon EC2.

### Summary
• File Gateway – For flat files, stored directly on S3.
• Tape Gateways – Used for backup and uses popular backup applications
like NetBackup, Backup Exec, Veear etc.
• Volume Gateway:
• Stored Volumes – Entire Dataset is stored on site and is
asynchronously backed up to S3.
• Cached Volumes – Entire Dataset is stored on S3 and the most
frequently accessed data is cached on site.
