=======================================
######### AWS STORAGE SERVICE #########

## AWS S3
- It is Simple Storage service 
- It is a scalable, high-speed, low-cost web-based service designed for online backup and archiving of data and application programs. 
- It allows to upload, store, and download any type of files up to 5 TB in size

## AWS Elastic File System
- Fully managed, scalable, and sharable storage among thousands of EC2 instances.

## Glacier
- Secure, durable, and extremely low-cost solutions for backup and archiving.

## Storage Gateway
- Seamlessly connect on-premise applications or services with the AWS cloud storage.

## 
=======================================
> three different metrics: availability, durability and reliability. 
## What durability is AWS?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## What durability is AWS?
> Durability is used to measure the likelihood of data loss. ... 
> AWS measures durability as a percentage. 
- For example, the S3 Standard Tier is designed for 99.999999999% durability. 
- This means that if you store 100 billion objects in S3, you will lose one object at most.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## What is reliability in AWS?

> The reliability pillar includes the ABILITY OF A SYSTEM TO RECOVER FROM INFRASTRUCTURE OR SERVICE DISRUPTIONS, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## What is Resiliency in AWS?

> The ability for a system to recover from a failure induced by load, attacks, and failures.
=======================================
##### SIMPLE STORAGE SERVICE(S3) ######

> S3 is one of the first services that has been produced by aws.
> S3 provides developers and IT teams with secure, durable, highly scalable object storage.
- It is easy to use with a simple web services interface to store and retrieve any amount of data from anywhere on the web.

## What is S3?
> S3 is a safe place to store the files.
- It is Object-based storage, i.e., you can store the images, word files, pdf files, etc.
- The files which are stored in S3 can be from 0 Bytes to 5 TB (i.e unlimited storage)

> Files are stored in Bucket. A bucket is like a folder available in S3 that stores the files.

> S3 is a universal namespace, i.e., the names must be unique globally. Bucket contains a DNS address. Therefore, the bucket must contain a unique name to generate a unique DNS address.
If you create a bucket, URL look like:
https://s3-eu-west-1.amazonaws.com/acloudguru 


> If you upload a file to S3 bucket, then you will receive an HTTP 200 code means that the uploading of a file is successful.

## ADVANTAGES OF AMAZON S3

1. Create Buckets: Firstly, we create a bucket and provide a name to the bucket. Buckets are the containers in S3 that stores the data. Buckets must have a unique name to generate a unique DNS address.

2. Storing data in buckets: Bucket can be used to store an infinite amount of data. You can upload the files as much you want into an Amazon S3 bucket, i.e., there is no maximum limit to store the files. Each object can contain upto 5 TB of data. Each object can be stored and retrieved by using a unique developer assigned-key.

3. Download data: You can also download your data from a bucket and can also give permission to others to download the same data. You can download the data at any time whenever you want.

4. Permissions: 
- You can also grant or deny access to others who want to download or upload the data from your Amazon S3 bucket. Authentication mechanism keeps the data secure from unauthorized access.

5. Standard interfaces: S3 is used with the standard interfaces REST and SOAP interfaces which are designed in such a way that they can work with any development toolkit.

6. Security: Amazon S3 offers security features by protecting unauthorized users from accessing your data.

### S3 is a simple key-value store
S3 is object-based. Objects consist of the following:

1. Key: It is simply the name of the object. For example, hello.txt, spreadsheet.xlsx, etc. You can use the key to retrieve the object.

2. Value: It is simply the data which is made up of a sequence of bytes. It is actually a data inside the file.

3. Version ID: Version ID uniquely identifies the object. It is a string generated by S3 when you add an object to the S3 bucket.

4. Metadata: It is the data about data that you are storing. A set of a name-value pair with which you can store the information regarding an object. Metadata can be assigned to the objects in Amazon S3 bucket.

5. Subresources: Subresource mechanism is used to store object-specific information.

6. Access control information: You can put the permissions individually on your files.

===================================
### Block versus object storage ###

## BLOCK STORAGE
> Block storage is SUITABLE FOR TRANSACTIONAL DATABASES, random read/write loads, and structured database storage

> Block storage divides the data to be stored in evenly sized blocks (data chunks), for instance, a file can be split into evenly sized blocks before it is stored 

> Data blocks stored in block storage would NOT CONTAIN METADATA (date created, date modified, content type...etc)

> Block storage only keeps the address(index) where the data blocks are stored

## OBJECT STORAGE
> It stores the files as a whole and does not divide them

> In object storage, an object is:
- A file/data itself
- Its metadata (data created, modified, security attributes, content type.etc)
- Object's global unique ID

> The Object Global Unique ID, Is a unique identifier for the object (can be the object name itself), and it must be unique such that it can be retrieved disregarding where its physical storage location is

> Examples for Objects,
- Photos, Videos, Music, Static Web Content, Data backups (snapshots), Archival images
- ANY DATA THAT CAN BE INCREMENTALY UPDATED and will not have a lot of writes/updates

> OBJECT STORAGE CAN GUARANTEE HIGH AVAILABILITY AND DURABILITY 
- data copies are stored on multiple, geographical distributed location 
- object storage cannot be mounted on a drive or directory, directly to an EC2 instance 
- object storage is a perfect solution for data growth storage problem
=======================================
###### DATA CONSISTENCY MODEL ####### v2
> Data consistency is relevant when we are considering copies of the same data object store over distributed system

> when the copies of data(stored on different systems) are read at the same time from different nodes, consistency level referred to how consistent will they be returned data (from the read), is it going to be 100% the same or slightly different?

####### STRONG CONSISTENCY ####
> Sometime also referred as immediate consistency

> Read from different data stores for listen data returns the exact same information
> Any update made to the date object in any storage node will be propagated and updated on all other storage nodes before the data is made available for read by clients
> Requires a locking mechanism to block read until the data is propagated and updated on all nodes
> Is good for transactional database and real time system with consistent writes
> Has limited scalability and reduced availability

#### EVENTUAL CONSISTENCY #####
> Read from different data stores for the same data result different returns
> There is no locking mechanism, if data is updated to an object, an immediate read from different note will not return the same data
- with time and as the changes/updates get propagated and updated on all other storage note, the reads will be will eventually consistent

> Eventually consistent can virtually provide unlimited 
- scalability 
- availability 
- data durability

#### S3 CONSISTENCY LEVEL
> S3 Provides:
- READ-AFTER-WRITE (IMMEDIATE OR STRONG) CONSISTENCY of PUTS of new objects (new object loads to S3)

> EVENTUAL CONSISTENCY FOR OVERWRITE data PUTS and DELETES (for Changes/updates to existing Objects in S3)

> Updates to an Object are atomic, i.e when you run a PUT for an object then you read (GET) that object, you will either get the updated object or the old one (before the update), you will NEVER GET PARTIAL OR CORRUPT DATA
=======================================
########### S3 ############## v3

> S3 is a storage for the internet. It has a simple web service interface for simple storing and retrieval of any amount of data, anytime, from anywhere on the internet 
> it is object based storage and not a block storage
> S3 has a DISTRIBUTED DATA-STORE ARCHITECTURE where object are redundant stored in multiple location

#### S3 BUCKETS ####
> Data is stored in buckets 
- It can be viewed as a flat container for objects that DOES NOT PROVIDE HIERARCHICAL OF OBJECT
- We can use object key(name) to mimic folders in a bucket when using the AWS console

> You can store unlimited object in your bucket but an object cannot exceed 5TB
> CAN CREATE FOLDER IN YOUR BUCKET (available through console only)
> You cannot create nested buckets (a bucket inside another)
> Bucket ownership is not transferable
> It is recommended to access S3 through SDK or API (console internally use API too)
> An S3 bucket is region specific
> You can have upto 100 buckets (soft limit) per account

> An S3 bucket has properties including 
- access permission 
- versioning status 
- storage classes

#### S3 BUCKET NAMING RULE #####
> S3 bucket names(keys) are globally unique across all region 
- bucket name cannot be changed after they created 
- if a bucket is deleted its name become available again to you or another account to use 
- bucket names must be at least 3 or no more than 63 character long 

> bucket names are part of the URL is to access bucket

> Bucket name must be a series of one or more label(mymain.bucket)
- Adjacent label are separated by a single period 
- bucket name can contain lowercase letter, numbers and hyphen
- each label must start and end with a lowercase letter or a number

> Bucket name must not be formatted as an IP address
=======================================
###### S3 buckets sub resources ####### v4
> Amazon S3 supports various option for you to configure your bucket 
- Amazon S3 support subject sources for you to store and manage the bucket configuration information 
- using the Amazon S3 API you can create and manage these resources 
- you can also use the console or AWS SDK

> By default a bucket its object and related sab resources are all private that is by default only the owner has access to the bucket and storage object

> Sub-resources for S3 bucket includes:
- LIFECYCLE : to decide and object lifecycle management
- WEBSITE : to hold configuration related to static website hosted in it
- VERSIONING: keep object version as it change
- Access control list(ACLs) 
- bucket policies

## S3 bucket DNS names
> The name is simply two parts 
- bucket region endpoints/bucketname
EX: https://s3-eu-west-1.amazonaws.com/cloudbucket1

> S3 buckets region
> For better performance low latency help to minimise cost, create the S3 bucket closer to your client DC are source of data to be stored
=======================================
############ S3 objects ############### v5

> An object size stored in S3 bucket can be 0 byte upto 5tb
> Each object is stored and retrieved by unique key
> An object is a 330 uniquely identified and address through
> Service centre point bucketname object key optional ITI object version
> Object store in S3 bucket in a region will never leave that region
-  unless you specify move them to another reason or enable cross region replication
> S3 PROVIDE HIGH DATA DURABILITY, object are redundantly stored on multiple devices across multiple facility in Amazon S3 region where the bucket exist

## S3 object sub resources
1. Access Control List :
- to define guarantee and permission granted to the object

2. Torrent 
- used by S3 in case bittorrent tries to download the object
=======================================
#### S3 RESOURCES AND SUB RESOURCES ### v6

> Bucket and object are primary S3 resources 
- each has its own sub resources

> Buckets sub-resources are lifecycle 
- Lifecycle, website, versioning, ACL and policies, Cross origin Resource sharing(CORS) and logging(bucket access log)

> Object sub-resources are 
- ACL and restore(restoring an archive)

> Operations on S3 are either bucket level operation or object label operation

## S3 RESOURCE OWNER

> By default all Amazon S3 resources are private 
- only a resource owner can access the resources

> resource owner refer to the account that create the resources.
- for example account that you use to create buckets and objects owns those resources

> If you create an AWS IAM user in your AWS account you are the parent owner 
- if the IAM USER UPLOAD AN OBJECT, THE PARENT ACCOUNT OWNS THE OBJECT

> A bucket owner can grant cross-account permissions to another AWS account(or user in another account) upload object. 
- AWS account that upload the object own them

> The bucket owner does not have permission on the object that other account own with the following exception
- The bucket owner pays the bill. He can deny access to any object regardless of who owns them
- The bucket owner can delete any object in the bucket, regardless of who owns them.
- The bucket owner  can archive any objects or restore archive object regardless of who owns them

## Managing access to resources(access policy option)
> managing access refers to granting other (AWS account and user) permission to perform the resources operations by writing an access policy

> You can grant S3 bucket/object permission to 
- individual user 
- AWS accounts 
- make the resource public, grant permission to everyone (also referred as anonymous access) 
- or to all authenticated user (user with AWS credentials)
=======================================
######### S3 access policy ############ v7

> Access policy describe who has access to what. 
- You can associate and access policy with a resource (bucket/object) OR user

>  Amazon S3 access policy are as follow: 
1. Resource based Policy
2. User Access Policy 

#### 1. S3 RESOURCE BASED POLICY ####
> bucket policy and access control list are resource-based because you attached them to your Amazon S3 resources

## 1a. ACL-BASED ACCESS POLICY (buckets and objects ACLs)
- each BUCKET AND OBJECT CAN HAVE AN ACL associated with it

> An ACL is a list of grants i.e. identifying guarantee(who) and permission granted(what)
> ACL GRANT BASIC READ/WRITE PERMISSION to other AWS account

##1b. BUCKET POLICY 
> FOR BUCKET AND OBJECT, we can add a bucket policy to grant other AWS account or IAM user permission 
- any object permission apply only to the object that the bucket owner creates 
- bucket policy supplement and in many cases replace ACL based access policy

###### 2. S3 USER ACCESS POLICY #######

> You can use AWS IAM to manage access to your Amazon S3 resources

> Using IAM, you can create IAM user, group and role in your account and attach access policy to them granting them access to AWS resources including Amazon S3

> You cannot grant anonymous permissions in any IAM user policy because the policy is attached to a user

> User policy can grant permission to a bucket and object in it

## How does S3 evaluate (allow/deny) a request for an S3 resource operation (Bucket or Object Operation)

> S3 evaluates:

User Context:
> S3 checks whether the user attached policies allows the request, so basically whether the parent account allows the operation to the IAM user or not
- If the user is the root account, this validation is skipped

Bucket Context:
- S3 validates whether the bucket owner (can be the same parent account or another AWS account) has allowed the requested operation to this user

- Bucket policy, Bucket ACL, and Object (if Object operation) are All checked

Notes:
> If the parent AWS account owns the resource (bucket or object), it can grant resource permissions to its IAM user by using either the user policy or the resource policy.

> If the bucket and object owner is the same,
- Access to the object can be granted in the bucket policy, which is evaluated at the bucket context.

> If the owners are different,
- The object owners must use an object ACL to grant permissions.

> If the AWS account that owns the object is also the parent account to which the IAM user belongs, it can configure object permissions in a user policy, which is evaluated at the user context.
=======================================
####### Bucket ACL permissions ######## v8

> Amazon S3 access control list(ACL) enable you to manage access to bucket and object

> Each bucket and object has an ACL attached to it as a sub-resources
- It DEFINE WHICH ACCOUNT(GRANTEE) OR PRE-DEFINED S3 GROUPS ARE GRANTED ACCESS AND THE TYPE OF ACCESS
- You cannot provide permissions to the individual IAM user
- Grantee accounts (cross account access) can then delegate access provided by other account to their individual user

> When you create a bucket or an object, Amazon S3 create a default ACL that grants the resources owner full control over the resources

### AWS S3 predefine groups
> Amazon S3 has a set of predefined groups that are

1. Authenticated user group 
- this group represent all AWS account. 
- Access permission to this group allow any AWS account to access the resources. 
- However all request must be signed(authenticated) 
- when you grant access to the authenticated user group any AWS authenticated user in the world can access our resources

2. All user group
> access permission to this group allows anyone in the world access to the resources 
- the request can be signed(authenticated) or unsigned(anonymous)
- unsigned request omit the authentication header in the request 
- AWS highly recommend that you never grant the 'all user group' WRITE, WRITE_ACP OR FULL_CONTROL permission

3. Log delivery group
- Write permission on a bucket enable this group to write server access logs
=======================================
########## v9

## When to use Object ACLs To manage Object Permissions

> An OBJECT ACL IS THE ONLY WAY TO MANAGE ACCESS TO OBJECTS NOT OWNED BY THE BUCKET OWNER
- This can happen when the bucket owner authorizes another account to upload objects to the bucket, but still the Bucket owner will not own the objects
- However, the bucket owner can deny, through the bucket policy, access to the object, even if the object ACL allows it
- The bucket owner can delete any object regardless whether it owns it or not

> Object ACLs is require to managing granular permissions at the object level because
- Bucket policies are limited to 20KB in size and won't be practical to be used for this

> Object ACLs are limited to 100 granted permissions per ACL


## When to use Bucket ACLS
> The only recommended use case for the bucket ACL is to grant write permission to the Amazon S3 Log Delivery group to write access log objects to your bucket

- If you want Amazon S3 to deliver access logs to your bucket, you will need to grant write permission on the bucket to the Log Delivery group.

> You can use bucket and Object ACLs to grant cross-account permissions to other accounts, but ACLS support only a finite set of permission, these don't include all Amazon S3 permissions.

######## ACL LIMITATIONS ###########
> You can use ACLs to grant basic read/write permissions to other AWS accounts.

> There are limits to managing permissions using ACLS.
- For example, you can grant permissions only to other AWS accounts; you cannot grant permissions to users in your account.

> You cannot grant conditional permissions, nor can you explicitly deny permissions. ACLs are suitable for specific scenarios.

=======================================
############ v10

## When to use Bucket Policies

> If an AWS account that owns a bucket wants to grant permission to users in its account, it can use either a bucket policy or a user policy.

> If the AWS account that owns the object also owns the bucket, then it can write a bucket policy to manage the object permissions.

> If you want to manage cross-account permissions for all Amazon S3 permissions 
- For Cross account permissions, when you use to provide another account full S3 permissions, you can only do that via Bucket policies

## When to use User Policies
> User policies support granting permission for all Amazon S3 operations

> The user policies are for managing permissions for users in your account.

> If the AWS account that owns the object wants to grant permission to a user in its account, it can use a user policy.

> An IAM uset must have permissions from the parent account to which it belongs, and from the AWS account that owns the resource the user wants to access. 
The permissions can be granted as follows:
- Permission from the parent account : The parent account can grant permissions to its user by attaching a user policy.
- Permission from the resource owner : The resource owner can grant permission to either the IAM user (using a bucket policy) or the parent account (using a bucket policy, bucket ACL, or object ACL).

## PERMISSIONS DELEGATION
>> If an AWS acount owns a resource, it can grant those permissions to another AWS account.
- account can then delegate those permissions, or a subset of them, to users in the account. This is referred to as permission delegation.
- An account that receives permissions from another account cannot delegate permission cross-account to another AWS account

=======================================
######## S3 BUCKET VERSIONING ######### v11

> Bucket versioning is a S3 bucket sub-resource, used to protect against accidental object/data deletion or overwrites

> Versioning can also be used for data retention and archive
> once you enable Bucket versioning on a bucket, it can not be disabled, however, it can be suspended

> When enabled, bucket versioning will protect existing and new objects, and maintains their versions as they are updated (edited, written to...)
- Updating objects refers to PUT, POST, COPY, DELETE actions on objects

> By default an HTTP GET retrieves the most recent version
> Only S3 bucket owner can permanently delete objects once versioning is enabled

###### S3 Delete markers #####
> When versioning is enabled, and you try to delete an object, a DELETE marker is placed on the object
- You can still view the object and the delete marker

> If you reconsider deleting the object, you can delete the "DELETE Marker", and the object will be available again

> You will be charged for all S3 storage costs for all Object versions stored
- You can use versioning with S3 lifecycle policies to delete older versions, OR, you can move them to a cheaper S3 storage (or Glacier)

> Bucket versioning states:
- Enabled
- Suspended
- Un-versioned

> Versioning applies to all objects in a bucket and not partially applied
> Objects existing before enabling versioning will have a version ID or "NULL"
> If you have a bucket that is already versioned, then you suspend versioning, existing objects and their versions remain as is
- However they will not be updated/versioned further with future updates,

## Suspended Bucket Versioning
> While the bucket versioning is suspended:
- New objects (uploaded after suspension), they will have a version ID "null"
• If the same object key (name) is used to store another object, It will override the existing one (complete object overwrite)

- Objects with versions that existed before the versioning suspenion; 
- a new object with the same object key (name) will replace the curreht/latést version, but will have an ID "null", and becomes the new current version
- a new object with the same key (name) will overwrite the one with the "null" ID

> An object deletion in a suspended versloning bucket, will only delete the object(s) with ID "null"

> The bucket owner canpermanently delete any versions

## Bucket Versioning- MFA Delete
> Multi Factor Authentication (MFA) Delete js a versioning capability that adds another level of security in case your account is compromised

> This adds another layer of security for the following:
- Changing your bucket's versioning state
- Permanently deleting an object version

> MFA Delete requires:
- Your security credentials
- The code displayed on an approved physical or SW-based authentication device
> This provides maximum protection to your objects
=======================================
Multipart Upload
Is used to upload an object (objects) in parts
Parts are uploaded independently and in parallel, in any order
It is recommended for objects sizes of 100MB or larger
However, you can use for object sizes starting from 5MB up to 5TB
You must use it for Objects larger than 5GB
This is done through the S3 Multipart upload API
[9:35 AM, 1/23/2020] Manish Agrawal: Copying S3 Objects
The copy operation creates a copy of an object that is already stored in Amazon S3
You can create a copy of your object up to 5 GB in size in a single atomic operation
However, to copy an object greater than 5 GB, you must use the mu
art upload API
You will incur charges if the copy is to a destination that is another AWS region
[9:36 AM, 1/23/2020] Manish Agrawal: Copying S3 Objects
Can be done using AWS SDKS or REST API
ES-315
Use the copy operation to:
Generate additional copies of the objects
Renaming objects (copy to a new name)
Changing the copy's storage class or encrypt it at rest (these can be done via AWS
Console now with the new UI)
$3- and Ss3-RRS
Move objects across AWS locations/region
Change object metadata
Once you upload an objest to S3, you can NOT change some of its metadata, this is
where the copy comes in handy
Pedire col
[9:40 AM, 1/23/2020] Manish Agrawal: Successful Upload Acknowledgement
When you successfully upload an object (a file) to S3, S3 returns a HTTP 200 OK message
for a successful PUT Operation
If uploading an Object and requesting SSE using Customer Provided Keys,and if the PUTPoT
operation is successful
Amazon S3 then returns the HTTP 200K, the encryption algorithm, and MD5 of the
encryption key you specified when uploading the object.
=======================================
############ S3 STORAGE CLASS #########

#### S3 STANDARD Storage Class ####
> Provides for 99.99% availability
> 11 9's Data Durability ( 99.999999999%)
> Data encrypted in-transit and at rest in S3
> Designed to sustain the concurrent loss of data in two facilities

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## S3 Infrequent Access Class(S3–IA) ##

> For long-lived, less frequently accessed data
> Designed to sustain the concurrent loss of data in two facilities

> Suitable for backups and older data (old pictures, files)
> 99.9% Availability
> 11  9's data durability

> Quick access and high performance like S3 Standard

> Data encrypted in-transit and at rest in S3 buckets

> Data must be kept for at least 30 days in this class

> Suitable for objects greater than 128 KB (Not less than that)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## S3 Reduced Redundancy Storage(S3–RRS)

> Designed to sustain the data loss in one facility

> 99.99% availability
> 99.99% data durability

> Use for non-critical, reproducible data

> Lower level of redundancy
> Use for images and save their thumbnails in S3 standard, transcoded media

> If an S3 RRS object is lost, AWS will return HTTP 405 error when the object is requested for read
> S3 can send a notification in case an object is lost
=======================================
##### GLACIER – ARCHIVING STORAGE ##### v14

> IT IS AN ARCHIVING STORAGE FOR INFREQUENTLY ACCESSED DATA
> Achieved objects are not available for real time access, you need to submit a retrieval request, 
- It restore the data first before you can read it (can take few hours),
- Requested archived data will be copied to RRS
- After data is retrieved, you have up to 24 hours to download it

> You can NOT specify Glacier as the Storage Class at the time you create an object

> 11 9's data durability

> No SLA

> Is designed to sustain data loss in two facilities
> You need to keep your data for a minimum of 90 days (minimum charge)

> Glacier automatically encrypts data at rest using AES-256-bit symmetric keys and supports
secure transfer of your data (In-transit) over Secure Sockets Layer (SSL)

> Glacier may NOT be available in all AWS regions, check for availability

> Glacier objects are visible and available only through AWS S3 not through AWS Glacier

> Glacier redundantly stores your data to multiple facilities, on multiple devices within each facility
synchronously

> Glacier awaits until the multiple facility upload is completed successfully before confirmting a
successful upload to the user

> Glacier does NOT archive object metadata, you need to maintain a client-side database to
maintain this information about the objects 
- Basically which archives hold which objects 

Suitable for:
-Archive
- Media asset archiving,
Health care information archiving,
- Regulatory and compliance archiving,
- A replacement for magnetic tapes

> You can upload archives to Glacier of a size:
- As small as 1 Byte
- As large as 40 TB

> Glacier file archives of size 1bytes to 4GB can be done in one shot

> Glacier file archives from 100MB up to 40TB, can be uploaded to Glacier using multi-part upload

> Uploading archives is Synchronous

> Downloading archives is Asynchronous

> The contents of an archive, once uploaded, can not be updated

> You can upload data to Glacier directly from CLI, SDKS, or through APIS

- You can not do that through the AWS Console

> AWS Import/Export Snowball comes in handy when you need to upload large amount of
data to Glacier
usually takes 5-7 days

> When transitioning objects to Glacier from other classes (via lifecycle rules),
- Glacier adds 32KB overheads (indexing and archive metadata) to the object
-• The number is 40KB overheads if you are using S3 to upload to Glacier
- You pay for these overheads in addition to your actual data
- The 32KB of overhead associated with each archive accounts för system
indexing/metadata overhead.


> It is recommended to group a large number of small objects, into a smaller number of larger objects to reduce these overhead charges.
- If you need to access individual archived files, make sure that you group them in a way
that allow this (compression techniques that allows the access to individual files)
• This will enable you to avoid downloading the entire archive to access individual
files

> Maintaining Client-Side Archive Metadata
- Amazon Glacier does not support any additional metadata for the archives, except the archive description (optional)
- You might maintain metadata about the archives on the client-side. The metadata can include archive name and some other meaningful information about the archive.

> After you upload an archive, you cannot update its content or its description.
- The only way you can update the archive content or its description is by deleting the archive and uploading another archive.

- Note that each time you upload an archive, Amazon Glacier returns to you a unigue archive ID.
=======================================
##### Glacier - Archive Retrieval ##### v15
> You can retrieve data from Glacier in multiple ways:

1. Expedited: 1-5 mts
- More expensive
- Use for urgent requests only

2. Standard: 3-5 Hrs
- Less expensive than Expedited
- You get 10GB data retrieval free /month

3. Bulk retrieval: 5-12 Hrs
- Cheapest
- Use to retrieve large amounts up to Petabytes in a day

> You can retrieve parts of an archive
> It is common to group multiple objects and compress them using TAR or ZIP format before archiving them

> Archived Data Class does not change when retrieving data from Glacier, because what gets retrieved in a copy of the data, placed into RRS, not the actual Archive in Glacier

> Is an asynchronous job, and you need to submit/initiate a request for that
- You can NOT use AWS Console for Archive Jobs retrieval
- You can use AWS SNS to be notified when a retrieval job has completed

> When you restore an archive (or objects therein) from Glacier:
- You can specify while initiating the restore how long (default is 24 hours) you need the restored copy to be available to you
- you can also change this after the copy is made available in RRS (you can make it longer or shorter) by re-issuing a restore, AWS will update the same copy's expiration

- A copy of the object is moved into RRS, and the original remains in Glacier
- While the restoration is in progress, the console shows "restoration in progress"
- You pay for both Glacier and RRS storage for the duration the archive copy is available in RRS, after which, you only pay for Glacipr storage

- THE ARCHIEVE STORAGE CLASS REMAINS GLACIER 

## GLACIER BYTE RANGES

> S3 and Glacier support HTTP GET requests with Range in HTTP header to identify a specific byte range you may want to-retrieve from your Glacier archived data, instead of retrieving the entire archive
- This can help you manage the download charges (RRS charges)
- You can also download the portion your object(s) are within (in case the Glacier archive has many objects)
- But this also implies, you must maintain a client database (outside of Glacier), which includes information about the archive, the files contained, and the byte ranges
- This can also help you break a large archive retrieval into multiple jobs 

> Byte range can start as low as 1byte, and ends at increments of 1MByte (archived portion retrieval can end at the archive end, or multiples of 1Mbyte)

=======================================
########## Glacier – Costs ########## v16
> No charge for data transferred between EC2 instances and Glacier in the same region
> If you delete your data from Glacier before for 90 days from when it was archived you will be charged a deletion fee

> When you restore a Glacier archive you pay for :
- The Glacier archive itself
- The request(s)
- The restored copy from Glacier to RRS for the duration you specified in your réstore request

### Glacier - Archiving Storage
> When transitioning objects to Glacier from other classes (via lifecycle rules),
- Glacier adds 32KB overheads (indexing and archive metadata) to the object
- The number is 40KB overheads if you are using S3 to upload to Glacier
- The 32KB of overhead associated with each archive accounts for system indexing/metadata overhead.
- You pay for these overheads in addition to your actual data

> It is recommended to group a large number of small objects, into a smaller number of larger archieve to reduce these overhead charges.
=======================================
####### S3 Life Cycle Policy ####### v17
> This is a bucket level sub-resource (configuration).
- It can be applied to certain objects in a bucket folder, objects with a specific tag, or objects with a specific prefix

> The purpose is to primarily perform desired actions on contents (or some) of the bucket

> You can configure two actions:
1. Transition actions: to another S3 storage tier after a configured period
2. Expiration actions: Where an expiration duration for object(s) is set, S3 will then delete the expired objects on your behalf

### S3 Standard to Standard-IA transition
> Objects less than 128KB will not be transitioned to S3 Standard-IA
> Objects must be stored in S3-IA for at least 30 days
> An object must be in S3-Standard for at least 30 days before it can be transitioned to S3-IA
- The same applies to non-current versions of an object (when versioning is enabled), they must stay in 53-Standard for 30 days before they can be moved to S3-IA

## Moving Objects from Glacier to S3-Standard or S3-IA
> You can NOT use lifecycle policies to move an archived object from Glacier to S3-Standard or S3-IA

> The workaround to this is:
- Restore the archived object from Glacier to RRS
- Copy the Object
  • While copying specify the new storage class of the copy
  • Storage class of the original object remains in Glacier

> You can NOT change an object from any storage Class (including S3 Standard and S3-IA) into RRS
> You can't change from S3-IA to S3-Standard or S3-RRS
=======================================
##### S3 Server Side Encryption (SSE)### v18

> This is primarily about encrypting data at rest in S3 buckets
> There are two main ways to encrypt data stored on S3 buckets:

1. Client side encryption:
- Where the client encrypts data on the Client side, then transfer the data encrypted to S3 buckets ( hence data is encrypted In-transit and at Rest)

2. Server Side Encryption (SSE):
- Data is encrypted by the S3 service before it is saved to S3 storage disks
- Data is decrypted when you download it

> User access to S3 bucket objects is the same whether the data is encrypted or not on S3 buckets (as long as the user has permission to access the data)

> At any given time, you can apply only one encryption type to an S3 Object

> Depending on how you manage encryption keys, there are three types of S3 SSE available:
1. SSE-S3:
Server Side Encryption on S3 using S3 managed encryption keys

2. SSE-KMS:
- Server Side Encryption using AWS KMS keys

3. SSE-C
- Server Side Encryption using Client provided keys

## S3 Server Side Encryption using AWS S3 Managed Keys (SSE-S3)

> Server Side Encryption on S3 using S3 managed encryption keys
> Each object is encrypted by a unique key, then the encryption key itself is encrypted using a master key
> S3 regularly rotates the master key
> Uses AES-256 bits
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## S3 Server Side Eneryption using AWS KMS Managed Keys (SSE-KMS)

> Server Side Encryption using AWS KMS keys, similar to SSE-S3
- KMS uses Customer Master Keys (CMKS)to êncrypt your S3 objects
- You can continue to use the automatically created default CMK key for encryption OR, 
- You can select a CMK that you created separately using AWS Key Management Service.

> Creating your own CMK will allow you to create, rotate, disable, and define access controls,
- Also allows you to audit the encryption keys used to protect your data.

> Separate permissions for an envelope key that protects/encrypts your object encryption keys

> Service is chargeable

## S3 Server Side Encryption using Customer Provided Keys (SSE-C)
> Server Side Encryption using Client provided keys
> Client manages the keys, S3 service manages encryption
> AWS does not store the client provided encryption key(s), so if the client looses the key(s), they can't access the abject, başically they loose the data.
=======================================
########## v19
[7:08 AM, 2/5/2020] Manish Agrawal: Encryption Model C- S3
There are three ways of encrypting your data in Amazon S3 using server-side encryption.
Server-side encryption: SSE-S3
1.
2. Server-Side Encryption using Customer Keys (SSE-C)
Server-Side Encryption using AWS KMS (SSE-KMS)
3.
[7:09 AM, 2/5/2020] Manish Agrawal: Encryption Model C – S3
1. Server-side encryption: SSE-S3
- KMS generates this data key and encrypts it using the master key that you specified earlier;
- KMS then returns this encrypted data key along with the plaintext data key to Amazon S3.
Amazon S3 encrypts the object using the plaintext data key first , and then stores the now
encrypted object (along with the encrypted object key) and deletes the plaintext object key
from memory.
- To retrieve this encrypted object, Amazon S3 sends the encrypted data key to AWWS KMS.
AWS KMS decrypts the data key using the correct master key and returns the decrypted
(plaintext) object key to S3.
- With the plaintext object key, S3 decrypts the encrypted object and returns it to you.
Each object is encrypted with a unique data key,
• This key is encrypted with a periodically rotated key managed by AWS 53.
• Amazon S3 server-side encryption uses 256-bit Advanced Encryption Standard (AES) keys
for both object and master keys.
This feature is offered at no additional cost beyond what you pay for using Amazon S3.
CMIC
[7:09 AM, 2/5/2020] Manish Agrawal: Encryption Model C – S3
2. Server-side encryption using customer provided keys: (SSE-C)
- Clients can use their own encryption key while uploading an object to Amazon S3.
- This encryption key is used by Amazon $3 to encrypt your data using AES-256.
S3 does not store the key, after the object is encrypted, the encryption key supplied by the
client is deleted from the Amazon S3 system that used it to protect the client's data.
When the client retrieves this object from Amazon S3, they must provide the same encryption
key in the request.
• Amazon S3 verifies that the encryption key matches,
• Decrypts the object, and returns the object to the requester.
[7:10 AM, 2/5/2020] Manish Agrawal: Encryption ModelC – 3
3. Server-side encryption using KMS:
- You can encrypt the data in Amazon 53 by defining an AWS KMS master key within your
account that you want to use to encrypt the unique object (data) key that will ultimately
encrypt your object (data).
deta
- When you upload your object, a request is sent to KMS to create an object key.
- The first time you add an SSE-KMS-encrypted object to a bucket in a region, a default CMK is
created for you automatically. DOLFIN
• This key is used for SSE-KMS encryption unless you select a CMK that you created
separately using AWS Key Management Service.
Creating your own CMK gives you more flexibility, including the ability to create, rotate,
disable, and define access controls, and to audit the encryption keys used to protect your
Custo
data.
[7:10 AM, 2/5/2020] Manish Agrawal: Encryption Model C – S3
Server-side encryption SSE-KMS
- Amazon S3 supports bucket policies that you can use if you require server-side encryption for
all objects that are stored in your bucket.
- For SSE to be requested in an API callI, the request has to include the x-amz-server-side-
encryption header requesting server-side encryption
- If you want SSE-KMS then the x-amz-server-side-encryption Keader has to define SEE-KMS.
"s3:x-amz-server-side-encryption":"aws:kms"
=======================================
########## v20

[7:23 AM, 2/5/2020] Manish Agrawal: S3 Static Website Hosting
The website S3 endpoint URL is :
<S3bucketname>,S3-website-<AWS-Region>.amazonaws.com
You need to allow read access to all your content on the bucket (website) content that you
want to make available
Use bucket policy or ACLS for this
83 RES API
$30E5T API
Requester Pays bucket do not work with website endpoint
Such a request (requester pays) to an S3 website endpoint will return
HTTP 403 Access Denied
[7:23 AM, 2/5/2020] Manish Agrawal: S3 Static Website Hosting – S3 Website Endpoint
It does not support HTTPS (SSL) connections
Returns an HTML document
RESTAPÊ
welosits
Endpornt
Supports object and bucket level redirects
Support GET & HEAD requests only on
Sbjectsed
Supports publicly readable content only
Wher no specific content is requested by the HTTP GET or HEAD requests, it returns the
index HTML document that you specify in your configuration
[7:23 AM, 2/5/2020] Manish Agrawal: S3 Static Website Hosting - Configurations
Enable website hosting to your bucket, and specify:
Index document (default web page) -> a MUST do
Error document (Instead of the standard HTML 4X error messages) -> Optional
The index/default web page document can be specified also under each folder
(directory) in your bucket to be used as a response to requests that de not specify
specific content
DOLFIN

## S3 Static Website Hosting – Redirection
If your Amazon S3 bucket is configured for website hosting, you can redirect requests for an
object to another object in the same bucket or to an external URL.
You can redirect (re-route) all requests (at the bucket level) to another website
You can do conditional redirection based on object or prefixes in a request
You can also redirect requests that would return an error
=======================================
########## v21

## Sharing S3 Objects via Pre-Signed URLS
> Pre-signed URLS can be used to provide temporary access to a specific object, to those who do not have AwS credentials,
- Example is customers who bought website subscription, or product subscription online

> By default, all objects are private and only the object owner can access it.

> To share an object you have to either:
- Make the object public, OR DOLFINE
- Generate a pre-signed URL to grant.access, for a limited-time, to this object(s), using the object owner's own security credentials

## Sharing S3 Objects via Pre-Signed URLS
> Expiration date and time must be configured when generating a pre-signed URL,
- The shared object(s) will be accessible by the signed URL through the expiration date/time

> Pre-signed URLS for S3 objects can be generated using SDKS for Java & .Net, and AWS Explorer for Visual Studio

> Pre-signed URLS can be used for downloading or uploading S3 object(s)
=======================================
########## v22


## S3 Cross Region Replication (CRR)
> Is a bucket level replication which enables
- Automatic, Asynchronous copying of Objects across buckets in different AWS regions

> Can be used for:
- Compliance requirements, where you need to store copy of your data a distance away
- Provides for low latency access to your data in other locations

> You can configure it from AWS Console, CLI, SDKS, APIS

> To activate this feature, you add a replication configuration to your source bucket.
- You need to define,
- Destination bucket where you want objects replicated to.
- You can request to replicate all or a subset of objects with specific key name prefixes."

## S3 Cross Region Replication
> You can configure AWS S3 CRR with S3 Lifecycle management rules
> The replicas will:
- Be exact replicas of the source bucket objects.
- Share the same key names and metadata (Creation time, version ID, ÁCL, Storage Class, User-defined metadata)

> Optional:
- You can specify a different storage class for object replicas while creating the replication configuration
- If you did not specify it, the same storage class as the source object class will be used for the replica in the destination bucket

> AWS S3 will encrypt data in-transit across regions using SSL

## S3 Cross Region Replication
> It requires:
- The source and destination buckets are in different AWS regions
- Both source and destination buckets MUST have Versioning enabled
- Replication can happen to only one destination bucket
- AWS S3 must have permission (IAM Role) to replicate objects from the source bucket to the destination bucket on your behalf.

## S3 Cross Region Replication
Bucket permissions:
if the object owner is the same as the bucket owner, the bucket owner has full
permissions to replicate the object.
• If not, the source bucket owner must have permission to read the object and
object ACL
(AWS S3 actions s3:GetObjectVersion and s3:GetObjectVersionACL)
If you are setting up cross-region replication in a cross-account (Different source &
destination buckets are owned by different AWS accounts), the source bucket owner
must have permission to replicate objects in the destination bucket.

## S3 Cross Region Replication – What triggers a replication
> After enabling CRR, the following will trigger a replication:
- Every object upload to the source bucket
- Every DELETE to an object in the source buckep
- Any changes to the object, its metadata, or ACL
=======================================
########## v23

[7:56 AM, 2/5/2020] Manish Agrawal: S3 Cross Region Replication – What is Replicated
Any new objects created after you add a replication configuration, and changes to existing objects
Objects created with SSE-S3 using the AWS S3-managed encryption key.
The replicated copy of the object is also encrypted using server-side encryption using the AWS
S3-managed encryption key.
Amazon S3 replicates only objects in the source bucket for which the bucket owner has permission
to read objects and read ACLS.
DOLFINES
Asyn-
Any object ACL updates are replicated,
There can be some delay before Amazon S3 can bring the two in sync.
This applies only to objects created after you add a replication configuration to the bucket.
S3 replicates object tags, if any.
Source: aws.amazon.com
[7:57 AM, 2/5/2020] Manish Agrawal: S3 Cross Region Replication – What is Replicated (DELETE Operation)
If you delete an object from the source bucket, the cross-region replication behavior is as follows:
If a DELETE request is made without specifying an object version ID,
AWS S3 adds a delete marker, which cross-region replication repticates to the destination
bucket.
If a DELETE request specifies a particular object version ID to delete,
• AWs S3 deletes that object version in the source bucket, but it does not replicate the
deletion in the destination bucket o
- It does not delete the same object version from the destination bucket
[7:57 AM, 2/5/2020] Manish Agrawal: S3 Cross Region Replication – What is Replicated (DELETE Operation)
If you delete an object from the source bucket, the cross-region replication behavior is as follows:
If a DELETE request is made without specifying an object version ID,
AWS S3 adds a delete marker, which cross-region replication repticates to the destination
bucket.
If a DELETE request specifies a particular object version ID to delete,
• AWS S3 deletes that object version in the source bucket, but it does not replicate the
deletion in the destination bucket
It does not delete the same object version from the destination bucket
» This behavior protects data from malicious deletions.
[7:57 AM, 2/5/2020] Manish Agrawal: S3 Cross Region Replication – What is NOT replicated
AWS S3 will not replicate objects that existed before you added replication configuration
You can use Copy API to copy existing bucket data
- Changes to existing objects that happen after CRR is enabled, are replicated
Objects created with SSE-C or SSE-KMS are not replicated
Amazon S3 does not keep the encryption keys you provide after the object is created in the
source bucket so it cannot decrypt the object for replication, and therefore it does not
replicate the object
If the object owner is different from the source bucket owner, and bucket owner does not have
permissions to the object, AWS S3 will not replicate these objects
Updates to bucket-level sub-resources are not replicated
This allows you to have different bucket configurations on the source and destination buckets.
[7:58 AM, 2/5/2020] Manish Agrawal: S3 Cross Region Replication – What is NOT replicated
53-IA
Only customer actions are replicated
Actions performed by lifecycle configuration are not replicated.
For example, if lifecycle configuration is enabled only on your source bucket, Amazon
S3 creates delete markers for expired objects, but it does not replicate those markers.
• However, you can have the same lifecycle configuration on both the source and
destination buckets if you want the same lifecycle actions to happen to both
DOLFINe
buckets.
Objects in the source bucket that are replicas, created by another cross-region replication,
are not replicated.
[7:58 AM, 2/5/2020] Manish Agrawal: S3 Cross Region Replication - Charges
You are charged for :
- Requests for upload
Inter-region data transfer
Storage at the destination bucket
DOLFINd
=======================================
########## v24

[9:07 AM, 2/5/2020] Manish Agrawal: S3 Cross -Origin Resource Sharing (CORS)
Is a way by which a client web application (webpage or a domain) can request resources from
another webpage (different domain)
CORS can be used to allow web applications to access resources on your S3 buckets/resources
As an example:
You can host web fonts on your S3 bucket, then configure your bucket to allow CORS requests
DOLFIN
for webfonts.
Other domains (web pages) will issue CORS requests to load webfonts from your S3
bucket
=======================================
########## v25

[9:07 AM, 2/5/2020] Manish Agrawal: S3 Transfer Acceleration
Is used to accelerate object uploads to S3 buckets from users over long distances
Typical use case is when uploading objects to your S3 bucket happens from users across the
world, over the internet
Transfer acceleration is as secure as direct upload to S3 over the internet
It utilizes AWS Cloudfront's edge location nearpst to the upload source (user), once data
arrives at the edge location, it gets routed to the destination S3 bucket over an optimized
network path
[9:08 AM, 2/5/2020] Manish Agrawal: S3 Transfer Acceleration
In order to use it:
Enable Transfer Acceleration on the S3 bucket
Once enabled, it can only be suspended, it can NOT be disabled
- Can only be done by the bucket owner, or a different user with permission
from the bucket owner
- After enabling it, it may take up to 30 mts before you notice upload speed
enhancements
DOLFINed
Point your PUT/GET requests to:
<bucketname>.S3-accelerate.amazonaws.com (for IPV4)
Bucket names must be DNS compliant, and MUST not have periods (.) in the bucket
name (between bucket name labels)
[9:08 AM, 2/5/2020] Manish Agrawal: S3 Transfer Acceleration
Health}
Transfer Acceleration is NOT HIPAA Compliant
Using Transfer acceleration incurs a charge
AWS checks for speed enhancements, and if no enhancement is provided, client does
not get charged for using Transfer Acceleration
No data is saved at Cloudfront edge locations (not cached)
DOLF
You can use multipart uploads with Transfer Acceleration
[9:08 AM, 2/5/2020] Manish Agrawal: S3 Transfer Acceleration – Speed Comparison Tool
You can use it to compare accelerated and non-accelerated upload speeds across amazon S3
regions
Uses multipart upload from your browser to multiple AWS, regions, and demonstrates speed
enhancements
DOLEFIN O
=======================================
########## v26

## S3 Intensive GET/ PUT
S3 maintains an index of object key names in each region
Object keys (names) are stored in an order across multiple partitions of this index
The object key name dictates which partition the key is stored in
Sequentially named objects are more likely to be saved in the same partition
- At high request rates to access sequentially named objects, S3 will place a higher load
on the available I/O of the partition hosting these keys (names) which will impact
performance

## S3 Intensive GET / PUT
If your bucket access (workload) involves only occasional bursts of 100 PUT/LIST/DELETE
requests per second, and less than 800 GET requests per second, no further actions are
required to optimize your S3 read/write performance
If your bucket access routinely exceeds 100 PUT/LIST/DELETE requests per second, or 300
GET requests per second, then you need to:
For PUT/LIST/DELETE requests, Introduce prefix randomization in your objects' key
names such that they will be stored in different partitions
For intensive GET requests,
• Introduce prefix (name) randomness
• And use CloudFront distributions to distribute the objects and offload S3
This will also decrease yours3 request charges, since CloudFront will cache
and distribute the objects/content

## S3 Intensive GET / PUT
For steadily increasing request rates, S3 can adjust
For rapidly increasing request rate where a bucket receives more than 300 PUT/LIST/DELETE
requests per second, or more than 800 GET requests per second,
Open a support case with AWS,
• This will allow to prepare for the workload and avoid any temporary limits on your
request rate
DOLFINed

=======================================
########## v27

## Billing Charges in S3
No charge for data transferred from/to EC2 to/from S3 in the same region
This includes data transferred via Copy command
Data transfer into S3 is free of charge
Data transfer out of S3 to EC2 instance in the same region is free
Data transferred out of S3 to EC2 in other regions is charged_
Data transfer via Copy to other regions is charged at the Internet data transfer rates
Data transferred from S3 to Cloudfront is free

## Billing Charges in S3
You pay for:
Per GB/Month storage fee
Data transferred in/out of S3 (as explained in the last slide)
Upload requests($0.05/1000 requests)
Both PUT and GET requests are charged
Retrieval request fee for S3-IA and Glacier
DOLFINed

## Requester Pays
If Requester pays is enabled on a bucket:
The bucket owner will only pay for Object Storage fees
The requester will pay for:
• Requests to S3 to upload/download objects
• Data transfer charges
403
Buckets with Requester Pays enabled do not allow anonymous access & Do not support
BitTorrent
DOLFIN
Requester pays buckets can not be the target for user logging
You can NOT specify requester pays at the Object level
Can be enabled from the bucket properties under the AWS Console


=======================================
########## v28

## S3 & Event Notification
> When certain bucket events occur, AWS S3 Can be configured to automatically send notifications to one of the following AWS Services:
- SNS Topic(s)
- SQS queue
- or AWS Lambda function(s)

> This is a bucket level configuration, and you can configure multiple events as required
> You need to configure SNS, SQS, Lambda before configuring S3 event notifications
> No extra charge is incurred for enabling event notifications to your bucket, however, SNS, SQS, Lambda charges apply

> You can set notifications for, Create object, Object delete, Object Delete marker created.. and many other bucket related events


## S3 Monitoring via CloudWatch
With AWS CloudWatch you can monitor multiple metrics.
CloudWatch can take actions based on a single metric
For S3, metrics that can be monitored by CloudWatch include:
S3 Requests, Bucket Storage, Bucket Size, All requests, HTTP 4XX, 5XX errors among
others
DOLFINe

## S3 Monitoring via CloudWatch
You can configure filters to filter CW data, these are called CloudWatch Dimensions
You can filter by bucket name, storage type, prefix, tag
Daily CloudWatch, Bucket-level storage metrics are turned on by default at no additional
cost.
1 minute CloudWatch metric can be configured both at the bucket and object levef
You can have up to 1000 metric configurations per bucket
DOLFING

## S3 API log monitoring via CloudTrail
CloudTrail captures all API requests made to S3 API
By default, AWS CloudTrail logs bucket level actions, however you can configure it to log the
object level actions (such as DELETE, GET, PUT, POST object events)
CloudTrail delivers these logs to a S3 bucket that you configure
CloudTrail collected information includes: FINE
Who made the request
When it was made
For what.etc
=======================================
######### AWS STORAGE GATEWAY #########


> AWS Storage Gateway is a service that CONNECTS AN ON-PREMISES SOFTWARE APPLIANCE WITH CLOUD-BASED STORAGE TO PROVIDE SEAMLESS AND SECURE INTEGRATION between an organization's on-premises IT environment and AWS's Storage infrastructure. The service enables you to securely store data to the AWS cloud for scalable and cost-effective storage.

> AWS STORAGE GATEWAY'S SOFTWARE APPLIANCE is available for download as a virtual machine (VM) image that you INSTALL ON A HOST IN YOUR DATACENTER.

> Storage gateway supports either VMware ESXI or Microsoft Hyper-V. 
- Once you have installed your gateway associated with your AWS account through the activation process, you can use the AWS management console to create the storage gateway option that is right for you.

##### Types of Storage Gateway #####
1. File Gateway (NFS)
2. Volumes Gateway (ISCSI)
   2a. Stored Volumes
   2b. Cached Volumes
3. Tape Gateway (VTL)

#### 1. File Gateway #####
> Files are stored as object in S3 Buckets, accessed through a Network File System (NFS) mount point. 
- Ownership, permissions and timestamps are durably stored in S3 in the user-metadata of the object associated with the file. 
- Once objects are transferred to S3 they can be managed as native S3 objects, and bucket policies such as versioning, lifecycle management and cross region replication apply directly to objects stored in bucket.

##### 2. Volume Gateway #####
> A volume gateway provides cloud-backed storage volumes that you can mount as Internet Small Computer System Interface (ISCSI) devices from your on-premises application servers. The gateway supports the following volume configurations:

2a. Cached volumes 
- YOU STORE YOUR DATA IN S3 AND RETAIN A COPY OF FREQUENTLY ACCESSED DATA SUBSETS LOCALLY. 
- Cached volumes offer a substantial cost savings on primary storage and minimize the need to scale your storage on-premises. 
- You also retain low-latency access to your frequently accessed data.

2b. Stored volumes 
– If you need low-latency access to your entire dataset, first configure your on-premises gateway to store all your data locally. Then ASYNCHRONOUSLY BACK UP POINT-IN-TIME SNAPSHOTS OF THIS DATA TO AMAZON S3. 
- This configuration provides durable and inexpensive offsite backups that you can recover to your local data center or Amazon EC2. For example, if you need replacement capacity for disaster recovery, you can recover the backups to Amazon EC2.

###### 3. Tape Gateways ######
> With a tape gateway, you can cost-effectively and durably archive backup data in Amazon Glacier. A tape gateway provides a virtual tape infrastructure that scales seamlessly with your business needs and eliminates the operational burden of provisioning, scaling, and maintaining a physical tape infrastructure.

> You can run AWS Storage Gateway either on-premises as a VM appliance, or in AWS as an Amazon Elastic Compute Cloud (Amazon EC2) instance. You deploy your gateway on an EC2 instance to provision ISCSI storage volumes in AWS. Gateways hosted on EC2 instances can be used for disaster recovery, data mirroring, and providing storage for applications hosted on Amazon EC2.

### Summary
• File Gateway – For flat files, stored directly on S3.
• Tape Gateways – Used for backup and uses popular backup applications
like NetBackup, Backup Exec, Veear etc.
• Volume Gateway:
• Stored Volumes – Entire Dataset is stored on site and is
asynchronously backed up to S3.
• Cached Volumes – Entire Dataset is stored on S3 and the most
frequently accessed data is cached on site.
