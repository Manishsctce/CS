=======================================
######## AWS COMPUTE SERVICES #########

#### Elastic Compute Cloud(EC2)
#### AWS LAMBDA
#### ELASTIC BEANSTALK
#### AWS ECR
#### AWS ECS
#### AWS Fargate
#### AWS Lightsail
=======================================
##### Elastic Compute Cloud(EC2) ######  

> AWS EC2 provides scalable computing capacity in the AWS cloud

> we can use EC2 to launch as many Virtual servers as we need, configure security and networking and manage storage. 

> it enables us to scale up or scale down the instance

> it is having 2 storage options i.e EBS & instance store 
> Preconfigured templates are available known as Amazon Machine Image. 

> By default, we can create max 20 EC2 instance per region per account.
- we can submit request to AWS to increase it

> There is some limit of vCPC, if we want to use vCPU beyond this then we need to submit request to increase it with our usecase.

> we have root access to each EC2 instance 
> we can stop, restart, reboot or terminate our EC2 instance
> EC2 availability SLA is 99.95% for each region during a month
- here 0.05% i.e. 22min per month, EC2 instance might not be available

> we can provision EC2 instance on shared or dedicated hosts(i.e physical servers) 

##### EC2 INSTANCE ACCESS #####

## How to Access EC2 Instance?

> To access an instance we need a key and key-pair name 
- when we launch a new EC2 instance, we can create a public/private key pair 
- we can download the private key only once 
- the public key is saved by AWS to match it with key-pair name and private key when we login to EC2 instance 
- if we launch instance without a key pair, we will not be able to acess it(via RDP/SSH) 
-------------------------------
> In order to access EC2 instance from the Internet, you need to have:

- An Internet Gateway (IGW) attached to the VPC.
- A route entry to the Internet gateway in the Route table of the VPC.
- A Public IP address attached to the EC2 instance.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#### EC2 - ROOT/BOOT VOLUME ###

> 2 types of Block store devices are supported: 

1. Elastic Block store (EBS) (Bootable)
- Persistent mean if we stop EC2 then data will not lost
- Network attached virtual drives i.e. it is not part of EC2

2. Instance store (Bootable)
- Ephimeral volume i.e. Not Persistent mean if we stop/terminate EC2 then data will lost
- Basically the virtual hard drive on the host allocated to this EC2 instance 
- Limited to 10GB per device  
- Instance-store backed EC2 instance has an instance-store root volume 

=====================================
####### EC2 instance families #######

###### 1. General Purpose #######
- Balanced memory and CPU 
- suitable for most application
- Ex- M3, M4, T2 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###### 2. COMPUTE OPTIMIZED #####
- More CPU than memory 
- Compute & HPC intensive use 

3 types : C4, C5, C5n 

##C4 
vCPU		: 2 to 36, 
RAM			: 3.75 to 60GB, 
Storage		: EBS only, 
N_Bandwidth	: 10Gbps 

##C5 
vCPU		: 2 to 72, 
RAM			: 4 to 192GB, 
Storage		: EBS only & NVMe SSD, 
N_Bandwidth	: 25 Gbps 

> C5 use Elastic Network Adapter 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###### 3. MEMORY OPTIMIZED #####
- More RAM/ memory 
- Memory intensive apps, DB and caching 
- R/X/Z series 

## R4/R5/R5a/R5ad/R5d 
- High performance
- suitable for both relational and No-sql DB 
- Used in financial service, Hadoop 
vCPU		: 2 to 96
RAM			: 16 to 768GB
Storage		: EBS only & NVMe SSD
N_Bandwidth	: 

## X1/X1E 
> Well suited for high performance DB, Memory intensive enterprise app, relation DB, workload SAP HANA, 
- Electronic design automation 
vCPU		: 4 to 128
RAM			: 122 to 3904GB
Storage		: Instance storage & NVMe SSD

## Z1d 
> all CPU core frequency is upto 400GHz, fastest of any cloud instance 
> It use Nitro System, Xeon processor, 
vCPU		: 2 to 48
RAM			: 16 to 384GB
Storage		: NVMe SSD
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#### 4. GPU compute instance ####

> Graphics optimized 
> High performance and parallel computing 

## F1 
Field programmable gate arrays (FPGA) 
- each FPGA contain 25million logic element(i.e gate) and 6800 DSP engines 
> Design to accelerate computational intensive algorithm, such as data flow or highly parallel operations 
> Used in Geronomic research, financial analytics, real time video processing & Big data. 

vCPU		: 8 to 64
FPGA 		: 1 to 8
RAM			: 122 to 976GB
Storage		: NVMe SSD

## P2/P3 
> It use NVIDIA Tesla GPU 
> provide high bandwidth networking
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###### 5. Storage OPTIMIZED #####
> Very High, low latency I/O
> I/O intensive apps, data warehousing, Hadoop

=====================================
##### EC2 EBS OPTIMIZED instance #### V10

> EBS optimized EC2 instances enable the full use of an EBS volume provisioned IOPS 
- They deliver dedicated performance between EC2 instance and their attached EBS volumes
- Are designed to work will all EBS volume types 
- This is all about high performance data transfer between EC2 instance and their attached EBS volumes

=======================================
###### EC2 Placement Groups(PG) ####### V11
> It is a logical grouping (clustering) of EC2 instance in the same AZ or in different AZ with the goal of providing low latency, and high network throughput for inter-instance communication 

> A placement group, determines how instance are placed on underlying hardware. 

> 3 strategies to create placement group :
1. CLUSTERS INSTANCES into a low-latency group in a single AZ. 
2. SPREAD INSTANCES across underlying hardware in multiple AZs 
3. PARTITION PG 
- spead instances across many different partition(which rely on different sets of racks) within AZ. scales to  100 EC2 instance per group


> Use SR-I/OV(Single Root I/O Virtualization) based enhanced networking instance for PG

> To guarantee availability, try to launch all required instance at the same time(Recommended)

> we can create PG across VPC peering 
> There is no charge for creating PG(except EC2 instance)
> PG NAME MUST BE UNIQUE WITHIN AN ACCOUNT FOR THE REGION
> We can use different instance type with PG however this is not recommended for closure PG
> we cannot merge two PG

> you can 
- move an existing instance to a PG 
- move an instance from one PG to another 
- remove an instance from a PG

> An instance cannot be launched in multiple placement groups at the same time

> Instances inside a PG can address each other using private or public IP addresses 
- best performance is achieved when they use private IP addresses for intra placement group communication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## CLUSTER PLACEMENT GROUP(CPG) 

> It is within a single AZ and cannot span multiple AZ
- all EC2 instance are in same rack and same AZ

> It is recommended when app require low network latency, high network throughput or both, and 
- if the majority of the network traffic is between the instance in the group 

> To provide the lowest latency and the highest PPS network performance for PG, choose instance type that support enhanced networking (SR-I/OV)  

> It is always RECOMMENDED TO LAUNCH THE PG INSTANCE AT THE SAME TIME 
- if you try to add instance to PG, and you can't due to availability reason, try to stop and start all instances.
- This may result in migration to other hosts that have availability of the specific instance types requested for the group 

> TRY TO AVOID LAUNCHING MORE THAN 1 INSTANCE TYPE IN THE PG (although possible), we increase our chances of getting an insufficient capacity error. 

> if we stop an instance in a PG and then start it again, it still runs in the PG. 
- However the starts fails if there isn't enough capacity for the instance. 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## SPREAD PLACEMENT GROUPS(SPG) 
> It is a group of instances that are each placed on distinct underlying hardware
- diff AZ and distinct hardware

> SPG are recommended for app that have small number of critical instances that should be kept separated from each other
- Launching instances in a SPG reduces the risk of simultaneous failure that might occur when instance share the same underlying hardware
- spread placement group provide access to distinct hardware and are therefore suitable for mixing instance type or launching instances over time

> SPG CAN SPAN MULTIPLE AZ
-- we can have a MAX OF 7 RUNNING INSTANCES PER AZ PER GROUP

> if you start on launch and instance in a SPG and there is insufficient unique hardware to fulfill the request request fails 
- you can try your request again later

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> If you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again. Restarting the instances may migrate them to hardware that has capacity for all the requested instances.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## PARTITION PLACEMENT GROUPS 

> upto 7 partition per AZ
- upto 100 EC2 instances 

> instances in a partition donot share racks with the instances in the other partitions.
- a partition failure can affect many EC2 but won't affect other partition
- EC2 instance get access to the partition information as metadata. 
- Use-case : HDFS, HBase, Cassandra, Kafka.
=======================================
########## EC2 STATUS CHECKS ########## V12 

> By default, EC2 perform status check EVERY 1 MINUTE automatically
- It is checked on running EC2 instance to identify any hardware or software issues
- it returns either pass or failure status
- If one or more status check return a fail, the overall EC2 instances status is changed to "impaired"

> once EC2 instances status changes to impaired because of host hardware/software problem, 
- AWS will schedule stop/start for EBS backed instances to relocate them to a different host 
- you can also do this manually

> Status checks are built-in EC2 services
- They cannot be configured, deleted, disabled or changed

> You can configure cloudwatch to initiate actions like reboot or recover on impaired EC2 instances (ie. for failure status check)

> EC2 service status checks are very important for ASG too, to determine EC2 instance status

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
######## EC2 Monitoring #########

> EC2 SERVICE SEND ITS METRIC DATA TO CLOUDWATCH EVERY 5 MINUTE(enable by default)
- This is free of charge 
- it is called basic monitoring

> you can choose to enable detailed monitoring while launching the instance (or later) where the EC2 will send it metric data to AWS cloudwatch every 1 minute 
- chargeable 
- it is called detailed  monitoring

> You can set cloudwatch alarm action on EC2 instance to 
- stop, restart, terminate or recover EC2 instance 
- we can use stop or terminate actions to save cost 
- we can use the reboot and recover to move EC2 instance to another host

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
######### EC2 States ########## v13
> When we launch instance, it goes through pending then running state 
- moving to running state mean, instance has started booting 
- then the instance receive a private DNS hostname, and possibly a public DNS hostname (depends on whether it is configured to receive a public IP) 

> When you launch an EC2 instance into a default VPC, AWS provides it with public and private DNS hostnames that correspond to the public IPv4 and private IPv4 addresses for the instance.
- However, when you launch an instance into a non-default VPC, AWS provides the instance with a private DNS hostname only. 
- New instances will only be provided with public DNS hostname depending on these two DNS attributes: 
1. the DNS resolution and 
2. DNS hostnames, 
- that you have specified for your VPC, and if your instance has a public IPv4 address.

> new EC2 instance does not automatically get a DNS hostname because the DNS resolution and DNS hostnames attributes are disabled in the newly created VPC.

> if you reboot EC2 instance, it is considered as running and does not add additional hour for bill 
- stopping and restarting an instance adds an hour to bill
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###### EC2 - STOPPED STATE ######
> When you stop an instance AWS shuts it down.
> Stopped instance maintain its instance ID and root volume

>> INSTANCE-STORE BACKED INSTANCE CANNOT BE STOPPED they can only be reboot and terminate

> We are not charged for EC2 instance if they are stopped, however attached EBS vol incur charges

> For stopped EC2 instance we can detach/re-attach their EBS vol including root vol
- when detached, we can attach it to another instance modify it and then re-attached it again to the stopped instance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##### STOPPING EC2 INSTANCE #####

> WHEN WE STOP AN EBS-BACKED INSTANCE(ROOT VOL), ANY DATA IN INSTANCE-STORE VOL IS LOST 
- despite the fact that the instance can be restarted, all instance store will be gone

> When you stop an EBS-backed EC2 instance
- Instance perform on shutdown 
- state change from running->stopping->stopped 
- EBS volume remain attached to the instance 
- any data cache in RAM or instance store volume is gone 
- most probably when restarted again it will RESTART ON A NEW PHYSICAL HOST 
- instance retain its private IPv4 address, any IPv6 address
- INSTANCE RELEASE ITS PUBLIC IPv4 address back to AWS pool
- instance retain its elastic IP address

> if instance was registered with ELB, it is recommended that de-register it from ELB, such that the ELB will stop healthcheck to the instance 
- you can re-register it later when used restart

> If instance was part of ASG, the ASG would mark it when stopped, as unhealthy, terminated it and replace it
- if you do not want this to happen you better remove/detach the instance from ASG before stopping it 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
######### EC2 REBOOT ########
> Rebooting an EC2 instances does not cause a new hourly bill

> best practice is to use EC2 reboot and not the instance OS reboot because
1. AWS when it initiates reboot, wait for 4 minutes, then if the instance did not reboot will force a hard reboot
2. AWS Reboot creates cloudtrail log which is used for forensic, troubleshooting and documentation/audit purpose
=======================================
###### EC2 - Instance Termination ##### v14

> When you terminate a running instance the instance states changes as follows
Running -> Shutting down -> Terminated
- During the Shutting down and Terminated states you do not incur charges
- you will not billed if it is preparing to stop however, you will still be billed if it is just preparing to hibernate.

> By default, EBS root vol. (created automatically when the instance is launched) are deleted automatically when the EC2 instance is terminated

> ANY ADDITIONAL NON-ROOT VOL. ATTACHED TO EC2 PERSIST AFTER THE INSTANCE IS TERMINATED BY DEFAULT,
- here attach vol. to the instance can be during launch or later

> You can modify both behaviors by modifying the "DeleteOnTermination" attribute of any EBS volume during instance launch or while running

> You can view EBS root volume DeleteOnTermination, behavior from "Block Device Mapping"

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
### EC2 TERMINATION PROTECTION ##

> This is a feature we can enable such that an EC2 instance is protected against accidental termination through API, Console, or CLI) SDK
- This can be enabled for both Instance-backed and EBS-Backed Instances
- CloudWatch can ONLY terminate EC2 instances if they do not have the termination protection

> If you want to terminate an instance that has termination protection turned on, you can do so by choosing OS shutdown, and configure AWS to treat OS shutdown as instance termination.

> This can be configured during launch, when the instance is running or stopped (if EBS backed instance)	
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Instance Immediate Termination

> AWS recommends that after you launch an EC2 instance, you check its status to confirm that it moved from pending to running, and not to a terminated state.

> Possible reasons that a launched instance immediately terminates are:
- The instance store-backed AMI you used to launch the instance is missing a required part.
- You've reached your EBS volume limit.
- An EBS snapshot is corrupt.

> To find the reason of the termination:
- From AWS Console : Go to Instances (select the instance) -> Description tab -> State Transition reason
- From CLI use the "describe-instance" command

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
### EC2 - Instance Meta Data ## v15

> Instance metadata can be use to configure or manage the instance
Example: IPv4 addr, IPv6 addr, DNS Hostnames, AMI-ID, Instance-ID, Instance-Type, Local-hostname, Public Keys, Security groups

> Metadata can be only viewed from within the instance itself

> METADATA CAN NOT PROTECTED BY ENCRYPTION, anyone that has access to the instance can view this data.

> To view EC2 Instance Metadata (from EC2 instance console):
GET http://169.254.169.254/latest/meta-data/
Curl http://169.254.169.254/latest/meta-data/

> To view specific metadata parameter, example to view local hostname
GET http://169.254.169.254/latest/meta-data/host-name/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
### EC2 - Instance Userdata ### v15

> It is data supplied by the user at instance launch in the form of a script to be executed during the instance boot

> USERDATA IS LIMITED TO 16KB

> Userdata can only be viewed from within the instance itself (logon to it)
> we can change user data, we need to stop instance first (EBS backed)
  Instance -> actions ->Instance-settings -> View/Change user data

> User data is not protected by encryption, do not include passwords or sensitive data in your user data (scripts)

> You are not charged for requests to read user data or metadata
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#### EC2 - VM Import/Export ### v16 

> It can be used to migrate Vmware, Microsoft, XEN VMs to the cloud (Import)

> It can be used to convert EC2 Instances to Vmware, Microsfot, XEN VMs to use on-premise (Export)

> This supports:
- Windows and Linux VMs
- Vmware ESX VMDK [Virtual Machine Disk] (and OVA [Open Virtualization Appliance] images for export only)
- Citrix XEN VHD [Virtual Hard Disk]
- MicroSoft Hyper-V VHD

>> VM Import/Export is supported through API / CLI, but NOT through AWS Console

> Before generating the VMDK or VHD images, MAKE SURE THE VM IS STOPPED AND NOT IN SUSPENDED OR PAUSED STATES

> For vmware, AWS HAS A VM CONNECTOR which is a plugin to vmware vCenter
- This allow the migration of VMs to AWS S3
- Convert it to EC2 AMI
- And progress can be tracked in vCenter

### EC2 - IAM Roles
> For an EC2 instance to have access to other AWS services (example S3) you need to configure an IAM Role, which will have an IAM policy attached, under the EC2 instance.
=======================================
############ BASTION HOSTS ############ v17

> Also know as Jumping box OR Jumping stone OR 'Remote Desktop for Windows Instances'

> It is an EC2 instance, whose interfaces will have SG allowing inbound SSH/RDP
- Using SG, we can limit which IP CIDR can access the Bastion Host.
- It is recommended to use a small instance because this host will only act as a jump server

> IT IS USED TO MANAGE AND ADMINISTER PUBLIC/PRIVATE EC2 INSTANCES
- It make secure connectivity to VPC

> It CAN HAVE AUTO-ASSIGNED PUBLIC IP/ELASTIC IP 
- Elastic IPs are better for security reasons and to fix the IP address bcz it get re-associated with the new instances(if launch by ASG) 

> It is possible to use SSH with an ordinary user ID and a pre-configured password as credentials but 
- it is more secure to use public key pairs for SSH authentication for better security.

> Once logged to the Bastion host, we can connect via RDP/SSH to the EC2 instance(s) we desire to manage

> To configure High Available bastion host, we can use ASG as follows:

1. Create ASG with desired capacity of 2, choose multi-AZ(2), using Elastic IPs on each (This is the recommended HA way)
- we can configure number of BH instance at launch 

2. Not an HA but saves on cost. Create an ASG with desired capacity 1, min 1, max 1, such that if the bastion instance fails/or gets terminated, the ASG will launch another one.
- until ASG launches another one, you may have a down time (this is for management/administration ), where downtime is acceptable
=======================================
######## EC2 PURCHASE OPTIONS ######### v18
 
1. ON-DEMAND INSTANCES 
– Pay-by-second, for the instances that we launch.

2. RESERVED INSTANCES 
– Reduce EC2 costs by making a commitment to a consistent instance configuration, including instance type and Region, for a term of 1 or 3 years.
- It has 2 offering 
  1. Standard RI - we can only modify AZ, Instance size(for Linux), Networking type and can sold in RI marketplace. 
  2. Convertible RI - we can modify most of the EC2 detail but cannot sold in RI marketplace

2B. Scheduled Reserved Instances (Scheduled Instances) 
- it enable us to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. 

3. Spot Instances 
– Request unused EC2 instances, which can reduce your Amazon EC2 costs significantly.

4. Dedicated Hosts 
– Pay for a physical host that is fully dedicated to running your instances, and bring your existing per-socket, per-core, or per-VM software licenses to reduce costs.
- we can control number of cores allocated to the application

5. Dedicated Instances 
– Pay-by-hour, for instances that run on single-tenant hardware.

Savings Plans – Reduce your Amazon EC2 costs by making a commitment to a consistent amount of usage, in USD per hour, for a term of 1 or 3 years.



Capacity Reservations – Reserve capacity for your EC2 instances in a specific Availability Zone for any duration.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##### 1. ON-DEMAND INSTANCE ###
> These EC2 instances are purchased at a fixed rate
- you pay for what you use. 
 
> using this instance, we are free from costing complexity like planning, purchasing and maintaining.
> No pricing for terminated or stopped instance.
 
# SCENARIO
- use it for app with short term irregular workload that cannot be interrupted.
- suitable for testing and development of app on EC2.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#### 2a. DEDICATED INSTANCE ###
> It run in a VPC on hardware that is dedicated to a single customer.

> it may share hardware with other instances from the same acct that are not dedicated instance.

> Pay for dedicated instances on-demand save upto 70% by purchasing reserve instances, or save upto 90% by purchasing spot instances. 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##### 2b. DEDICATED host ######
> it is a physical server with EC2 instance that is fully dedicated to a customer.
> it helps to address compliance requirement and reduce cost by allowing existing server bound software licenses.
> pay for physical host and bring ur existing per-socket, per-core, per VM software license to reduce code.

16vCU 	 - can create 4 * 4vCPU instances 
64GB RAM - can create 4 * 16GB RAM instances
 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
##### 1. RESERVED INSTANCE(RI)
> we do not actually buy Instances, rather, we reserve long term capacity and pay for it for instance family/configuration, anytime we run (in that AZ or region) an on-demand instance that matches the reserved one, reserve pricing will apply to the on-demand instance

- You can purchase it at a significant discount
- Purchased reserved instance are always available

> When purchased with an AZ scope, capacity reservation in the AZ is guaranteed
> Term options are 1 year or 3 years

> Once purchased, it can NOT be refunded or Cancelled
- You can however, sell them on AWS reserved instance marketplace
- Only the ones with AZ scope can be sold on the Marketplace

> You are billed for reserved instance, whether it is running or stopped

> Reserved instances DO NOT RENEW AUTOMATICALLY when the reserved term expires, rather, it will be billing as on-demand billing rates

> You have no control over which EC2 on-demand instance will have the reserved instance prices applied to

> Reserved Instance benefits can only apply to on-demand instances (not Spot or Dedicated)

> By default, a Reserved Instance scope is Region, however, it can be AZ per Region
- If AZ, you get guaranteed reserved capacity in the AZ You can also sell the reservations with AZ scope on AWS RI market place
- If the scope Region, you do not get reserved capacity guarantees You can change from Scope AZ to Scope Region

> It has 2 offering 
  1. Standard RI - we can only modify AZ, Instance size(for Linux), Networking type and can sold in RI marketplace. 
  2. Convertible RI - we can modify most of the EC2 detail but cannot sold in RI marketplace

#### Modifying EC2 Reserved Instances ####
> What you can change is:
- Change AZ within the same Region (keep its scope as(AZ but for a different AZ)
- This also means you can migrate RI bet AZs in the same region
- Change the scope from AZ to a Region or the other way around

- Change instance size within the same instance family
- You can modify all or a subset of your reservation

> You need to submit a request for the desired modification, through AWS consate, CLI, or API
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###### EC2 SPOT INSTANCES #######
> AWS Spot Instances allow customers to use compute capacity without upfront commitments, AT CHEAPER PRICES than on-demand instance pricing
- CUSTOMERS BID ON SPOT INSTANCES. AWS Off-peak pricing fluctuates, if the price meets the bid price, the instances are allocated for the bidding account
- IT MAY TERMINATED AT ANYTIME by AWS when the market prices goes higher than the previous bid price by the client (who got the instances)

> all Instance families are available for spot instances (T2 for example)

> Encrypted EBS volumes are supported in Spot instance
- If you specify in your launch configuration, it will be honored
=======================================
#### Elastic Network Interface(ENI) ### v19

## How multiple IP addresses work with Network Interfaces

> Eth0 is the Primary network interface, 
- You can't move/detach the primary (eth0) interface from an instance

> By default, Eth0 is the only ENI created with an EC2 instance when launched

> An ENI is bound to an Availability Zone
- You can specify which subnet/AZ you want the additional ENI be added in
- You can specify exactly which IP address in the subnet to be configured on instances, or leave AWS assign one automatically from the available subnet IPs

> SG apply to network interfaces not to individual IPs on the interface, hence, IP addresses are also subject to the interface security group

## Default EC2 virtual network interface (eth0)

> You can create (optional) only one additional Ethernet interface (eth1) when launching an EC2 instance, 
- but you can create and attach more ENIs to the EC2 instance (the number of which depends on the Instance family/type)

> Attaching ENI when the instance is running is called "hot attach"
> Attaching ENI when the instance is stopped is called "warm attach"
> Attaching ENI when the instances is launched is called “cold attach"

## How to Add additional Network Interfaces at Instance Launch
> When launching an EC2 instance, after selecting your VPC and subnet, you will be able to "add a device", this is how you can add one more (eth1) interface

> CAUTION:
- you do this, AWS will no longer assign a public IPV4 address to your eth0 (primary network interface), and you will have to use an Elastic IP address mapped to your eth0 (manually) in order to be able to connect from the internet to your instance

## Elastic Network Interfaces (ENI)
> By default, network interfaces created "automatically" during EC2 instance launch by AWS console, and terminated when the instance is terminated,
- Does not include eth1 that you can add during launch (manually added)
> Network interfaces created by CLI are "NOT" terminated automatically when the Ec2 instance terminates.

> In both cases above, you can change the default behavior by changing the "termination behavior" from :
Instances -> Net Interfaces -> Change termination behavior
=======================================
####### ENI IP Address #########v20

## Virtual Elastic Network Interfaces (ENI) - Attributes
> Each Elastic Network Interface can have up to:
- A description
- One Primary IPV4 addresses
- One or more secondary IPV4 addresses
- One Elastic IP address corresponding to each IPV4 address (via NAT)
- One Public-IPv4 address (automatically assigned)
- One of more IPv6 addresses 
- Upto 5 Security groups
- A MAC address
- A source/destination check flag 

## Secondary IP addresses - Benefits
> You can configure secondary IPv4 addresses to your EC2 instance's Interfaces and ENIs

> Benefit of assigning multiple IP addresses to an EC2 instance in your VPC:
- Hosting multiple websites on a single server (multiple SSL certificates each associated with one IP address)

- Security and network appliances use in VPC
- Redirecting internal traffic to a standby EC2 instance in case primary EC2 instance fails,
- This can be achieved by moving (reassigning) the secondary IPv4-address from the failed instance to the standby 

## Secondary IPV4 address Re-Assignment
>> When you configure secondary IPV4 addresses, and if you allow re-assignment, they can be reassigned to another network interface (moved, transitioned)
- comes in handy in failure scenarios if your traffic was directed to the secondary IPV4 address

## Elastic IP and Secondary IPV4 address Re-Assignment
> Each private IPv4 address can be associated with a single Elastic IP address, and vice versa.


## Virtual Elastic Network Interfaces (ENIs)
> Any network interface can be assigned a secondary IPv4 address from the same subnet of the network interface or EC2 instance

> You can assign/remove IP addresses from EC2 instances ENI while they are running or stopped

> Detaching an ENI
- Except for Eth0, any interface can be attached/dettached to other instances
- Primary private IPv4, Secondary private IPv4, IPV6, and Elastic IP addresses they continue to be with the same network interface even when it is detached/attached to another instance

 
>> To attach a network interface in a subnet to EC2 instance in another subnet, they both "MUST" be in the same AWS Region and same AZ

> Network NIC teaming is not achievable by adding ENIS to an EC2 instance 

> The drawing shows how you can establish network management and public facing interfaces on the same instances using Eth0 and Eth1 ENIs
=======================================
##Q: You have just launched an EC2 instance, and get the following error message when you try to connect to it from a workstation running Windows 7.
Error: Server refused our key or No supported authentication methods available
What could be the reason for this error? 

There are two parts of the error message.
1. The first part is that ‘Server refused our key’

> Putty: This is a CLI (Command Line Interface) tool in Windows. 
- To log in to this Putty tool we need to have the keys in the Putty readable format which would have the extension of ‘.ppk’.
- There is a tool called ‘PuttyGen’ by which we can convert our ‘.pem’ key to the ‘.ppk’ format and 
- if we feed this key to the Putty tool we can log in to our respective instance.

> Load the ‘.pem’ key in the ‘PuttyGen’ tool and use the ‘Save private key’ in the ‘PuttyGen’ to save the private key in ‘.ppk’ format.
- Then load this key in the ‘Auth’ section of ‘Putty’ tool as shown below
- The username should be provided in the ‘Data’ section 

2. The second part of the error message is ‘No supported authentication methods available’.
- Please refer to the below table for the AMI and their corresponding usernames
1. Linux : ec2-user
2. Centos : centos
3. Debian : admin / root
4. Fedora : ec2-user / fedora
5. RHEL : ec2-user / root
6. SUSE : ec2-user / root
7. UBUNTU : ubuntu

############ EC2- SR-I/OV ########
> SR-I/OV mean Single Root I/O Virtualization

> SR-I/OV is a network interface (host interface) virtualization 
- whereby, a guest machine (EC2 instance) has direct access to VNI without hypervisor being in the middle (emulating a vNIC) 
- basically, virtualizing the network adaptor on the physical host 

> SR-I/OV
- Not supported on all instance types 
- it is supported on R4, X1, P2, C3/4, R3, I2, M4, d2 

###### ADVANTAGE #######
> For supported instance types, SR-I/OV provides: 
- higher PPS performance for data transfer
- Lower latency 
- very low network jitter 

> EC2 enhanced networking can be enabled on EBS-backed or Instance-backed instance 
> EC2 enhanced networking can function across Multi-AZ 

> To use enhanced networking, the EC2 instance need to 
- Support SR-I/OV
- Should be created form HVM(Hardware Virtual Machine) AMI
- be launched in a VPC (default)

> Using enhanced networking does not cost extra 


> An Elastic Fabric Adapter (EFA) is a network device that you can attach to EC2 instance to accelerate HPC and ML apps. 
- it enables you to achieve the app performance of an on-premises HPC cluster, with the scalability, flexibility, and elasticity provided by the AWS Cloud.
- it provides lower and more consistent latency and higher throughput than the TCP transport traditionally used in cloud-based HPC systems. 
- It enhances the performance of inter-instance communication that is critical for scaling HPC and ML apps. 
- It is optimized to work on the existing AWS network infrastructure and it can scale depending on app requirements.

> Elastic Network Adapters (ENAs) provide traditional IP networking features that are required to support VPC networking. 
- EFAs provide all of the same traditional IP networking features as ENAs, and they also support OS-bypass capabilities. 

> OS-bypass enables HPC and ML apps to bypass the OS kernel and to communicate directly with the EFA device.
- The OS-bypass capabilities of EFAs are not supported on Windows instances. 
- If you attach an EFA to a Windows instance, the instance functions as an ENA, without the added EFA capabilities.
=======================================
############# AWS LAMBDA ##############

## What is AWS Lambda?

- It is ADVANCE VERSION OF EC2
- It is a compute service that lets u run code without provisioning or managing servers.
> it is a compute service where we can upload our code and create the Lambda function.
- it takes care of provisioning and managing the servers used to run the code.
- While using Lambda, we don't have to worry about scaling, patching, OS etc.
- it runs code on a HA compute infrastructure.

> While working with Lambda function, if there is a need to access other resources, ensure that IAM role is in place. 

> it executes code only when needed and scales automatically, from a few requests per day to thousands of RPS.
> pay only for the compute time - No charge when code is not running.

> use the API gateway and integrate with AWS Lambda func which will call on HTTPS


##### IMPORTANT POINTS
> It support Node.js, Java, Python, C#, Go, Powershell, Ruby

> Lambda function can trigger other Lambda event
> Architectures can get extremely complicated, AWS X-ray allows you to debug what is happening

- Lambda can do things globally, you can use it to back up S3 buckets to other S3 buckets etc

> Trigger Lambda - API Gateway, CloudWatch Event/logs, DynamoDB, Kinesis, S3, SNS, SQS, App Load Balancer, CodeCommit
- Not Trigger Lambda - RDS 

## How Is Lambda Priced

> Number Of Requests
- First 1 million requests are free. 
- $0.20 per 1 million requests thereafter

> Duration
- Duration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 100ms. 
- The price depends on the amount of memory you allocate to your function. You are charged $0.00001667 for every GB-second used.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#### USE-CASE ####

> It can be used as an event-driven compute service where AWS Lambda runs our code in response to events. 
- These events could be changes to data in S3 bucket OR DynamoDB table.

> It can be used as a compute service to run our code in response to HTTP requests using Amazon API calls made using AWS SDKs.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## What is the benefit of Lambda?

1.  No Servers 
- we do not need to run our own server, Lambda will do everything for us. We just need to focus on our code.

2. Continuous scaling: 
- Lambda will automatically scale up or scale out.

3. Super super super cheap: 
- It is very cheap as you do not require a server.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## AWS Lambda - Triggers

You can use AWS Lambda to run your code in response to:
- Events such as changes to data in an Amazon S3 bucket or an Amazon DynamoDB table.
- To run your code in response to HTTP/HTTPS requests using Amazon API gateways.
- Invoke your code using API calls made using AWS SDKS.

## AWS Lambda - Trigger
> The user creates an object in S3 bucket.
- Amazon S3 detects the object created event.
- Amazon S3 invokes your lambda function using the permission provided by execution role.
- Amazon S3 knows which lambda function to invoke based on the event source mapping that the bucket is stored in configuration. notification

## AWS Lambda – Building Blocks
> Lambda Function
- The foundation, it is compromised of your custom code.

> Event Source
- An AWS Service, such as an Amazon SNS, or a custom service that triggers your functions and executes its logic.

> Downstream resources
- An AWS service, such as DynamoDB tables or Amazon S3 bucket, that your lambda function calls once it is triggered.

> Log Streams
- While Lambda automatically monitors your function invocations and report metrics to cloudwatch, you can annotate your function code with custom logging statement that allow you to analyze the performance of you lambda function to ensure its working properly.

## AWS Lambda Function Configurations
> You only specify the amount of memory you want to allocate for your lambda function.
> You can update the configuration and request additional memory in 64MB increments from 128MB to 3008MB.

> To prevent your lambda function from running indefinitely, you specify a timeout.
- When the specified timeout is reached, AWS lambda terminates your Lambda function.

> Maximum execution time of function is 300 Seconds and default is 3 seconds *

## AWS Lambda Function Configurations
• You can also setup AWS Lambda to invoke your code on regular,
scheduled basis using the AWS Lambda console.
• You can specify a fixed rate (number of hours, days or weeks) or you can
specify a cron expression.

## AWS Event Source Mapping
In AWS Lambda, Lambda Functions and event sources are the core components in AWS Lambda.
- An event source is the entity that publishes events, and a lambda function is the custom code that process the events.
- Event source mapping basically maps an event source to a lambda function.
- Each event source mapping identifies the type of events to publish and the lambda function to invoke when event occurs.
- The specific lambda function then receives the event information as a parameter, your lambda function code can then process the event.

> AWS Lambda is the FaaS (function as a service). 
- It has integrations with many other AWS services, including AWS DynamoDB and AWS API gateway. 
- Due to its popularity and support for multiple languages, AWS Lambda is a safe move for going serverless.
=======================================
########## ELASTIC BEANSTALK ##########

> With Elastic Beanstalk, we can QUICKLY DEPLOY AND MANAGE APP in the AWS Cloud WITHOUT HAVING TO LEARN ABOUT THE INFRASTRUCTURE that runs those appl. 
- It is a web app server
- It is example of PaaS

> It REDUCES MANAGEMENT COMPLEXITY without restricting choice or control. 
- we simply upload our app, and it automatically handles the details of capacity provisioning, load balancing, scaling, and app health monitoring.

> It supports app developed in Go, Java, .NET, Node.js, PHP, Python, Ruby, Go and Docker
- When we deploy app, it builds the selected supported platform version and provisions one or more AWS resources, such as EC2 instances, to run app.
- can provision most database instances
- allows full control of the underlying resources

> we can interact with Elastic Beanstalk by using the Elastic Beanstalk console, AWS CLI

> It is recommended for test or development apps Not for production.
> it use for long-running processes as worker process

> AWS Elastic Beanstalk provisions and configures all the AWS resources required to run and support your application. 
- For Amazon RDS database instance to be launched in the production environment, BEST PRACTICE IS TO LAUNCH DB OUTSIDE AWS ELASTIC BEANSTALK ENVIRONMENT. 
- It helps in having multiple environments connecting to a single database, using database types not supported with the integrated database, performing blue/green deployments. 
- Also, the database instance remains up & running when AWS Elastic Beanstalk environment is terminated. 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Elastic Beanstalk Components

1. Application:
- An application in Elastic Beanstalk is conceptually similar to a folder
- An application is a collection of components including environments, versions and environment configuration

2. Application Version:
- It refers to a specific, labeled iteration of deployable code for a web application
- IT POINTS TO S3 OBJECT THAT CONTAINS THE DEPLOYABLE CODE such as a Java WAR file

3. Environment:
- Environments within Elastic Beanstalk Application is where the current version of the app will be active
- EACH ENVIRONMENT RUNS ONLY A SINGLE APP VERSION AT A TIME. 
- But it is POSSIBLE TO RUN SAME OR DIFFERENT VERSIONS OF APP IN MANY ENVIRONMENTS at the same time

4. Environment Tier:
- Based on requirement beanstalk offers two different Environment tiers: Web Server Environment, Worker Environment
a. Web Server Environment: Handles HTTP requests from clients
b. Worker Environment: Processes background tasks which are resource consuming and time intensive

##
> It need ".ebextensions/*.config" file to run the app 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##### Logging & Monitoring ####
> Elastic Beanstalk stores application files and optionally, server log files in S3. 
- If we are using the AWS Management Console, AWS Toolkit for Visual Studio, or AWS Toolkit for Eclipse, an S3 bucket will be created in account and the files will be automatically copied from local client to S3. 
- Optionally, you may configure Elastic Beanstalk to copy server log files every hour to S3.
- You do this by editing the environment configuration settings.

> With CloudWatch Logs, you can monitor and archive Elastic Beanstalk app, system, and custom log files from EC2 instances of your env. 
=======================================
############# AWS ECR #################

> Elastic Container Registory
=======================================
############## AWS ECS ################
> It is Elastic Container Service
> It is HA, fast and container management service 
- it supports Docker containers and allows you to easily run apps on a managed cluster of EC2 instances.

> we can launch and stop container-based app, query the complete state of clusters with simple API call.

> It is a regional service.
> There is no additional charge for Amazon ECS. 
- You pay for AWS resources (e.g. EC2 instances or EBS volumes) 

> ECS lets you run batch workloads with managed or custom schedulers on EC2 On-Demand Instances, Reserved Instances, or Spot Instances.
- You can launch a combination of EC2 instances to set up a cost-effective architecture depending on workload. 

> we can create ECS cluster within a new or existing VPC.
- After cluster is up and running, we can define task definition and services that specify which docker container image to run across cluster.

> container image are stored in and pull from container registries, which may exist within or outside AWS infra.
- ECR is managed AWS docker registry service that is secure, scalable and reliable.

> ECR supports private docker repositories with resource-based permission using IAM so that specific users or EC2 instance can access repositories amd image

> Developers can use docker CLI to push, pull and manage images.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## CONTAINER MANAGEMENT SERVICE includes
- Deployment of Containers
- Redundancy and availability of Containers
- Scaling up or down of Containers
- Load Balancing
- Health Monitoring of Containers and Hosts
- Service Discovery
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
######### COMPONENTS ##########

> Containers are created from a read-only file called image.
> Image are build from a DockerFile (plain text file that specify all the components that are included in the container)

> Docker Volumes can be a local Instance store / EBS / EFS 
- connect docker container to these volume using docker drivers and plugin


## Task Components
> Task definitions specify various parameters for your application. 
- It is a text file, in JSON format, that describes one or more containers, up to a maximum of ten, that form your application.

> Task definitions are split into separate parts:

1. Task family 
- the name of the task, and each family can have multiple revisions.

2. IAM task role 
- specifies the permissions that containers in the task should have.

3. Network mode 
- determines how the networking is configured for your containers.

4. Container definitions 
- specify which image to use, how much CPU and memory the container are allocated, and many more options.

5. Volumes 
- allow you to share data between containers and even persist the data on the container instance when the containers are no longer running.

6. Task placement constraints 
- lets you customize how your tasks are placed within the infrastructure.

7. Launch type 
- determines which infrastructure your tasks use.

Example:
{
 "family":"webserver",
 "containerDefinitions":[{
	"name":"web",
	"image":"nginx",
	"memory":"100",
	"cpu":"99",
  }]
  "requiresCompatibilities":["FARGATE"],
  "networkMode":"awsvpc",
  "memory":"512",
  "cpu":"256"
}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Tasks and Scheduling

> A task is the instantiation of a task definition within a cluster. 
- After you have created a task definition for your application, you can specify the number of tasks that will run on your cluster.

- Each task that uses the Fargate launch type has its own isolation boundary and does not share the underlying kernel, CPU resources, memory resources, or elastic network interface with another task.

> The task scheduler is responsible for placing tasks within your cluster. There are several different scheduling options available.

- REPLICA - places and maintains the desired number of tasks across your cluster. By default, the service scheduler spreads tasks across Availability Zones. You can use task placement strategies and constraints to customize task placement decisions.

- DAEMON - deploys exactly one task on each active container instance that meets all of the task placement constraints that you specify in your cluster. When using this strategy, there is no need to specify a desired number of tasks, a task placement strategy, or use Service Auto Scaling policies.

> You can upload a new version of app task definition, and the ECS scheduler automatically starts new containers using the updated image and stop containers running the previous version.

> ECS tasks running on both Amazon EC2 and AWS Fargate can mount EFS
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
########## USE-CASE ###########

> ECS service can optionally be configured to use AS to adjust its desired count up or down in response to CloudWatch alarms. 
- AS is available in all regions that support Amazon ECS.

> It is possible to use Elastic Beanstalk to handle the provisioning of ECS cluster, balancing load, auto-scaling, monitoring, and placing your containers across your cluster.

> It is possible to associate a service on Amazon ECS to an Application Load Balancer (ALB) for the Elastic Load Balancing (ELB) service.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


=======================================
############ AWS Fargate ##############

> we can use Fargate with ECS to run container without having to manage server or cluster of EC2 instance
- doesn't require to provision, configure or scale cluster of VM to run container 

>> It only supports container images hosted on ECR or Docker Hub.
> currently it is not available on all regions.

#### FARGATE TASK DEFINITION REQUIRE

> NETWORK MODE = "awsvpc".
- it provide each task its own ENI.

> specify CPU and memory at the task level.
> it only support "awslogs" log driver for the log configuration.
- so that tasks can send log to cloudWatch logs.

> Task storage is ephemeral. 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
####### ECS vs FARGATE ########

> ECS - Need to mantain our own infrastructure like EC2, AS, ELB etc
- FARGATE - AWS manage infrastructure


=======================================
########### AWS Lightsail #############

> It is an easy way for developers to get started with a simple virtual private server (VPS) solution.

> It provides compute, storage, and networking capacity and capabilities to deploy and manage websites and webapps in the cloud. 

> Lightsail includes everything you need to launch your project quickly - a virtual machine, SSD-based storage, data transfer, DNS management, and a static IP.

> Lightsail is best suited to projects that require a few VPS and users who prefer a simple management interface.

> Common use cases for Lightsail include running websites, webapp, blogs, e commerce sites, simple software, and more.

> A Lightsail plan includes a virtual server with a fixed amount of memory (RAM) and compute (vCPUs), SSD based storage (disks), and a free data transfer allowance. 
- Lightsail plans also offer static IP addresses (5 per account) and DNS management (3 domain zones per account). 
- Lightsail plans are charged on an hourly, on-demand basis, so you only pay for a plan when you're using it.
=======================================