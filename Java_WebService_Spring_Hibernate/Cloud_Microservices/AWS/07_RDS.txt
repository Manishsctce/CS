######## AWS DATABASE SERVICE #########

## RDS - OLTP (Online Transaction processing)
- SQL server - doesn't support read replica
- Oracle 	 - upto 5 Replica
- PostgreSQL - upto 5 Replica, Highly Reliable & stable
- MariaDB	 - MySQL Compatible, can grow upto 64TB
- AWS Aurora - upto 15 Replica, High Throughput, can grow upto 64TB, replica lag < 100ms, support schema change
- MySQL 	 - upto 5 Replica, can grow upto 64TB
-------------------------------
## AWS Aurora
- It is fully managed by AWS
- upto 15 Aurora Replicas across AZs AND upto 5 MySQL Read Replicas across Azs
- Use Global DB, span into multiple AWS regions for fast local reads(latency <1sec) and quick DR(can promote to full RW capabilities in <1min).
> During failover, aurora flip CNAME of DB to healthy replica (which become new primary in 30sec)
- during failover with 1 instance(no replica), it will create new DB instance in same AZ
-------------------------------
## AWS DynamoDB 
- fully managed, multi-AZ, No SQL, with CRR, < 10ms latency, 400Kb max item size
- can provision RCU & WCU in provision mode(can scale multiple time but scale down 4time in 24hr) 
- primary key is required attribute for items in table. Primary key can be partitionKey, partitionKey + sortKey
- supports 2 type of secondary indexes for querying table: GSI (diff partitionKey + sortKey) & LSI (same partitionKey+diff sortKey).
- good for mobile, web, gaming, ad-tech, IOT
- DynamoDB Stream help to list item level changes in table in last 24hr and can associate with Lambda
- DAX provide in-memory cache, improve by 10x (from milli -> microsec)
- it can be auto-scale(enable by default, when create through console) using scaling policy for a table or GSI

> Keep item sizes small; use separate tables for days, weeks, monthly basis data; Store more/less frequently accessed data in separate tables.
-------------------------------
## AWS Redshift 
> fully managed, petabyte-scale DATA WAREHOUSE, OLAP, Columnar Data Storage, Adv. compression, Massive Parallel processing, used for BI
> REDSHIFT SPECTRUM - query to data stored in S3 like "S3 Select"
> REDSHIFT ENHANCED VPC ROUTING for accessing S3 endpoint
- For DR, automatically copy snaphots in another region when Cross-Region snaphots enable.
- can take manual/auto snaphots. Manual-SS are retained indefinitely in S3(can specify retention period). Auto-SS delete when cluster delete but manual doesn't.
- Default retention is 24hr for auto-backup, can vary from 0-35days
- 2 type of node: Single Node & Multi-node
- Encrypted in-transit using SSL and at rest using AES-256 encryption. can be encrypted with AWS KMS or HSM 
- Redshift currently SUPPORTS ONLY ONE AZ (no Multi-AZ option)
-------------------------------
## Amazon ElastiCache  - to speed up performance
- It is fully managed by AWS
=======================================
############## AWS RDS ################ v4

> It is fully managed relational DB engine service where AWS is responsible for below:
- Security and patching of the DB instances
- Automatic backup for DB instance (defaults setting)
- Software update for the DB engine
- Easy scaling for storage and compute as required
- If selected multi-AZ with synchronous replication between the active and standby DB instance
- Automatic failover if multi-AZ options was selected
- Providing the ability to create DB read replicas for DB read scaling( intensive free deployment)

> Every DB instance has a weekly maintenance window 
- if you do not specify one at the time you created the DB instance 
- it will choose one randomly for you that is 30 minute long

> RDS runs on virtual machines
- we cannot log into those OS 
- Patching of the RDS OS and DB is Amazon responsible

> RDS is NOT SERVERLESS
> AURORA SERVERLESS IS SERVERLESS

>> DB parameter groups act as a container for engine configuration values that are applied to one or more DB instances.

> You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token.
=======================================
## AWS is not responsible for
- Managing DB setting 
- building a relational DB schema 
- DB performance tuning

## Supported Relational DB engine
- SQL server - doesn't support read replica
- Oracle 	 - upto 5 Replica
- PostgreSQL - upto 5 Replica, Highly Reliable & stable
- MariaDB	 - MySQL Compatible, can grow upto 64TB
- AWS Aurora - upto 15 Replica, High Throughput, can grow upto 64TB, replica lag < 100ms, 
- MySQL 	 - upto 5 Replica, can grow upto 64TB

> All support Multi-AZ, 

## 2 Licensing model
1. Bring your own licence
2. Licence included

## RDS LIMITS
> upto 40 DB instances per account
- 10 of this 40 can be Oracle or MS SQL server under Licence included model

> Under BYOL model, all 40 can be any DB engine you need 

## RDS instance storage
> Amazon RDS USE EBS VOLUME (not instance store) for DB and logs storage

1. General purpose storage 
- it used for DB works with moderate I/O requirements
- Limits - 20GB (min) - 16TB (max)

2. Provision IOPS RDS storage 
- use for high performance OLTP workload
- Limits - 100GB (min) - 16TB (max)

> Magnetic RDS storage used for small DB workloads (No Any more)

## Maximum storage capacity for RDS DB instances
> Upto 4tb for my SQL server
> upto 6TB for the the rest of supported RDS DB engines

> Both Oracle and sqL Server db engines have limits to how many DB that can run per instance. 
- Primarily, this is due to the underlying technology being proprietary and reqairing specific licensing to operate. 
- The database engines based on Open Source technology such as Aurora, MYSQL, MariaDB or PostgresQL have no such limits.

=======================================
######### MULTI-AZ RDS OPTION #########

> Multi-AZ is for disaster recovery.
> You can select the multi-AZ option during RDS DB instance launch

> RDS service CREATE A STANDBY INSTANCE IN A DIFFERENT AZ IN THE SAME REGION and configure SYNCHRONOUS replication between the primary and standby
- we/app cannot read and write to the standby RDS DB instance

> you cannot dictate/select which AZ in the region will be chosen to create the standby DB instance 
- we can view which AZ is selected after the standby is created

> depending on the instance class, it may take 1 to few minutes to failover to the standby instance 
- It is recommended to implement DB connection retries in appl, 
- App may take few minute to switch to standby in case of failure.

> AWS RECOMMENDATION TO USE PROVISIONED IOPS INSTANCES FOR MULTI-AZ RDS INSTANCES
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###### FAILOVER TRIGGER #######

> Failover may be triggered when
- Loss of primary AZ or primary DB instance failure
- Loss of network connectivity to primary
- Compute(EC2) unit failure on primary
- Storage(EBS) unit failure on primary
- Primary DB instance is changed
- Patching the OS of the DB instance

- Manual failover(reboot with failover on primary)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## MULTI-AZ FAILOVER LOGISTICS

> during failover the CNAME of the RDS DB instance is updated to map to the standby IP address 
- this is why it is recommended to use endpoint to reference DB instance and not its IP addresses

> CNAME does not change because the RDS endpoint is not changed

> RDS endpoints does not change by selecting multi-AZ option
-  however the primary and standby instance will have different IP address given they are in different AZs
- Hence it is always recommended that do not use the IP address to point to your multi-AZ RDS instance rather use the endpoint

> This is very helpful in a failover scenario the primary will fail to the standby i.e. IP address of RDS instance will change
- Referencing by endpoint means there is no change(endpoint wise) when a failover happened, so no disruptions or manual intervention to facilitate RDS DB instance access in a failover scenario
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## MULTI-AZ RDS Manual failover and reboot

> In multi-AZ deployment, you can only initiate a MANUAL RDS DB instance FAILOVER(primary to secondary) when rebooting
- This is BY SELECTING THE "REBOOT WITH FAILOVER" reboot option on the primary RDS DB instance

> A DB instance reboot is required for changes to take effect when we change the DB parameter group or static DB parameter
- This reboot restart the DB engine

> The DB parameter group is a configuration container for the DB engine configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## MULTI-AZ RDS EVENT NOTIFICATIONS 
> You will be alerted by a DB instance event when a failover occurs

> AWS RDS uses AWS SNS to send RDS event via SNS notification
- we can use API calls to the RDS service to list the RDS events in the past 14 days(DescribeEvents API)
- You can view past 14 days events using CLI
- Using AWS console, you can only view RDS event for the last one day i.e 24 hour
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## PROCESS THAT HAPPENED ON THE STANDBY FIRST
> The following procedure are done on the standby first then on primary
- OS patching
- system upgrades 
- DB scaling

> in multi-AZ snapshot and automatic backup are done on the standby instance to avoid I/O suspension on primary instance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Multi-AZ deployment maintenance
> The sequence is as follow 
- maintenance on standby is performed 
- standby promoted to primary 
- maintain is performed on old primary (current standby)

## Version upgrade
> You can manually upgrade DB instance to a supported DB engine version from AWS console as follows

RDS -> DB instance -> modify DB instance -> set DB engine version

> By default, change will take effect during the next maintenance window
> You can force an immediate upgrade if you need to

> In multi-AZ deployments:
- Version upgrade will be conducted on both primary and standby at the same time, which will cause an outage (unavailability of both)
> Advised to do during change/maintenance window

## Security group and NaCl
> In a multi-AZ scenario make sure that SG and NACL will allow app server to communicate with both primary and standby instance 
- in case of a failure failover the standby become the primary
=======================================
##### DB AUTOMATIC BACKUP OR MANUAL SNAPSHOT 

> These are the two method to backup and restore your RDS DB instances
1. AWS RDS automatic backups
2. user intiated manual backup

> either one backup up the entire DB instance and not just the individual DB
> either one create a storage volume snapshot of your entire DB instance not just the individual database
> You can make copies of automatic backup and of manual snapshot

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##### DB AUTOMATED BACKUPS ######

>> Amazon RDS performs a storage volume snapshot of the entire Database Instance. 
- Also, it captures transaction logs every 5 minutes. 
- To restore a DB instance at a specific point of time, a new DB instance is created using this DB snapshot.

> Automated backup by AWS, backup DB data to multi-AZ to provide for data durability
> Are labelled automated in AWS console (easily distinguishable)
> Stored in Amazon S3
> Multi-AZ automatic backup will be taken from the standby instance not the primary
> The DB instance must be in "active" state for the automatic backup to happen 
- if in any other state like "storage_full" state the automatic backup will not happen

> Automatic backup (not the manual one) are used for point-in-time DB instance recovery

> It can RESTORE DB UPTO 5MIN IN-TIME, using the DB transaction logs and the automated snapshot

> RDS automatically backup the DB instance daily, by creating a storage volume snapshot of DB instance (full daily snapshot), including the DB transaction logs(modifications) which are critical to be able to restore upto last 5 minutes in time
- You can choose when during the day this is done (backup window)

> No additional charge for RDS backing up DB instance
> ENABLED BY DEFAULT, we can disable it by setting retention period to 0

> During daily backup window, I/O MAY BE SUSPENDED (for standalone RDS deployment)
> For multi-AZ deployment, backup are taken from the standby DB instance (true for MariaDB, mySQL, Oracle, PostgreSQL)

>> Automatic backup deleted when we delete RDS DB instance

> Automatic backup are currently supported only for InnoDB storage engine for MySQL (and not for myISAM engine)
> Use to automatic backup with other MySQL DB storage engines including ISAM, may lead to unpredictable behaviour during restoration

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###### DB RETENTION PERIOD ######
> The period of time AWS keep the automatic backup before deleting them
- 7days (by default) is configured from the AWS console for all DB engine (except Arora default is one day regardless how it was configured)

> 1 day is configured from API or CLI (for all DB engine including Arora)
> You can increase it upto 35days
> Value of 0 disabled automatic backup completely

> A DB outage if you change your retention period from 0 to non-zero value for the other way around from non-zero to 0 value
=======================================
######### DB SUBNET GROUP ########## v9

> It is a collection of subnet in a VPC that we want to allocated for DB instance launched in the VPC
- Each DB subnet group must have at least one subnet in each AZ in a region
- Even if you are starting with standalone RDS instance, configure the subnet group with the subnet in each AZ in the region
- This will facilitate launching your standby instance in the subnet group when you opt for multi-AZ deployment

> During creating RDS instance we can select a preferred AZ and specify which subnet group and subnet of that group for RDS DB instance
- then RDS service will allocate an IP address in that subnet to your RDS instance
- then RDS service will create an ENI and attached to the RDS instance and assign the above IP address to it
=======================================
### ENCRYPTION OF EXISTING UNENCRYPTED RDS INSTANCE v10
 
> we cannot encrypt an existing unencrypted DB instance

> To do that 
1. you need to create a new encrypted instance and migrate you data to it (from the unencrypted to the encrypted)
2. Restore from backup/snapshot into a new encrypted RDS instance

### App to DB instances encryption
> RDS supports SSL encryption for communication bet app instance and the RDS DB instance
- RDS generates a certificate for the instance which is used to encrypt this communication
- RDS service supports encryption at rest for all DB engine using AWS KMS

## RDS DB encryption at rest
> For an encrypted at rest DB instance
- All its snapshot
- Backup
- Data at storage(on the DB instance)
- Read replica related from the DB instance

- All are encrypted as well
- Encryption/decryption is handled transparently

## RDS DB instance security best practice
> Use AWS IAM account to control access to RDS API action
> Assign an individual IAM account to each person who manages RDS resources
> Grand the least permission required by each user to perform the assigned duties
> Use IAM groups to manage/grant permission to multiple user at a time
> Rotate your IAM credential regularly
=======================================
########### RDS Billing  ########### v11

> No upfront costs

> You pay for:
- DB instance hours (partial hours charged as full hours)
- EBS Storage GB/month.
- I/O requests/month - for Magnetic RDS storage Instance only
- Provisioned IOPS/month – For RDS Provisioned IOPS SSD instance
- Internet data transfer
- Backup Storage that is in S3 (for DB backups, and Active manual Snapshots)
  - This increases by increasing DB backups retention period
  - Backup storage for automated RDS backups (not the manual snapshots) up to the Provisioned RDS instance's EBS volume size (EBS volume) is free of charge

## RDS Billing – Multi-AZ deployments
> AWS will charge for the following (in addition to the single AZ DB instance charges)
- Multi-AZ DB hours
- Provisioned Storage (Multi-AZ)
- Double write I/O (writing to the Active/Primary, and-Replication from Primary to Standby)
- You are not charged for DB data transfer during replication from primary to standby
- Your DB storage does not change between Standalone and Multi-AZ deployments (same DB and same AWS Storage volume for that DB in multiple AZs for durability)

## Free Tier

> Single AZ:
- 750 Micro Instance Hours/month for each account for 1 year
- For ORACLE you need to BYOL

##### RESERVED DB INSTANCES (RIs) #####
> Similar to EC2 Reserved Instances
> DB RIs ARE "REGION" SPECIFIC NOT AZ SPECIFIC
> One or three year term options

> Each reservation must be specific in:
- DB Engine
- DB Instance class
- Multi-AZ option
- License Model
- Region
  - For RDS RI pricing to apply, an Exact RDS instance must be created on-demand, exact on all above (DB Engine, Instance class, Multi AZ option, License model, and region)

> You can NOT move RDS RIs between regions
> You can move RDS RIs between AZs in the same region
> You can NOT cancel an RDS RI's reservation
=======================================
############ READ REPLICAS ############ v12

> Read Replica is for Performance 
> When the required read I/O capacity is reached but still more I/O capacity is required for heavy/intensive read applications, read replicas can come in handy

> READ REPLICA is a replica of the primary RDS DB instance, but they CAN ONLY BE USED FOR READ actions
- Primary DB instance becomes the source of the replication to this read replica

> The data is first written to the source/primary DB Instance, then USING ASYNCHRONOUS REPLICATION, it gets replicated to the read replica i.e there is a TIME LAG BETWEEN when the data is written to the PRIMARY AND when it gets replicated to the READ REPLICA
- Transactions in primary are not impacted.

> Multi-AZ with read replicas can be combined in one deployment


> Read replicas can be created from the console or API

> AUTOMATIC BACKUPS (RETENTION PERIOD NOT ZERO) MUST BE ENABLED AND REMAIN ENABLED FOR READ REPLICAS TO WORK
> we can have multi-AZ Read replica, replica in another region
- NOW POSSIBLE TO CREATE REPLICA of READ REPLICAS 

> IS SUPPORTED WITH TRANSACTIONAL DB STORAGE ENGINES, and are supported on InnoDB engines not MYISAM (MYMAPO):
- MYSQL
- MariaDB
- PostegreSQL

> Each of these DB engines SUPPORT UPTO 5 READ REPLICAS per source/primary DB instance

## Read Replicas – Use cases
> Shifting read intensive applications such as Business (or Sales) reporting, or Data Warehousing to read from read replicas as opposed to overload the primary DB
> Scaling beyond the I/O capacity of your main DB instance for read-heavy workloads
> Service read traffic while the source is unavailable

#### CREATING READ REPLICAS ######
> It can be done from API or AWS console
> We can specify AZ in which we want our read replica

> READ REPLICA'S STORAGE TYPE OR INSTANCE CLASS CAN BE DIFFERENT from the source DB instance
> DB engine type can not be change though, it is inherited from the source (primary) DB instance
> Connecting to the DB engine on the read replica is possible, the same way (DB console) you connect to the primary DB instance
> If you scale the source DB instance, you should also scale the Read Replicas

> You cannot have more than four instances involved in a replication chain.
> If the source instance of a Multi-AZ deployment fails over to the secondary, any associated Read Replicas are switched to use the secondary as their replication source.

> You must explicitly delete Read Replicas, using the same mechanisms for deleting a DB instance.
 o If you delete the source DB instance without deleting the replicas, each replica is promoted to a stand-alone, single-AZ DB instance. 

> If you promote a MYSQL or MariaDB Read Replica that is in turn replicating to other Read Replicas, those Read Replicas remain active.

> If replication is stopped for more than 30 consecutive days, either manually or due to a Rplication error, Amazon RDS terminates replication between the master DB instance and all Read Replicas

======================================
##### v13

## Cross Region Read Replicas
> You can create read replicas in another region for MYSQL, MariaDB, PostgreSQL (but not for Aurora DB)
- Uses Asynchronous replication
- This can be used in improving DR capabilities by providing a lower RPO in case of a regional failure
- It can also help in case you have a requirement of Analytics to run in a different region (B) for a DB in region A.

## Read Replicas of Read Replicas
> MYSQL and MariaDB (MyMa):
- Allow for creating read replicas of the read replicas (second/third tier read replicas)

## Promoting Read replicas to Standalone DB instance
> You can promote a read replica into a standalone/single AZ database instance
- This is true for MYSQL, MariaDB, PostgreSQL (MyMaPo)

> Promotion process takes several minutes because the DB will reboot before the promoted replica becomes available as a standalone DB instance (allow for read/write, snapshots/backups...etc)

> The promoted replica into a standalone DB instance will Pretain:
- Backup retention period
- Backup window
- DB parameter group

> In case of multiple read replicas (MYSQL, MariaDB), Promoting one to a standalone DB instance does not affect the other read replicas, those will continue to read from the former primary (source DB instance)

=======================================
##### v14

## Storage Scaling
> You can scale an RDS storage up only, you can not decrease or scale down the storage size
- You can NOT decrease the allocated storage for an RDS instance
> You can scale storage and also change storage type for all supported DB engines, except MSSQL server

## Scaling
> You can scale (Up only, not down) the compute and storage capacity of your existing RDS DB instances

> Scaling storage can happen while the RDS instance is still running, available/accessible
- This may cause some performance degradations during the change, but will result in enhanced I/O after

> Scaling compute will cause a downtime to DB instance

> You do the required scaling changes, then you can
- Immediately have the changes take effect ,
OR
- Leave it to the default, which will make the changes take effect during thế Maintenance window

## Scaling – MS SQL Server
> You can NOT change the storage capacity nor type of storage of the MS SQL Server on windows-based RDS instances
- This is due to extensibility limitations of striped storage attached to windows server

> To work around the above, you can take a snapshot, use it to start a new DB instance with the desired Storage type and capacity (must be an increase, not a decrease)
- Note that a new instance, will have a new RDS endpoint

## Scaling beyond RDS instance capabilities
> If you hit the largest RDS DB instance, and you still need to scale, you can;
- Use partitioning and split your RDS DB over multiple RDS instances
=======================================
########## AWS RDS SECURITY ########### 

> AWS provides multiple features to provide RDS security: 

1. DB instance can be hosted in a VPC for the greatest possible network access control
2. IAM policies can be used to assign permissions that determine who is allowed to manage RDS resources
3. Security groups allow to control what IP addresses or EC2 instances can connect to the databases on a DB instance
4. Secure Socket Layer (SSL) connections with DB instances
5. RDS encryption to secure RDS instances and snapshots at rest.
6. Network encryption and transparent data encryption (TDE) with Oracle DB instances

=======================================
############# AWS Aurora ##############

> It is a fully managed relational database engine that's compatible with MySQL and PostgreSQL.

> It can deliver up to 5 times the throughput of MySQL and 
- up to 3 times the throughput of PostgreSQL 
- at a price one-tenth of that Commercial databases while delivering similar performance and availability.

> it support working with schema changes

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
####### Aurora Scaling ########
> The underlying storage grows automatically as needed, upto 64 tebibytes (TiB)
-  A tebibyte equals nearly 1.1 TB

> Compute resources can scale upto 64 vCPUs and 488 GB of memory.

> It maintains 2 copies of data in each AZ, with a minimum of three AZ. 
- Therefore, we can say that it maintains 6 copies of your data.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
########## Replicas ###########
> There are 2 types of Replicas:
1. Aurora Replicas(upto 15 Aurora Replicas across AZs).
2. MySQL Read Replicas (upto 5 Read Replicas across Azs)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
####### Global Database #######
> For globally distributed applications you can use Global Database, where a single Aurora database CAN SPAN MULTIPLE AWS REGIONS to enable fast local reads and quick disaster recovery.

> Global Database uses storage-based replication to replicate a database across multiple AWS Regions, with typical latency of less than 1 second.
- You can use a secondary region as a backup option in case you need to recover quickly from a regional degradation or outage.
- A database in a secondary region can be promoted to full read/write capabilities in less than 1 minute.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##### Automatic Failover ######

> Failover is automatically handled by Aurora so that apps can resume database operations as quickly as possible without manual administrative intervention.

> If you have Aurora Replica in the same or different AZ, when failing over, Aurora flips the CNAME for your DB Instance to point at the healthy replica, which in turn is promoted to become the new primary. Start-to-finish, failover typically completes within 30 seconds.

> If you are running Aurora Serverless and the DB instance or AZ become unavailable, Aurora will automatically recreate the DB instance in a different AZ.

> If you do not have an Aurora Replica (i.e. single instance) and are not running Aurora Serverless, Aurora will attempt to create a new DB Instance in the same AZ as the original instance. 
- This replacement of the original instance is done on a best-effort basis and may not succeed, 
- for example, if there is an issue that is broadly affecting the Availability Zone.

=======================================
############ AWS DynamoDB #############

> Amazon DynamoDB is a fast and flexible NOSQL DB service for all app that require consistent single-digit millisecond latency at any scale.
> It is a FULLY MANAGED DB that supports both document and key-value data models.

> Its flexible data model and performance makes it a great fit for mobile, web, gaming, ad-tech, IOT, and many other applications.
- It is durable, scalable, and HA data store which CAN BE USED FOR REAL-TIME TABULATION. 

> It is stored in SSD storage.
> It is normally used in conjuction with S3. 
- Ex- after storing image in S3, we can store their metadata in DynamoDB. We can also create 2nrd index for DynamoDB Table.

>> The maximum size of a DynamoDB item is 400KB
> It stores data indexed by a primary key
> It is spread across 3 geographically data centres

> When interacting with DynamoDB directly, there is a short list of header attributes that are required :
x-Amz-Target : operation name like GetIten
X-amz-date
host
content-type

> The primary key is the only required attribute for items in a table and it uniquely identifies each item.
- A primary key can either be one of the following types:
1. Partition key: 
- A simple primary key, composed of one attribute known as the partition key.

2. Partition key and sort key: 
- Referred to as a composite primary key.
- Composed of two attributes: partition key and sort key.

> A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key.

> DynamoDB supports 2 KINDS OF SECONDARY INDEXES:
1. Global secondary index(GSI) 
– An index with a partition key and sort key that can be different from those on the table.

2. Local secondary index (LSI) 
– An index that has the same partition key as the table, but a different sort key.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###### DynamoDB stream ########
> A DynamoDB stream is an ordered flow of information. It help you to keep a list of item level changes in DynamoDB table in last 24hr.
- When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.

> If you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#### DynamoDB Accelerator (DAX) 
> It is a fully managed, highly available, in-memory cache for DynamoDB.
- It provides an in-memory cache that delivers up to 10x performance improvement from milliseconds to microseconds or even at millions of requests per second.

> You do not need to modify application logic, since DAX is compatible with existing DynamoDB API calls.
> You can enable DAX with just a few clicks in the AWS Management Console or using the AWS SDK.


DynamoDB <---- DAX(with IAM role to access DynamoDB, within VPC) <-----EC2 Instance(with IAM role to access DynamoDB+DAX, within VPC)

- You can apply an IAM role to the the DAX nodes
- You can apply SGs to the DAX nodes
- DynamoDB DAX sits within your VPC

> ElastiCache can also be used in front of DynamoDB for performance of reads on infrequently changed data.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Because of its availability in 3 geographically DC, It consists of 2 different types of consistency models:

1. Eventual Consistent Reads (by default)
2. Strongly Consistent Reads

## AWS DynamoDB Throughput Capacity
> DynamoDB throughput capacity depends on the read/write capacity modes for performing read/write operation on tables.

> There are 2 types of read/write capacity modes:
1. Provisioned mode
2. On-demand mode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##### 1. PROVISIONED MODE #####
> It defines the maximum amount of capacity that an app can use from a specified table.

> In a provisioned mode, we need to specify the number of reads and writes per second required by the app.
- If the limit of Provisioned mode throughput capacity is exceeded then define, then this leads to the request throttling.

> A provisioned mode is GOOD FOR APP THAT HAVE PREDICTABLE AND CONSISTENT TRAFFIC.

> The Provisioned mode consists of 2 capacity units:
1. Read Capacity unit(RCU)
2. Write Capacity unit(WCU)

> Arora is provision mode is non-Serverless DB cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#### 1a. READ CAPACITY UNIT(RCU)

> The total number of RCU depends on the item size, and read consistency model.

> Read Capacity unit represents 2 types of consistency models:
1. Strongly Consistent model 
2. Eventually Consistent model

> 1 RCU = 1 Strong consistent OR 2 eventually consistent reads per second for an item up to 4KB in size.

> DynamoDB will require additional read capacity units when an item size is greater than 4KB. 
- For example, if the size of an item is 8KB, 2 read capacity units are required for strongly consistent read while 1 read capacity unit is required for eventually consistent read.
---------------------------------
#### 1b. WRITE CAPACITY UNIT (WCU) 

> The total number of write capacity unit depends on the item size.
> Only 1 WCU is required for an item upto size 1KB.

> DynamoDB will require additional WCU, when size is greater than 1KB. 
- For example, if an item size is 2KB, 2WCU are required to perform 1 write per second.
- For example, if you create a table with 20WCU, then you can perform 20 writes per second for an item up to 1KB in size.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###### 2. ON-DEMAND MODE ######

> DynamoDB on-demand mode has a flexible new billing option which is capable of serving thousands of requests per second without any capacity planning.
> It offers 'pay-per-request' pricing for read and write requests 
- so that you need to pay only for what you use, thus, making it easy to balance costs and performance.

> In On-Demand mode, DynamoDb accommodates the customer's workload instantly as the traffic level increases or decreases.
- it doesn't require to specify the DB instance class size. 
- but WE CAN SET THE MIN AND MAX CAPACITY. 

> On-Demand mode supports all the DynamoDB features such as encryption, point-in-time recovery, etc 
- IT DOESN'T SUPPORT AUTO-SCALING

> If you do not perform any read/write, then you just need to pay for data storage only.

> On-Demand mode is useful for those apps that have unpredictable traffic and database is very complex to forecast.

> AWS AURORA IN ON-DEMAND MODE IS SERVERLESS.
- it provides a relatively simple, cost-effective option for infrequent, intermittent, sporadic or unpredictable workloads. 
- An Aurora Serverless DB cluster, automatically starts up, shuts down, and scales up or down its compute capacity based on app's needs.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
########## SCALABILITY ##########
> It provides push button scaling where we can increase the read/write throughput without downtime or performance degradations
- we can scale provisioned capacity of DynamoDB table anytime 
- we can scale down provisioned capacity only 4 time during day. 

>> If we use Console to create a table or a Global secondary index, DynamoDB auto scaling is enabled by default.

> DynamoDB Auto Scaling uses AWS app AS to dynamically adjust provisioned throughput capacity in response of actual traffic 
- This help to automate capacity management of tables and global secondary indexes. 
- we simply specify the desired target utilization and provide upper and lower bounds for read and write capacity.
- will be ON by default for all new tables and indexes

- Storage and processing are separate, so you can scale down to zero processing and pay only for storage.

- Aurora Serverless clusters and provisioned clusters both have the same kind of high-capacity, distributed, and highly available storage volume.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
####### BEST PRACTICES ########
> Keep item sizes small.
> If you are storing serial data in DynamoDB that will require actions based on date/time use separate tables for days, weeks, months.
> Store more frequently and less frequently accessed data in separate tables.
> If possible compress larger attribute values.
> Store objects larger than 400KB in S3 and use pointers (S3 Object ID) in DynamoDB.
=======================================
############ AWS Redshift #############

> Redshift is a fast and powerful, fully managed, petabyte-scale DATA WAREHOUSE SERVICE

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
######### BENEFITS ############

> Query our Data Lake:
-- can directly query open data formats stored in S3 with REDSHIFT SPECTRUM(a feature of Redshift without need for unnecessary data movement)
- Redshift Spectrum is similar in-query functionality like S3 Select 
- The Redshift queries are run on cluster resources against local disk.
 
> SECURE
- It can be encrypted with AWS KMS or HSM

> For DR, automatically copy snaphots in another region when Cross-Region snaphots enable.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#### USE CASES ####
> Accelerated analytics workload 
> Unified data warehouse and data lake 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> AWS Documentation mentions the following on MONITORING REDSHIFT CLUSTERS:

> Amazon CloudWatch metrics help to monitor the physical aspects of cluster, such as CPU utilization, latency, and throughput. 
- Metric data is displayed directly in the Redshift console. 
- we can also view it in CloudWatch console, or 
- we can consume it in any other way such as with CloudWatch CLI OR SDKs
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

> We can configure CROSS-REGIONAL SNAPHOTS to automatically copy snapshots(automated or manual) to another region for backup purpose.
> Regardless of whether we enable automated snapshots, we can take a manual snapshot whenever we want at any time. 
- By default, manual snapshots are retained indefinitely, even after we delete cluster. 
- we can specify the retention period when we create manual snapshot or we can change the retention period by modifying the snapshot. 
-- If you create a snapshot using Redshift console, it defaults the snapshot retention period to 365 days.

>> Automated snapshots are automatically deleted within the period of 1(Least) to 35(Max) days (Based on the retention period settings). 

> So we have to take care of the Manual snapshots instead of Automated snapshots. 
- Redshift never deletes Manual snaphots automatically, like how it does for Automatic Snapshots.

-------------------------
> For DR plan, we can configure Redshift to copy snapshots for a cluster to another region. 
- To configure cross-region snapshot copy, you need to enable this copy feature for each cluster and configure where to copy snapshots and how long to keep copied automated snapshots in the destination region. 
- When cross-region copy is enabled for a cluster, all new manual and automatic snapshots are copied to the specified region.
-------------------------
> When you use REDSHIFT ENHANCED VPC ROUTING, Redshift forces all COPY and UNLOAD traffic between your cluster and your data repositories through your Amazon VPC.
- If Enhanced VPC Routing is not enabled, Redshift routes traffic through the Internet, including traffic to other services within the AWS network.
- Redshift will not be able to access the S3 VPC endpoints without enabling Enhanced VPC routing

#### REDSHIFT CONFIGURATION ####
> Redshift consists of two types of nodes:

1. Single node: 
- A single node stores up to 160 GB.

2. Multi-node: 
> It is a node that consists of more than one node. 
> It is of two types:

2a. LEADER NODE
> IT MANAGES THE CLIENT CONNECTIONS AND RECEIVES QUERIES. 
> A leader node receives the queries from the client applications, parses the queries, and develops the execution plans. 
- It coordinates with the parallel execution of these plans with the compute node and combines the intermediate results of all the nodes, and then return the final result to the client application.

2b. COMPUTE NODE
> A compute node executes the execution plans, and then intermediate results are sent to the leader node for aggregation before sending back to the client application. 
- It can have up to 128 compute nodes in a cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##### REDSHIFT – PERFORMANCE ####

1. Columnar Data Storage:
- Instead of storing data as series of rows, Redshift organizes the data by column.
- Unlike Row-based systems, which are ideal for transaction processing's, column-based systems are ideal for data warehousing and analytics, where queries often involve aggregates performed over large data sets.
- Since only the columns involved in the queries are processed and columnar data is stored sequentially on the storage media, column-based systems require far few I/Os, greatly improving query performance.

## Advanced Compression:
> Columnar data stores can be compressed much more than row-based data stores.
- Redshift employs multiple compression techniques and can often achieve significant compression relative to traditional relational data stores.
- When loading data into an empty table, Redshift automatically samples your data and selects the most appropriate compression technique.

> Massively Parallel processing (MPP):
- Redshift automatically distributes data and query load across all nodes.
- Redshift makes it easy to add nodes to data warehouse and enables you to maintain fast query performance as data warehouse grows.

## REDSHIFT – SECURITY
> Encrypted in transit using SSL
> Encrypted at rest using AES-256 encryption
> It uses a hierarchy of encryption keys to encrypt the database. 
- You can use either AWS KMS or HSM to manage the top-level encryption keys in this hierarchy.
- By Default, Redshift takes care of key management.

## Redshift – Backup Retention
> Redshift automaticallly patches and backup (snapshots) your data warehouse, storing the backups for a user-defined Retention period in S3.
- It keeps the backup by default for 1day (24hrs) but you can configure it from 0-35 days.
- Automatic backups are stopped if you choose retention period of 0
- You have access to these automatic snapshots during the retention period.
- If you delete the cluster, your automatic snapshots are deleted as well.
- Manual backups are not deleted automatically, if you not delete them manually, you will be charged standard S3 storage rates.

## Redshift – Restore
- Redshift currently SUPPORTS ONLY ONE AZ (no Multi-AZ option)
- You can restore from your backup to a new Redshift cluster in the same or different AZ

## Redshift – Monitoring
> Metrics for compute utilization, storage utilization and read/write traffic to your Redshift data warehouse cluster, are available free of charge via AWS Cloudwatch.
> You can also add additional user-defined, metrics via AWS Cloudwatch custom metric functionality.

## Q: There is a requirement to load a lot of data from your on-premises network to AWS Redshift. Which of the below options can be used for data transfer? Please select two possible answers

A) Direct Connect
B) Snowball
C) AWS VPN
D) Can't transfer huge amount of data to AWS
=======================================
########## Amazon ElastiCache #########
=======================================
> Amazon ElastiCache makes it easy to set up, manage, and scale DISTRIBUTED in-memory cache environments

> It is AWS FULLY MANAGED WEB SERVICE.
- It is an IN-MEMORY KEY-VALUE DATA STORE ENGINE in the cloud and provide ultra-fast (sub-millisecond latency) and inexpensive access to copies of data.

> Improves response times for user transactions and queries
- Can enhance response time for read-intensive And/or compute- intensive workloads.
- It offloads the read workload from the main DB instances
- It does this by storing the results of frequently accessed pieces of data in-memory

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
######### USE CASE ############
> ELASTICACHE IS USED IN BETWEEN WEBSERVER AND DATABASE TO STORE CACHE. WebServer might response through cache if the request match.

> it can be USED FOR STORING SESSION DATA for webapp.
- It is used for distributed session data management
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##### DESIGN & ARCHITECT ######
> ElastiCache shard - A collection of multiple nodes that make up a cluster
> CLUSTER - it is a collection of one or more nodes using the same caching engine

> Each node runs an instance of the Memcached or Redis protocol-compliant service and has its own DNS name and port.

> Failed nodes are automatically replaced.

> Elasticache EC2 nodes cannot be accessed from the Internet, nor can they be accessed by EC2 instances in other VPCs.
- It is ONLY ACCESSIBLE TO RESOURCE OPERATING WITH THE SAME VPC to ensure low latency.
- It can use 'on-demand' or 'reserved' instances (but not Spot instances). 

> 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
########## FEATURE ############

> Push-button scalability for memory, writes and reads.

> Automatic detection of and recovery from cache node failures.
> Flexible AZ placement of nodes and clusters.
> Integration with other AWS services such as EC2, CloudWatch, CloudTrail, and SNS to provide a secure, high-performance, managed in-memory caching solution.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> ElastiCache Supports 2 Caching engines:
1. Memcached (its not a data store (DB), only cache )
2. Redis (is the fast NOSQL – can be used as database)

######## 1. MEMCACHED #########
> it is preferred for caching HTML fragments. 
- It is simple K-V store
> Ideal front-end for data stores (RDS, DynamoDB)

> Use Cases:
- cache contents of a DB
- cache data from dynamically generated webpages
- Transient session data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
########### 2. REDIS ##########
> Redis is persistent
> It is the fast NoSQL engine 
> It can be used as a data store.
> It keep track of unread notification data. 

> Use cases:
- Web
- Mobile Apps
- Financial Apps
- Gaming
- Ad-tech
- IoT

> Using Redis AUTH command can improve data security by requiring the user to enter a password before they are granted permission to execute Redis commands on a password-protected Redis server.

> For Redis AUTH, clusters must be enabled with in-transit encryption during initial deployment. 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
