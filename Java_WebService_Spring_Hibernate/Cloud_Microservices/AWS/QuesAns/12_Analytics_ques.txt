=======================================
############ AWS KINESIS ##############
############ AWS Athena ###############
############## AWS Glue ###############
########## AWS CloudSearch ############
############## AWS EMR ################
######### AWS Data Pipeline ###########

=======================================
############ AWS KINESIS ##############

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## You are deploying an application to track the GPS coordinates of delivery trucks in the United States. Coordinates are transmitted from each delivery truck once every three seconds. You need to design an architecture that will enable real-time processing of these coordinates from multiple consumers. Which service should you use to implement data ingestion? [WL328]

A. Amazon Kinesis
B. AWS Data Pipeline
C. Amazon AppStream
D. Amazon Simple Queue Service


EXPLANATION:
Correct Answer - A

AWS documentation mentions the following:

> Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information.
- Amazon Kinesis offers key capabilities to process streaming data cost-effectively at any scale, along with the flexibility to choose the tools that best suit the requirements of your application. 
 
With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and responds instantly instead of waiting until all your data is collected before the processing can begin. 
For more information on Amazon Kinesis, please visit the following URL:
https://aws.amazon.com/kinesis/
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
### As a Solutions Architect for a multinational organization having more than 150000 employees, management has decided to implement a real-time analysis for their employees' time spent in offices across the globe. You are tasked to design an architecture that will receive the inputs from 10000+ sensors with swipe machine sending in and out data across the globe, each sending 20KB data every 5 Seconds in JSON format. The application will process and analyze the data and upload the results to dashboards in real-time. 
Other application requirements will include the ability to apply real-time analytics on the captured data, processing of captured data will be parallel and durable, the application must be scalable as per the requirement as the load varies and new sensors are added or removed at various facilities. The analytic processing results are stored in a persistent data storage for data mining. 
What combination of AWS services would be used for the above scenario? [WL214]


A. Use EMR to copy the data coming from Swipe machines into DynamoDB and make it available for analytics

B. Use Amazon Kinesis Streams to ingest the Swipe data coming from sensors, Custom Kinesis Streams Applications to analyze the data and then move analytics outcomes to RedShift using AWS EMR 

C. Use SQS to receive the data coming from sensors, Kinesis Firehose to analyze the data from SQS, then save the results to a Multi-AZ RDS instance

D. Use Amazon Kinesis Streams to ingest the sensors’ data, custom Kinesis Streams applications to analyze the data, and move analytics outcomes to RDS using AWS EMR



EXPLANATION: Kinesis
ANSWER : B

Option A is incorrect. EMR is not for receiving the real-time data from thousands of sources, EMR is mainly used for Hadoop ecosystem-based data used for Big data analysis.

Option B is correct as the Amazon Kinesis streams are used to read the data from thousands of sources like social media, survey-based data, etc. 
- The Kinesis streams can be used to analyze the data and can feed it using AWS EMR to the analytics-based database like RedShift which works on OLAP.
 
Option C is incorrect, SQS cannot be used to read the real-time data from thousands of sources. 
- Besides, the Kinesis Firehose is used to ship the data to other AWS service, not for the analysis. And finally, RDS is again an OLTP based database.
 
Option D is incorrect as the AWS EMR can read large amounts of data, however, RDS is a transactional database that works based on the OLTP. Thus, it cannot store the analytical data.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
### 55:WL7: You are working as an AWS architect for a global financial company which offers real-time stock trading quotes to customers. You are using Kinesis Data Streams to process stock market feeds from stock exchanges & provide a real-time dashboard to the customers. During stock market hours, there are a large number of users accessing these dashboards while after market hours, there are very few users accessing these dashboards.  The management team is looking for an optimum number of Kinesis Shards within Kinesis Data Streams. What would be a cost-effective solution with no performance impact and no additional management overload? [WL755]

A. Use CloudWatch alarms to monitor Amazon Kinesis Streams Metrics and send a notification to Application Auto Scaling. This will call API Gateway to initiate a Lambda function modifying the number of Shards in Kinesis Data Streams.

B. Use Amazon Kinesis Scaling Utility to modify the number of Shards in Kinesis Data Streams.

C. Use Amazon Kinesis Scaling Utility along with AWS Elastic Beanstalk to automatically modify the number of Shards in Kinesis Data Streams.

D. Use UpdateShardCount to scale Shard count as per requirement. 


EXPLANATION: Kinesis
Correct Answer – A

> AWS Application Auto scaling can be used to automatically scale Kinesis Streams. 
- For this, CloudWatch can be used to monitor Kinesis Data Stream shard metrics. 
- Based on the changes in these metrics, CloudWatch can initiate a notification to Application Auto Scaling. 
- This will trigger an API Gateway to call Lambda Functions to increase/decrease the number of Kinesis Data Stream Shards based upon metric values.
 
Option B is incorrect because Amazon Kinesis Scaling Utility is not a cost-effective solution. 
Option C is incorrect as Using Elastic Beanstalk along with Amazon Kinesis Scaling Utility will be more complex than using AWS Application Auto Scaling. 
Option D is incorrect as using UpdateShardCount will be a manual process which will incur additional admin work. 

For more information on Scaling Kinesis Data Streams using Application Auto Scaling, refer to the following URL:
https://aws.amazon.com/blogs/big-data/scaling-amazon-kinesis-data-streams-with-aws-application-auto-scaling/
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 

=======================================
############ AWS Athena ###############
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
### 39:WL7: You are working as an AWS Administrator for a media company. They are using AWS resources in various regions for broadcasting live sports matches. Multiple EC2 On-Demand, Spot & Reserved Instances are launched as per user traffic on a daily basis resulting in huge monthly charges. Top management is looking for customized analysis for these charges based upon various factors like month-wise, instance-wise to deep dive into the incurred cost. To meet this requirement, the accounts team is looking for a simpler way to analyze these charges and query this report. Suggest a cost-effective solution that can be set up with the least efforts.

A. Upload the AWS Cost and Usage Reports to S3. Integrate these reports with Amazon Athena to analyze billing data
B. Upload AWS Cost & Usage Reports to Amazon QuickSight & analyze billing data
C. Download CSV report from Amazon S3 & analyze cost & usage using a third-party tool.
D. Upload AWS Cost & Usage Reports to Amazon Redshift & analyze billing data


EXPLANATION:
Correct Answer – A

> Amazon Athena is a serverless platform which can be used to analyze Cost & Usage Reports uploaded in Amazon S3 buckets. 
- Using Athena, a customized query can be requested using standard SQL. 

Option B is incorrect as this will result in additional admin work for Managing Amazon QuickSight. 
Option C is incorrect as using a third-party tool to analyze cost & usage reports will be costly. 
Option D is incorrect as this will incur additional admin work for Managing Amazon Redshift. 
For more information on Analyzing Cost & Usage Reports using Amazon Athena, refer to the following URL,

https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/setting-up-athena.html
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 7:WL5: You are working for an electrical appliance company that has web-application hosted in AWS. This is a two-tier web application with web-servers hosted in VPC’s & on-premise data-center.  You are using a Network Load balancer in the front end to distribute traffic between these servers. You are using instance Id for configuring targets for Network Load Balancer. Some clients are complaining about the delay in accessing this website. 
To troubleshoot this issue, you are looking for a list of Client IP address having longer TLS handshake time. You have enabled access logging on Network Load balancing with logs saved in Amazon S3 buckets. Which tool could be used to quickly analyze a large amount of log files without any visualization in a cost-effective way? 

A. Use Amazon Athena to query logs saved in Amazon S3 buckets.
B. Use Amazon S3 console to process logs.
C. Export Network Load Balancer access logs to third-party application.
D. Use Amazon Athena along with Amazon QuickSight to query logs saved in Amazon S3 buckets. 


EXPLANATION: Athena
Correct Answer – A

> Amazon Athena is a suitable tool for querying Network Load Balancers logs. 
- In the above case since a large amount of logs are saved in S3 buckets from Network load balancer, Amazon Athena can be used to query logs and generate required details of client IP address and TLS handshake time. 

Option B is incorrect as processing a large number of logs directly from the S3 console will be a time-consuming process. 
Option C is incorrect as using a third-party tool will not be a cost-effective solution. 

Option D is incorrect as in the above case, we require only details of Client IP details along with TLS handshake time for troubleshooting purposes. 
- Amazon QuickSight will be useful in case you need data visualization. 

For more information on using Amazon Athena to query Network Load Balancer logs, refer to the following URL:
 https://docs.aws.amazon.com/athena/latest/ug/networkloadbalancer-classic-logs.html

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 10:WL5: You are working for a global financial company. Company locations spread across various countries upload transaction details data to S3 bucket in the US-West region. A large amount of data is uploaded on a daily basis from each of these locations simultaneously. You are using Amazon Athena to query this data & create reports using Amazon QuickSight to create a daily dashboard to the management team. In some cases, while running queries, you are observing Amazon S3 exception errors. 
Also, in the monthly bills, a high percentage of cost is associated with Amazon Athena. Which of the following could be helpful in eliminating S3 errors while querying data and reducing the cost associated with queries?  (SELECT TWO)


A. Partition data based upon user credentials
B. Partition data based upon date & location. 
C. Create a separate Workgroups based upon user groups. 
D. Create a single Workgroup for all users. 


EXPLANATION: Athena
Correct Answers – B and C

> AWS Athena pricing is based upon per query and the amount of data scanned in each query. 
- In the above case, each regional office is uploading a large amount of data simultaneously, this data needs to be partitioned based upon location & date. 
- A separate Workgroup can be created based upon users, teams, application or workloads. 
- This will lead to minimizing the amount of data scanned for each query, improving performance & reducing cost. 

Option A is incorrect as partitioning the data on user credentials is irrelevant here. 
Option D is incorrect as a single Workgroup will not decrease the amount of data scanned per query. 

For more information on Partitioning data & using Workgroups, refer to the following URLs:
https://docs.aws.amazon.com/athena/latest/ug/partitions.html
https://docs.aws.amazon.com/athena/latest/ug/manage-queries-control-costs-with-workgroups.html
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


=======================================
############## AWS Glue ###############
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## You are working for a global financial company. Company locations spread across various countries upload transaction data to S3 bucket in the US-West region. You will be using AWS Glue & Amazon Athena to further analyze this data. You are using Crawler that will scan all data from S3 buckets & populate Glue Data Catalog, to which Amazon Athena will query. 
A large amount of CSV data is uploaded on a daily basis from all the global locations simultaneously. To decrease scanning time while scanning data in S3 buckets, you need to ensure only changes in datasets are scanned. Which of the following configurations would meet this requirement? [WL517]

A. Reset Job Bookmark in AWS Glue.
B. Disable Job Bookmark in AWS Glue.
C. Pause Job Bookmark in AWS Glue. 
D. Enable Job Bookmark in AWS Glue. 


EXPLANATION: Glue
Correct Answer – D

> AWS Glue keeps a track of processed data using Job Bookmark. 
- Enabling Job Bookmark will help to scan only changes since the last bookmark and prevent the processing of whole data again. 

Option A is incorrect as resetting Job Bookmark in AWS Glue will reprocess all data & will not prevent the reprocessing of already scanned data. Option B is incorrect as disabling Job Bookmark will process all data each time. 
Option C is incorrect as pausing Job Bookmark will process incremental data since the last data scan but will not update state information. So, for the succeeding process, it will start scanning all data since the last bookmark.  

For more information on Job Bookmark in AWS Glue, please refer to the following URL,

https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


=======================================
########## AWS CloudSearch ############
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## An organization has a requirement to store 10TB worth of scanned files across multiple availability zones. It plans to have a search application in place to search through the scanned files. 
Which of the following options is ideal for implementing the search facility? [WL516]

A. Use S3 with reduced redundancy to store and serve the scanned files. Install a commercial search application on EC2 Instances and configure with Auto-Scaling and an ElasticLoad Balancer.

B. Model the environment using CloudFormation. Use an EC2 instance running Apache webserver and an open-source search application, stripe multiple standard EBS volumes together to store the scanned files with a search index.

C. Use S3 with standard redundancy to store and serve the scanned files. Use CloudSearch for query processing, and use Elastic Beanstalk to deploy the website across multiple Availability Zones.

D. Use a single-AZ RDS MySQL instance to store the search index for the scanned files and use an EC2 instance with a custom application to search based on the index. 


EXPLANATION: CloudSearch
Correct Answer – C

With Amazon CloudSearch, you can quickly add rich search capabilities to your website or application. 
- You don't need to become a search expert or worry about hardware provisioning, setup, and maintenance. 
- With a few clicks in the AWS Management Console, you can create a search domain and upload the data that you want to make searchable, and 
- Amazon CloudSearch will automatically provision the required resources and deploy a highly tuned search index. 
- You can easily change your search parameters, fine-tune search relevance, and apply new settings at any time. 
- As your volume of data and traffic fluctuates, Amazon CloudSearch seamlessly scales to meet your needs. 
 
For more information on AWS CloudSearch, please visit the link below:
https://aws.amazon.com/cloudsearch/

For more information, please check the URL below:
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/awseb-dg.pdf
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


=======================================
############## AWS EMR ################

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


=======================================
######### AWS Data Pipeline ###########

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## An International company has deployed a multi-tier web application that relies on DynamoDB in a single region. For regulatory reasons they need disaster recovery capability in a separate region with a Recovery Time Objective of 2 hours and a Recovery Point Objective of 24 hours. They should synchronize their data on a regular basis and be able to provision me web application rapidly using CloudFormation.
The objective is to minimize changes to the existing web application, control the throughput of DynamoDB used for the synchronization of data and synchronize only the modified elements.
Which design would you choose to meet these requirements?


A. Use AWS data Pipeline to schedule a DynamoDB cross region copy once a day. create a Lastupdated' attribute in your DynamoDB table that would represent the timestamp of the last update and use it as a filter.

B. Use EMR and write a custom script to retrieve data from DynamoDB in the current region using a SCAN operation and push it to QynamoDB in the second region.

C. Use AWS data Pipeline to schedule an export of the DynamoDB table to S3 in the current region once a day then schedule another task immediately after it that will import data from S3 to DynamoDB in the other region.

D. Send also each Ante into an SQS queue in me second region; use an auto-scaiing group behind the SQS queue to replay the write in the second region.



EXPLANATION : Data pipeline
ANSWER : C
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Your company is planning on setting up an application that will consist of a presentation layer and a datastore in DynamoDB. The data in DynamoDB will only used frequently within the week in which the data is inserted. After a week, the data would tend to become stale. But the stale data would need to be available on durable storage for future analysis on historical data. Which of the following would be the ideal implementation steps for this sort of architecture? [WL760] Choose 2 answers from the options given below

A. Setup DynamoDB tables. Scan the tables for older data and transfer them to another DynamoDB table. 
B. Setup DynamoDB tables on a weekly basis. Ensure the most recent week table has a higher throughput setup. 
C. Use the AWS Data Pipeline service to transfer the older data to EBS volumes
D. Use the AWS Data Pipeline service to transfer the older data to Amazon S3


EXPLANATION: DynamoDB , Data Pipeline
Answer – B and D

The AWS Documentation mentions the following

> AWS Data Pipeline is a web service that you can use to automate the movement and transformation of data. 
- With AWS Data Pipeline, you can define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks. 
- You define the parameters of your data transformations and AWS Data Pipeline enforces the logic that you've set up. 

Option A is invalid because this would be an inefficient way to handle the data. You will be using too much throughput in the scan process. 
Option C is invalid because EBS volumes is not durable storage

For more information on DynamoDB best practises and AWS Data Pipeline, please refer to the below URL

https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 


EXPLANATION : 
ANSWER : 

