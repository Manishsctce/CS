=======================================
######### AWS DEVELOPER TOOLS #########

####### AWS CodeCommit ########
####### AWS CodeBuild #########
####### AWS CodeDeploy ########
####### AWS CodePipeline ######
######## AWS X-Ray ############
=======================================
########### AWS CodeCommit ############


=======================================
########### AWS CodeBuild #############


=======================================
########### AWS CodeDeploy ############
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## You are working as an AWS Architect for a global IT company that is using AWS EC2 instance for hosting their sessionless application. The development team has come up with a new version of this application which needs to be deployed in all EC2 instance across various regions. Which service can be used for an in-place upgrade? [WL733]

A. AWS CodePipeline.
B. AWS Elastic Beanstalk.
C. AWS CodeDeploy.
D. AWS CloudFormation. 

EXPLANATION:
Correct Answer – C

The in-place upgrade involves an application update on a live Amazon EC2 instance. For performing application updates on these instances, deployment content should be developed & saved in any repository like Amazon S3 buckets. AWS CodeDeploy can be used to automate application deployment using new versions from S3 buckets.              

Option A is incorrect as AWS CodePipeline is used to automate the release process for building, testing & deploying code based on the release process model. Option B is incorrect as these are live EC2 instance, AWS Elastic Beanstalk is not a correct option for performing application updates. Option D is incorrect as AWS CloudFormation is used to create templates that are used for creating resources required for the application deployment. For more information on choosing deployment service for the in-place upgrade, refer to the following URL:

https://d0.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##30:UD6: A web application is hosted on a fleet of EC2 instances inside an Auto Scaling Group with a couple of Lambda functions for ad hoc processing. Whenever you release updates to your application every week, there are inconsistencies where some resources are not updated properly. You need a way to group the resources together and deploy the new version of your code consistently among the groups with minimal downtime. 
Which among these options should you do to satisfy the given requirement with the least effort?

A. Create OpsWorks recipes that will automatically launch resources containing the latest version of the code.
B. Create CloudFormation templates that have the latest configurations and code in them.
C. Use deployment groups in CodeDeploy to automate code deployments in a consistent manner.
D. Use CodeCommit to publish your code quickly in a private repository and push them to your resources for fast updates.


EXPLANATION : CodeDeploy
ANSWER : C

> CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions. 
- It allows you to rapidly release new features, update Lambda function versions, avoid downtime during application deployment, and handle the complexity of updating your applications, without many of the risks associated with error-prone manual deployments.

Option-D is incorrect as you mainly use CodeCommit for managing a source-control service that hosts private Git repositories. 
- You can store anything from code to binaries and work seamlessly with your existing Git-based tools. 
- CodeCommit integrates with CodePipeline and CodeDeploy to streamline your development and release process.

Option-B is incorrect since 
- CloudFormation is used for provisioning and managing stacks of AWS resources based on templates you create to model your infrastructure architecture. 
- CloudFormation is recommended if you want a tool for granular control over the provisioning and management of your own infrastructure.

Option-A is still incorrect because you don't need to launch new resources containing your new code when you can just update the ones that are already running.

References:
https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html
https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html

Overview of Deployment Options on AWS whitepaper
https://d0.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf

Check out this AWS CodeDeploy Cheat Sheet:
https://tutorialsdojo.com/aws-cheat-sheet-aws-codedeploy/

Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy:
https://tutorialsdojo.com/aws-cheat-sheet-elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/

=======================================
########## AWS CodePipeline ###########

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 13:WL6: A global media firm is using AWS CodePipeline as an automation service for releasing new features to customers. All the codes are uploaded in the Amazon S3 bucket. Changes in files stored in  Amazon S3 bucket should trigger AWS Codepipeline that will further initiate AWS Elastic Beanstalk for deploying additional resources. What is the additional requirement that should be configured to trigger CodePipeline in a faster way? [WL613]

A. Enable periodic checks and create a Webhook which triggers pipeline once S3 bucket is updated.
B. Disable periodic checks, create an Amazon CloudWatch Events rule & AWS CloudTrail trail.
C. Enable periodic checks, create an Amazon CloudWatch Events rule & AWS CloudTrail trail.
D. Disable periodic checks and create a Webhook which triggers pipeline once S3 bucket is updated. 


EXPLANATION: CodePipeline
Correct Answer – B

> To automatically trigger pipeline with changes in the source S3 bucket, Amazon CloudWatch Events rule & AWS CloudTrail trail must be applied. 
- When there is a change in S3 bucket, events are filtered using AWS CloudTrail & then Amazon CloudWatch events are used to trigger the start of pipeline. 
- This default method is faster & periodic checks should be disabled to have events-based triggering of CodePipeline. 

Option A is incorrect as Webhooks are used to trigger pipeline when the source is GitHub repository. Also, the periodic check will be a slower process to trigger CodePipeline. 

Option C is incorrect as Periodic checks are not a faster way to trigger CodePipeline. 

Option D is incorrect as Webhooks are used to trigger pipeline when the source is GitHub repository. For more information on Automatically Triggering Pipeline, refer to the following URL:

https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html


=======================================
############# AWS X-Ray ###############
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 21:WL5: You are working with a global IT firm that has web-application hosted in AWS. This is a two-tier web application with web-servers behind Application load balancers. A new application is developed for which you need to analyze performance at each node. 
These parameters will be used as a reference before making this application into commercial services & henceforth for any operational challenges. You are using AWS X-Ray for this purpose. Which of the following would help to get traces while ensuring cost is within limits? 

A. Sampling at default rate.
B. Sampling at higher rate.
C. Filter expression
D. Sampling at low rate. 


EXPLANATION: X-Ray
Correct Answer – D

> The sampling rate can be lower to collect a significant number of requests statistically, to get optimum traces, and have a cost-effective solution.
 
Option A is incorrect as the Default Sampling rate is conservative but can be further customized to sample at a lower rate based upon your sampling requirements. 

Option B is incorrect as Sampling at a higher rate will not only collect a lot of data but also incur an additional cost. 

Option C is incorrect as Filter expression can be used to narrow down results from the number of traces scanned during a defined period. It will not affect the number of traces scanned. For more information on X-Ray parameters, refer to the following URL,

https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##21: An application is using a RESTful API hosted in AWS which uses Amazon API Gateway and AWS Lambda. There is a requirement to trace and analyze user requests as they travel through your Amazon API Gateway APIs to the underlying services. 
Which of the following is the most suitable service to use to meet this requirement? 

A. CloudWatch
B. CloudTrail
C. VPC Flow Logs
D. AWS X-Ray


EXPLANATION : x-ray
ANSWER : D

> You can use AWS X-Ray to trace and analyze user requests as they travel through your Amazon API Gateway APIs to the underlying services. 
- API Gateway supports AWS X-Ray tracing for all API Gateway endpoint types: regional, edge-optimized, and private. 
- You can use AWS X-Ray with Amazon API Gateway in all regions where X-Ray is available.

> X-Ray gives you an end-to-end view of an entire request, so you can analyze latencies in your APIs and their backend services. 
- You can use an X-Ray service map to view the latency of an entire request and that of the downstream services that are integrated with X-Ray. 
- And you can configure sampling rules to tell X-Ray which requests to record, at what sampling rates, according to criteria that you specify. 
- If you call an API Gateway API from a service that's already being traced, API Gateway passes the trace through, even if X-Ray tracing is not enabled on the API.

You can enable X-Ray for an API stage by using the API Gateway management console, or by using the API Gateway API or CLI.


VPC Flow Logs is incorrect because this is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your entire VPC. Although it can capture some details about the incoming user requests, it is still better to use AWS X-Ray as it provides a better way to debug and analyze your microservices applications with request tracing so you can find the root cause of your issues and performance.

CloudWatch is incorrect because this is a monitoring and management service. It does not have the capability to trace and analyze user requests as they travel through your Amazon API Gateway APIs.

CloudTrail is incorrect because this is primarily used for IT audits and API logging of all of your AWS resources. It does not have the capability to trace and analyze user requests as they travel through your Amazon API Gateway APIs, unlike AWS X-Ray.


Reference:
https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-xray.html


Check out this AWS X-Ray Cheat Sheet:
https://tutorialsdojo.com/aws-cheat-sheet-aws-x-ray/


Instrumenting your Application with AWS X-Ray:
https://tutorialsdojo.com/aws-cheat-sheet-instrumenting-your-application-with-aws-x-ray/
