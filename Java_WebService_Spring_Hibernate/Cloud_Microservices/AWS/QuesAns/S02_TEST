## A customer wants to create EBS Volumes in AWS. The data on the volume is required to be encrypted at rest. How could this be achieved? [WL201]

A. Create an SSL Certificate and attach it to the EBS Volume
B. Use KMS to generate encryption keys which can be used to encrypt the volume
C. Use CloudFront in front of the EBS Volume to encrypt all requests
D. Use EBS Snapshots to encrypt the requests. 

ANSWER : b

EXPLANATION: 

When you create a volume, you have an option to encrypt the volume using keys generated by the Key Management Service.

Option A is incorrect since SSL helps to encrypt data in transit.
Option C is incorrect because it also does not help in encrypting the data at rest.
Option D is incorrect because the snapshot of an unencrypted volume is also unencrypted.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## A company is planning on testing a large set of IoT enabled devices. These devices will be streaming data every second. A proper service needs to be chosen in AWS which could be used to collect and analyze these streams in real-time. Which AWS service would be the most appropriate for this purpose? [WL203]

A. Use AWS EMR to store and process the streams
B. Use AWS Kinesis to process and analyze the data
C. Use AWS SQS to store the data
D. Use SNS to store the data. 


ANSWER : B

EXPLANATION: 
Option A is incorrect. Amazon EMR can be used to process applications with data-intensive workloads.
Option B is correct. Amazon Kinesis can be used to store, process, and analyze real-time streaming data.
Option C is incorrect. SQS is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications.
Option D is incorrect. SNS is a flexible, fully managed pub/sub messaging and mobile notifications service for coordinating the delivery of messages to subscribing to endpoints and clients.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## You have instances hosted in a private subnet in a VPC. There is a need for instances to download updates from the Internet. As an architect, what change would you suggest to the IT Operations team that would also be the most efficient and secure? [WL2]

                                         
A. Create a new public subnet and move the instance to that subnet
B. Create a new EC2 Instance to download the updates separately and then push them to the required instance
C. Use a NAT Gateway to allow the instances in the private subnet to download the updates
D. Create a VPC link to the Internet to allow the instances in the private subnet to download the updates. 

ANSWER : C

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## A company plans to have its application hosted in AWS. This application allows the users to upload files and then using a public URL for downloading them at a later stage. Which design would help fulfill this requirement? [WL207]
                                         
A. Have EBS Volumes hosted on EC2 Instances to store the files
B. Use Amazon S3 to host the files
C. Use Amazon Glacier to host the files since this would be the cheapest storage option
D. Use EBS Snapshots attached to EC2 Instances to store the files. 


ANSWER : B

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## An EC2 Instance hosts a Java-based application that accesses a DynamoDB table. This EC2 Instance is currently serving production users. What would be a secure way for the EC2 Instance to access the DynamoDB table? [WL209]

                                         
A. Use IAM Roles with permissions to interact with DynamoDB and assign it to the EC2 Instance
B. Use KMS Keys with the right permissions to interact with DynamoDB and assign it to the EC2 Instance
C. Use IAM Access Keys with the right permissions to interact with DynamoDB and assign it to the EC2 Instance
D. Use IAM Access Groups with the right permissions to interact with DynamoDB and assign it to the EC2 Instance. 



ANSWER : A

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## A company planning to build and deploy a web application on AWS needs to have a data store to store session data. Which AWS services should be used to meet this requirement? (SELECT TWO) [WL210]

A. AWS RDS
B. AWS SQS
C. DynamoDB
D. AWS ElastiCache


ANSWER : C, D

EXPLANATION: 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## An application currently uses AWS RDS MySQL as its data layer. Due to recent performance issues on the database, it has been decided to separate the querying part of the application by setting up a separate reporting layer. What should be an additional step to improve the performance of the underlying database? [WL213]

                                         
A. Make use of Multi-AZ to set up a secondary database in another Availability Zone
B. Make use of Multi-AZ to set up a secondary database in another region
C. Make use of Read Replicas to set up a secondary read-only database
D. Make use of Read Replicas to set up a secondary read and write database. 


ANSWER : C

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## An application running on EC2 Instances processes sensitive information stored on Amazon S3. This information is accessed over the Internet. The security team is concerned that the Internet connectivity to Amazon S3 could be a security risk. Which solution will resolve this security concern? [WL215]
                                         
A. Access the data through an Internet Gateway
B. Access the data through a VPN connection
C. Access the data through a NAT Gateway
D. Access the data through a VPC endpoint for Amazon S3. 


ANSWER : D

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## You have a web application hosted on an EC2 Instance in AWS which is being accessed by users across the globe. The Operations team has been receiving support requests about extreme slowness from users in some regions. What can be done to the architecture to improve the response time for these users? [WL217]

A. Add more EC2 Instances to support the load.
B. Change the Instance type to a higher instance type.
C. Add Route 53 health checks to improve the performance.
D. Place the EC2 Instance behind CloudFront.


ANSWER : D

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## A company wants to have a fully managed data store in AWS. It should be a compatible MySQL database, which is an application requirement. Which AWS database engine could be used for this purpose? [WL219]

A. AWS RDS
B. AWS Aurora
C. AWS DynamoDB
D. AWS Redshift


ANSWER : B

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## A Solutions Architect is designing an online shopping application running in a VPC on EC2 Instances behind an Elastic Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The application tier must read and write data to a customer-managed database cluster. There should be no access to the database from the Internet but the cluster must be able to obtain software patches from the Internet. Which VPC design meets these requirements? [WL220]

A. Public subnets for both the application tier and the database cluster.
B. Public subnets for the application tier and private subnets for the database cluster.
C. Public subnets for both application tier and NAT Gateway and private subnets for the database cluster.
D. Private subnets for the application tier and private subnets for both the database cluster and NAT Gateway


ANSWER : C

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## It is expected that only certain specified customers can upload images to the S3 bucket for a certain period of time. What would you suggest as an architect to fulfill this requirement? [WL221]

A. Create a secondary S3 bucket. Then, use an AWS Lambda to sync the contents to the primary bucket.
B. Use pre-signed URLs for uploading the images.
C. Use ECS Containers to upload the images.
D. Upload the images to SQS and then push them to the S3 bucket. 


ANSWER : B

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## You have a set of on-premises virtual machines used to serve a web-based application. You need to ensure that a virtual machine if unhealthy is taken out of the rotation. Which of the following options would be used for health checking and DNS failover features for a web application running behind  ELB, to increase redundancy and availability. [WL223]

A. Use Route 53 health checks to monitor the endpoints.
B. Move the solution to AWS and use a Classic Load Balancer.
C. Move the solution to AWS and use an Application Load Balancer.
D. Move the solution to AWS and use a Network Load Balancer. 


ANSWER : A

EXPLANATION: 
Route 53 health checks can be used for any endpoint that can be accessed via the Internet. Hence, this would be an ideal option for monitoring endpoints.

AWS Documentation mentions the following:

You can configure a health check that monitors an endpoint that you specify either by IP address or by the domain name. At regular intervals that you specify, Route 53 submits automated requests over the internet to your application, server or other resources to verify that it's reachable, available and functional. 

 

For more information on Route 53 Health checks, please refer to the URL below.

https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-simple-configs.html

Note:

As per AWS, once enabled, Route 53 automatically configures and manages health checks for individual ELB nodes. Route 53 also takes advantage of the EC2 instance health checking that ELB performs. By combining the results of health checks of your EC2 instances and your ELBs, Route 53 DNS Failover is able to evaluate the health of the load balancer and the health of the application running on the EC2 instances behind it. In other words, if any part of the stack goes down, Route 53 detects the failure and routes traffic away from the failed endpoint.

For more information on Amazon Route 53 ELB Integration DNS Failover, please visit the following URL:

https://aws.amazon.com/blogs/aws/amazon-route-53-elb-integration-dns-failover/
 https://aws.amazon.com/premiumsupport/knowledge-center/route-53-dns-health-checks/


AWS documentation states that you can create a Route 53 resource record that points to an address outside AWS. You can set up health checks for parts of your application running outside AWS, and you can failover to any endpoint that you choose, regardless of location. 

For example, you may have a legacy application running in a datacenter outside AWS and a backup instance of that application running within AWS. You can set up health checks of your legacy application running outside AWS, and if the application fails the health checks, you can failover automatically to the backup instance in AWS.

Please refer to the following URL for more information:

https://aws.amazon.com/route53/faqs/

Note: 

As per AWS, Route 53 has health checkers in different locations around the world. When you create a health check that monitors an endpoint, health checkers start to send requests to the endpoint that you specify to determine whether the endpoint is healthy. You can choose which locations you want Route 53 to use, and you can specify the interval between checks: every 10 seconds or every 30 seconds. Note that Route 53 health checkers in different data centers don't coordinate with one another, so you'll sometimes see several requests per second regardless of the interval you chose, followed by a few seconds with no health checks at all.
Each health checker evaluates the health of the endpoint based on two values:

Response time
Whether the endpoint responds to a number of consecutive health checks that you specify (the failure threshold)
Route 53 aggregates the data from the health checkers and determines whether the endpoint is healthy:

If more than 18% of health checkers report that an endpoint is healthy, Route 53 considers it healthy.
If 18% of health checkers or fewer report that an endpoint is healthy, Route 53 considers it unhealthy.

The response time that an individual health checker uses to determine whether an endpoint is healthy depends on the type of health check:
HTTP and HTTPS health checks, TCP health checks or HTTP and HTTPS health checks with string matching.

Regarding your specific query where we are having more than 2 servers for the website, AWS docs state that:

When you have more than one resource performing the same function—for example, more than one HTTP server or mail server—you can configure Amazon Route 53 to check the health of your resources and respond to DNS queries using only the health resources. For example, suppose your website, example.com, is hosted on six servers, two each in three data centers around the world. You can configure Route 53 to check the health of those servers and to respond to DNS queries for example.com using only the servers that are currently healthy. The configuration details are provided in the second link.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## A company has a set of web servers. It is required to ensure that all the logs from these web servers can be analyzed in real-time for any sort of threat detection. What could be the right choice in this regard? [WL224]

A. Upload all the logs to the SQS Service and then use EC2 Instances to scan the logs.
B. Upload the logs to Amazon Kinesis and then analyze the logs accordingly.
C. Upload the logs to CloudTrail and then analyze the logs accordingly.
D. Upload the logs to Glacier and then analyze the logs accordingly. 


ANSWER : B

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## You currently have the following architecture in AWS:

a. A couple of EC2 Instances located in us-west-2a
b. The EC2 Instances are launched via an Auto Scaling group. 
c. The EC2 Instances sit behind a Classic ELB. 

Which additional step would ensure that the above architecture conforms to a well-architected framework? [WL225]

A. Convert the Classic ELB to an Application ELB.
B. Add an additional Auto Scaling Group.
C. Add additional EC2 Instances to us-west-2a.
D. Add or spread existing instances across multiple Availability Zones. 


ANSWER : D

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## Your company manages an application that currently allows users to upload images to an S3 bucket. These images are picked up by EC2 Instances for processing and then placed in another S3 bucket. You need an area where the metadata for these images can be stored. What would be an ideal data store for this? [WL226]

A. AWS Redshift
B. AWS Glacier
C. AWS DynamoDB
D. AWS SQS


ANSWER : C

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## An application team needs to quickly provision a development environment consisting of a web and database layer. What would be the quickest and most ideal way to get this set up in place? [WL227]

A. Create Spot Instances and install the web and database components.
B. Create Reserved Instances and install the web and database components.
C. Use AWS Lambda to create the web components and AWS RDS for the database layer.
D. Use Elastic Beanstalk to quickly provision the environment. 


ANSWER : D

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## A company has an application that stores images and thumbnails on S3. The thumbnail needs to be available for download immediately. Additionally, both the images and thumbnails are not accessed frequently. What would be the cost-efficient storage option that meets the above-mentioned requirements? [WL229]

A. Amazon Glacier with Expedited Retrievals.
B. Amazon S3 Standard Infrequent Access
C. Amazon EFS
D. Amazon S3 Standard


ANSWER : B

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## You have an application hosted on AWS consisting of EC2 Instances launched via an Auto Scaling Group. You notice that the EC2 Instances are not scaling out on demand. Which checks should be done to ensure that the scaling occurs as expected? [WL231] (Select 2)


A. Ensure that the right metrics are being used to trigger the scale-out. 
B. Check your scaling policies to see whether more than one policy is triggered by an event. 
C. Ensure that AutoScaling health checks are being used. 
D. Ensure that you are using Load Balancers. 


ANSWER : 

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## A company hosts a popular web application that connects to an Amazon RDS MySQL DB instance, running in a private VPC subnet created with default ACL settings. The web servers must be accessible only to customers on an SSL connection and the database should only be accessible to web servers in a public subnet. As an architect, what would you not recommend for such an architecture? [WL232]

A. Create a separate web server and database server security group.
B. Ensure that the web server security group allows HTTPS port 443 inbound traffic from anywhere (0.0.0.0/0)
C. Ensure that the web server security group allows MySQL port 3306 inbound traffic from anywhere (0.0.0.0/0)
D. Ensure that the DB server Security Group allows MySQL port 3306 inbound traffic from Web Server Security Group

## Your company has designed an app and requires it to store data in DynamoDB. The company has registered the app with identity providers so users can sign-in using third-parties like Google and Facebook. What must be in place such that the app can obtain temporary credentials to access DynamoDB? [WL233]

]                                         
A. Multi-factor authentication must be used to access DynamoDB
B. AWS CloudTrail needs to be enabled to audit usage
C. An IAM role allowing the app to have access to DynamoDB
D. The user must additionally log into the AWS console to gain database access


ANSWER : 

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## A company has an entire infrastructure hosted on AWS. It requires to create code templates used to provision the same set of resources in another region in case of a disaster in the primary region. Which AWS service can be helpful in this regard? [WL234]

]                                         
A. AWS Beanstalk
B. AWS CloudFormation
C. AWS CodeBuild
D. AWS CodeDeploy


ANSWER : 

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## You are working as an AWS Architect for a start-up company. The company has web-servers deployed in all AZ’s in eu-central-1 (Frankfurt) region. These web servers serve German news & local web content for people accessing these websites within Germany. These web servers have multiple records created for a single domain. Company is looking for a random selection of web-servers that will increase the availability. What would be the most appropriate routing policy for this requirement? [WL235]

A. Latency routing policy
B. Weighted routing policy
C. Multivalue answer routing policy
D. Geolocation routing policy


ANSWER : 

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## Your recent security review revealed a large spike in attempted logins to your AWS account. With respect to sensitive data stored in encryption enabled S3, the data has not been encrypted and is susceptible to fraud if it was to be stolen. You’ve recommended AWS Key Management Service as a solution. Which of the following is true regarding the operation of KMS? [WL236]
                                         
A. Only KMS generated keys can be used to encrypt or decrypt data
B. Data is encrypted at rest
C. KMS allows all users and roles to use the keys by default
D. Data is decrypted in transit


ANSWER : 

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## Your company has a set of EC2 Instances hosted in AWS. It is mandatory to prepare for disasters and come up with the necessary disaster recovery procedures. What would be helpful in mitigating the effects of a disaster for the EC2 Instances? [WL2]

]                                         
A. Place an ELB in front of the EC2 Instances.
B. Use Auto Scaling to ensure that the minimum number of instances are always running.
C. Use CloudFront in front of the EC2 Instances.
D. Use AMIs to recreate the EC2 Instances in another region. 

ANSWER : 

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## A company currently hosts a Redshift cluster in AWS. For security reasons, it should ensure that all traffic from and to the Redshift cluster does not go through the Internet. Which features can be used to fulfill this requirement in an efficient manner? [WL2]

]                                         
A. Enable Amazon Redshift Enhanced VPC Routing.
B. Create a NAT Gateway to route the traffic.
C. Create a NAT Instance to route the traffic.
D. Create a VPN Connection to ensure traffic does not flow through the Internet. 

ANSWER : 

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## A company has a set of Hyper-V machines and VMware virtual machines. They are now planning to migrate these resources to the AWS Cloud. What should they use to move these resources to the AWS Cloud? [WL239]

]                                         
A. DB Migration utility
B. AWS Server Migration Service
C. Use AWS Migration Tools.
D. Use AWS Config Tools. 

ANSWER : 

EXPLANATION: 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
## You’ve implemented AWS Key Management Service to protect your data in your applications and other AWS services. Your global headquarters is in Northern Virginia (US East (N. Virginia)) where you created your keys and have provided the appropriate permissions to designated users and specific roles within your organization. While the N. American users are not having issues, German and Japanese users are unable to get KMS to function. What is the most likely cause of it? [WL240]

]                                         
A. KMS is only offered in North America
B. AWS CloudTrail has not been enabled to log events
C. KMS master keys are region-specific and the applications are hitting the wrong API endpoints
D. The master keys have been disabled

Explanation:
Correct Answer: C

Option C is correct. This is the most likely cause as the application should be sure to hit correct region endpoint. Option A is incorrect. KMS is offered in several regions but keys are not transferrable out of the region they were created in. Option B is incorrect. CloudTrail is recommended for auditing but is not required
Option D is incorrect. The keys are working as expected where they were created; keys are region-specific
References:

https://aws.amazon.com/kms/faqs/
ttps://docs.aws.amazon.com/general/latest/gr/rande.html#kms_region
https://www.slideshare.net/AmazonWebServices/encryption-and-key-management-in-aws
Ask our Experts
Rate this Question? [WL2]
                     
Question 41
Unattempted                             
                         
Domain :Design Cost-Optimized Architectures
A company with a set of Admin jobs (.NET core) currently set up in the C# programming language, is moving its infrastructure to AWS. What would be an efficient mean of hosting the Admin related jobs in AWS? [WL241]

]                                         
A. Use AWS DynamoDB to store the jobs and then run them on demand.
B. Use AWS Lambda functions with C# for the Admin jobs.
C. Use AWS S3 to store the jobs and then run them on demand.
D. Use AWS Config functions with C# for the Admin jobs. 
Explanation:
Correct Answer - B

The best and most efficient option is to host the jobs using AWS Lambda. This service has the facility to have the code run in the C# programming language. 
AWS Documentation mentions the following on AWS Lambda:

AWS Lambda is a compute service that lets you run code without provisioning or managing servers. AWS Lambda executes your code only when needed and scales automatically, from a few requests per day to thousands per second. You pay only for the compute time you consume - there is no charge when your code is not running. With AWS Lambda, you can run code virtually for any type of application or backend service - all with zero administration. 
 

For more information on AWS Lambda, please visit the following URL:

https://docs.aws.amazon.com/lambda/latest/dg/welcome.html
Try now labs related to this question
Introduction to Amazon Lambda
This lab walks you through creation and usage of AWS Serverless service called AWS Lambda. In this lab, we will create a sample lambda function which is triggered on S3 Object upload event and makes a copy of that object on another S3 Bucket. 
Credit Needed10
Time 0 : 30                                                                                                        
Try Now
                                         
Ask our Experts
Rate this Question? [WL2]
                     
Question 42
Unattempted                             
                         
Domain :Design Cost-Optimized Architectures
Your company has a set of resources hosted on the AWS Cloud. As part of the new governing model, there is a requirement that all API activity on AWS resources should be monitored. What is the most efficient way to have this implemented? [WL242]

]                                         
A. Use VPC Flow Logs to monitor all activity in your VPC.
B. Use AWS Trusted Advisor to monitor all of your AWS resources.
C. Use AWS Inspector to inspect all of the resources in your account.
D. Use AWS CloudTrail to monitor all API activity. 
Explanation:
Answer – D

AWS Documentation mentions the following on AWS CloudTrail:

AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting. 
Visibility into your AWS account activity is a key aspect of security and operational best practices. You can use CloudTrail to view, search, download, archive, analyze, and respond to account activity across your AWS infrastructure. You can identify who or what took which action, what resources were acted upon, when the event occurred, and other details to help you analyze and respond to activity in your AWS account. 
You can integrate CloudTrail into applications using the API, automate trail creation for your organization, check the status of trails you create, and control how users view CloudTrail events. 
More information is available at the below URLs:
https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html
https://aws.amazon.com/cloudtrail/
Ask our Experts
Rate this Question? [WL2]
                     
Question 43
Unattempted                             
                         
Domain :Design Resilient Architectures
You are planning to use Docker containers on a cluster of EC2 instances. These EC2 instances will be launched in a VPC and will require access to ECR and S3 to download Docker images and other images respectively. Additionally, the EC2 instances require secure connectivity to the ECS control plane. 
You have created public and private subnets to launch the EC2 instances. What would be helpful to enable secure connectivity and ensure all container orchestration traffic stays within the VPC? [WL243] (SELECT TWO)


A. Use AWS PrivateLink to connect to the Amazon S3 buckets for downloading images. 
 
B. For the instances in the public subnets, use Internet Gateway to access Amazon ECS, ECR, and S3 buckets. 
 
C. Use a Gateway VPC Endpoint to download images from the S3 bucket. 
 
D. Use AWS PrivateLink to connect to Amazon ECS for control plane connectivity and ECR for downloading Docker images. 
 
E. For the instances in the private subnets, use NAT to access Amazon ECS, ECR, and S3. 
 
F. Use a Gateway VPC Endpoint to connect to Amazon ECS for control plane connectivity and ECR for downloading Docker images. 
Explanation:
Correct Answer –  C and D

Gateway VPC Endpoint provides secure private access to Amazon S3 and DynamoDB without traffic routing via the Internet. When Gateway Endpoints are created, VPC Endpoint is created along with the addition of S3 prefixes in the routing table, pointing to VPCE. 
AWS PrivateLink provides secure private access to various AWS services by adding an Elastic Network Interface within a VPC. AWS creates a local/ regional DNS entry which resolves to the local IP address assigned to ENI. 
Option A is incorrect as AWS PrivateLink does not support access to Amazon S3. Amazon S3 can be accessed privately from a VPC via Gateway VPC Endpoint. Options B and E are incorrect as with this, the Traffic from EC2 instance to ECS, ECR, and Amazon S3  will be flowing over the Internet. Option F is incorrect as Gateway VPC Endpoint does not support access to Amazon ECR; it supports private access only to Amazon S3 & Amazon DynamoDB.  
For more information on VPC, Gateway VPC Endpoints, and AWS PrivateLink, refer to the following URLs:

https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html
https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html
https://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html
Ask our Experts
Rate this Question? [WL2]
                     
Question 44
Unattempted                             
                         
Domain :Design Cost-Optimized Architectures
A company currently uses Redshift in AWS. The Redshift cluster is required to be used in a cost-effective manner. As an architect, what would you consider to ensure cost-effectiveness? [WL244]

]                                         
A. Use Spot Instances for the underlying nodes in the cluster.
B. Ensure that unnecessary manual snapshots of the cluster are deleted.
C. Ensure that VPC Enhanced Routing is enabled.
D. Ensure that CloudWatch metrics are disabled. 
Explanation:
Correct Answer - B

AWS Documentation mentions the following:

Amazon Redshift provides free storage for snapshots that is equal to the storage capacity of your cluster until you delete the cluster. After you reach the free snapshot storage limit, you are charged for any additional storage at the normal rate. Because of this, you should evaluate how many days you need to keep automated snapshots and configure their retention period accordingly and delete any manual snapshots that you no longer need. 
For more information on working with Redshift Snapshots, please visit the following URL:

https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html


Note: 

Redshift pricing is based on the following elements. 
Compute node hours 
Backup Storage
Data transfer – There is no data transfer charge for data transferred to or from Amazon Redshift and Amazon S3 within the same AWS Region. For all other data transfers into and out of Amazon Redshift, you will be billed at standard AWS data transfer rates. Data scanned 

There is no additional charge for using Enhanced VPC Routing. You might incur additional data transfer charges for certain operations, such as UNLOAD to Amazon S3 in a different region or COPY from Amazon EMR or SSH with public IP addresses. 

Enhanced VPC routing does not incur any cost but any Unload operation to a different region will incur a cost. With Enhanced VPC routing or without it, any data transfer to a different region incurs the cost. 
But with Storage, increasing your backup retention period or taking additional snapshots increases the backup storage consumed by your data warehouse. There is no additional charge for backup storage up to 100% of your provisioned storage for an active data warehouse cluster. Any amount of storage exceeding this limit incurs the cost. 
For Redshift, spot Instances is not an option

Amazon Redshift pricing options include:

On-Demand pricing: no upfront costs - you simply pay an hourly rate based on the type and number of nodes in your cluster. Amazon Redshift Spectrum pricing: enables you to run SQL queries directly against all of your data, out to exabytes. In Amazon S3 - you simply pay for the number of bytes scanned. Reserved Instance pricing: enables you to save up to 75% over On-Demand rates by committing to using Redshift for a 1 or 3-year term. Ask our Experts
Rate this Question? [WL2]
                     
Question 45
Unattempted                             
                         
Domain :Specify Secure Applications and Architectures
A company has a set of resources hosted in an AWS VPC. Having acquired another company with its own set of resources hosted in AWS, it is required to ensure that resources in the VPC of the parent company can access the resources in the VPC of the child company. How could this be accomplished? [WL245]

]                                         
A. Establish a NAT Instance to establish communication across VPCs.
B. Establish a NAT Gateway to establish communication across VPCs.
C. Use a VPN Connection to peer both VPCs.
D. Use VPC Peering to peer both VPCs. 
Explanation:
Correct Answer - D

AWS Documentation mentions the following about VPC Peering:

A VPC Peering Connection is a networking connection between two VPCs that enables you to route traffic between them privately. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC Peering Connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS region. 
For more information on VPC Peering, please visit the following URL:

https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html

 

NAT Instance, NAT Gateway, and VPN do not allow VPC-VPC connectivity. 
Ask our Experts
Rate this Question? [WL2]
                     
Question 46
Unattempted                             
                         
Domain :Design Resilient Architectures
An application consists of the following architecture:

a. EC2 Instances in a single AZ behind an ELB

b. A NAT Instance which is used to ensure that instances can download updates from the Internet

What could be done to ensure better fault tolerance in this set up? [WL246] (SELECT TWO)

 


A. Add more instances in the existing Availability Zone. 
 
B. Add an Auto Scaling Group to the setup. 
 
C. Add more instances in another Availability Zone. 
 
D. Add another ELB for more fault tolerance. 
Explanation:
Correct Answer – B and C

AWS Documentation mentions the following:

Adding Auto Scaling to your application architecture is one way to maximize the benefits of the AWS Cloud. When you use Auto Scaling, your applications gain the following benefits:

Better fault tolerance. Auto Scaling can detect when an instance is unhealthy. Then it terminates that instance, and launches an instance to replace it. You can also configure Auto Scaling to use multiple Availability Zones. If one Availability Zone becomes unavailable, Auto Scaling can launch instances in another one to compensate. Better availability. Auto Scaling can help you ensure that your application always has the right amount of capacity to handle the current traffic demands.  

For more information on the benefits of Auto Scaling, please visit the following URL:

https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html
 

Try now labs related to this question
Build Amazon VPC with Public and Private Subnets from Scratch
Learn how to build Public and Private subnets from scratch. 
VPC wizard will not be used. So every component required to build public and private subnets will be created and configured manually. 
This will give an in-depth understanding of internal components of VPC and subnets. 
Credit Needed10
Time 0 : 30                                                                                                        
Try Now
                                         
Ask our Experts
Rate this Question? [WL2]
                     
Question 47
Unattempted                             
                         
Domain :Define Performant Architectures
A company has a lot of data hosted on their On-premises infrastructure. Running out of storage space, the company wants a quick win solution using AWS. Which of the following would allow easy extension of their data infrastructure to AWS? [WL247]

]                                         
A. The company could start using Gateway Cached Volumes.
B. The company could start using Gateway Stored Volumes.
C. The company could start using the Simple Storage Service.
D. The company could start using Amazon Glacier. 
Explanation:
Correct Answer - A

Volume Gateways and Cached Volumes can be used to start storing data in S3. 
AWS Documentation mentions the following:

You store your data in Amazon Simple Storage Service (Amazon S3) and retain a copy of frequently accessed data subsets locally. Cached volumes offer substantial cost savings on primary storage and minimize the need to scale your storage on-premises. You also retain low-latency access to your frequently accessed data. 
For more information on Storage Gateways, please visit the following URL:

https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html
 

Note: 

The question states that they are running out of storage space and they need a solution to store data with AWS rather than a backup. So for this purpose, gateway-cached volumes are appropriate which will help them to avoid scaling their on-premises data center and allows them to store on AWS storage service while having the most recent files available for them at low latency. 
 

This is the difference between Cached and stored volumes:
 

Cached volumes – You store your data in S3 and retain a copy of frequently accessed data subsets locally. Cached volumes offer substantial cost savings on primary storage and "minimize the need to scale your storage on-premises. You also retain low-latency access to your frequently accessed data."

Stored volumes – If you need low-latency access to your entire data set, first configure your on-premises gateway to store all your data locally. Then asynchronously back up point-in-time snapshots of this data to Amazon S3. "This configuration provides durable and inexpensive off-site backups that you can recover to your local data center or Amazon EC2." For example, if you need replacement capacity for disaster recovery, you can recover the backups to Amazon EC2. 

As described in the answer: The company wants a quick win solution to store data with AWS, avoiding scaling the on-premise setup rather than backing up the data. 

 

In the question, they mentioned that "A company has a lot of data hosted on their On-premises infrastructure." From On-premises to cloud infrastructure, you can use AWS storage gateways. Option C is talking about the data store. But here the requirement is (How) to transfer or migrate your data from On-premises to Cloud infrastructure. So there is no clear process mentioned in Option C. 

Ask our Experts
Rate this Question? [WL2]
                     
Question 48
Unattempted                             
                         
Domain :Design Resilient Architectures
A company has a sales team and each member of this team uploads their sales figures daily. A Solutions Architect needs a durable storage solution for these documents and also a way to preserve documents from accidental deletions. Which of the following choices would deliver protection against unintended user actions? [WL248]

]                                         
A. Store data in an EBS Volume and create snapshots once a week.
B. Store data in an S3 bucket and enable versioning.
C. Store data in two S3 buckets in different AWS regions.
D. Store data on EC2 Instance storage. 
Explanation:
Correct Answer - B

Amazon S3 has an option for versioning as shown below. Versioning is on the bucket level and can be used to recover prior versions of an object. 


 

For more information on Amazon S3, please visit the following URL:

https://aws.amazon.com/s3/
 

Try now labs related to this question
How to enable versioning Amazon S3
This lab walks you through to the steps how to Enables Versioning to a AWS S3 Bucket. Versioning enables you to keep multiple versions of an object in one bucket. In this lab we learn how to enable object versioning on a S3 bucket. 
Credit Needed10
Time 0 : 30                                                                                                        
Try Now
                                         
Ask our Experts
Rate this Question? [WL2]
                     
Question 49
Unattempted                             
                         
Domain :Design Resilient Architectures
An application requires a highly available relational database with an initial storage capacity of 8TB. This database will grow by 8GB every day. To support the expected traffic, at least eight read replicas will be required to handle the database reads. Which AWS service would help to meet these requirements? [WL249]

]                                         
A. DynamoDB
B. Amazon S3
C. Amazon Aurora
D. Amazon Redshift

Explanation:
Correct Answer – C

AWS Documentation mentions the following:

Aurora Replicas

Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region. The DB cluster volume is made up of multiple copies of the data for the DB cluster. However, the data in the cluster volume is represented as a single, logical volume to the primary instance and to Aurora Replicas in the DB cluster. 
As a result, all Aurora Replicas return the same data for query results with minimal replica lag—usually, much less than 100 milliseconds after the primary instance has written an update. Replica lag varies depending on the rate of database change. That is, during periods where a large number of write operations occur for the database, you might see an increase in replica lag. 
 

Aurora Replicas work well for read scaling because they are fully dedicated to read operations on your cluster volume. Write operations are managed by the primary instance. Because the cluster volume is shared among all DB instances in your DB cluster, minimal additional work is required to replicate a copy of the data for each Aurora Replica. 
To increase availability, you can use Aurora Replicas as failover targets. That is, if the primary instance fails, an Aurora Replica is promoted to the primary instance. There is a brief interruption during which read and write requests made to the primary instance fail with an exception, and the Aurora Replicas are rebooted. If your Aurora DB cluster doesn't include any Aurora Replicas, then your DB cluster will be unavailable for the duration it takes your DB instance to recover from the failure event. However, promoting an Aurora Replica is much faster than recreating the primary instance. For high-availability scenarios, we recommend that you create one or more Aurora Replicas. These should be of the same DB instance class as the primary instance and in different Availability Zones for your Aurora DB cluster. 
For more information on Aurora Replicas as failover targets, see Fault Tolerance for an Aurora DB Cluster. 
Note:

You can't create an encrypted Aurora Replica for an unencrypted Aurora DB cluster and vice-versa. 
 

For details on how to create an Aurora Replica, see Adding Aurora Replicas to a DB Cluster. 
 

Replication with Aurora MySQL

In addition to Aurora Replicas, you have the following options for replication with Aurora MySQL:

Two Aurora MySQL DB clusters in different AWS Regions, by creating an Aurora Read Replica of an Aurora MySQL DB cluster in a different AWS Region. 
Two Aurora MySQL DB clusters in the same region, by using MySQL binary log (binlog) replication. 
An Amazon RDS MySQL DB instance as the master and an Aurora MySQL DB cluster, by creating an Aurora Read Replica of an Amazon RDS MySQL DB instance. Typically, this approach is used for migration to Aurora MySQL, rather than for ongoing replication. 
 

For more information on AWS Aurora, please visit the following URL:

https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.Replication.html
 

Try now labs related to this question
Introduction to Amazon Aurora
This lab walks you through the creation and testing of an Amazon Aurora database. We will create an Aurora MySQL Database and and test the connection. 
Credit Needed10
Time 1 : 0                                                                                                        
Try Now
                                         
Ask our Experts
Rate this Question? [WL2]
                     
Question 50
Unattempted                             
                         
Domain :Design Cost-Optimized Architectures
A company has an application that delivers objects from S3 to users. Of late, some users spread across the globe, have been complaining of slow response times. Which additional step would help in building a cost-effective solution and ensure that the users get an optimal response to objects from S3? [WL250]

]                                         
A. Use S3 Replication to replicate the objects to regions closest to the users.
B. Ensure S3 Transfer Acceleration is enabled to ensure that all users get the desired response times.
C. Place an ELB in front of S3 to distribute the load across S3.
D. Place the S3 bucket behind a CloudFront distribution. 
Explanation:
Correct Answer - D

AWS Documentation mentions the following:

If your workload is mainly sending GET requests, in addition to the preceding guidelines, you should consider using Amazon CloudFront for performance optimization. 
Integrating Amazon CloudFront with Amazon S3, you can distribute content to your users with low latency and a high data transfer rate. You will also send fewer direct requests to Amazon S3, which will reduce your costs. 
For example, suppose that you have a few objects that are very popular. Amazon CloudFront fetches those objects from Amazon S3 and caches them. Amazon CloudFront can then serve future requests for the objects from its cache, reducing the number of GET requests it sends to Amazon S3. 
 

For more information on performance considerations in S3, please visit the following URL:

https://docs.aws.amazon.com/AmazonS3/latest/dev/request-rate-perf-considerations.html
 

Option A is incorrect. S3 Cross-Region Replication is not the correct answer for this business scenario.  You are asked on how to provide easier & faster access to data in S3 bucket, and this option is used to replicate S3 bucket data across regions. Option B is incorrect. S3 TA is used for fast, easy, and secure file transfer over long distances between your client and your Amazon S3 bucket. S3 Transfer Acceleration does leverage Amazon CloudFront’s globally distributed AWS Edge Locations, but would be too costly for this situation. Option C is incorrect. ELB is used to distribute traffic on to EC2 Instances. Try now labs related to this question
Introduction to Amazon CloudFront
This lab walks you through to Amazon CloudFront creation and working. In this lab you will create an Amazon CloudFront distribution. It will distribute a publicly accessible image file stored in an Amazon S3 bucket. 
Credit Needed10
Time 1 : 30                                                                                                        
Try Now
                                         
Ask our Experts
Rate this Question? [WL2]
                     
Question 51
Unattempted                             
                         
Domain :Design Resilient Architectures
An application needs to have a messaging system in AWS. It is of the utmost importance that the order of messages is preserved and duplicate messages are not sent. Which of the following services can help fulfill this requirement? [WL251]

]                                         
A. AWS SQS FIFO
B. AWS SNS
C. AWS Config
D. AWS ELB

Explanation:
Correct Answer – A

One can use SQS FIFO queues for this purpose. 

AWS Documentation mentions the following on SQS FIFO Queues:

Amazon SQS is a reliable and highly-scalable managed message queue service for storing messages in transit between application components. FIFO queues complement the existing Amazon SQS standard queues, which offer high throughput, best-effort ordering, and at-least-once delivery. FIFO queues have essentially the same features as standard queues, but provide the added benefits of supporting ordering and exactly-once processing. FIFO queues provide additional features that help prevent unintentional duplicates from being sent by message producers or from being received by message consumers. Additionally, message groups allow multiple separate ordered message streams within the same queue.  

For more information on SQS FIFO Queues, please visit the following URL:

https://aws.amazon.com/about-aws/whats-new/2016/11/amazon-sqs-introduces-fifo-queues-with-exactly-once-processing-and-lower-prices-for-standard-queues/


Note:

 

As per AWS, SQS FIFO queues will ensure the delivery of the message only once and it will be delivered in sequential order. (i.e. First in First Out) whereas SNS cannot guarantee the delivery of the message only once. 
Read the following AWS SNS FAQ,


Q: How many times will a subscriber receive each message? [WL2]
Although most of the time each message will be delivered to your application exactly once, the distributed nature of Amazon SNS and transient network conditions could result in occasional, duplicate messages at the subscriber end. Developers should design their applications such that processing a message more than once does not create any errors or inconsistencies. 
FIFO FAQs states that


Using SQS FIFO queues will satisfy both the requirements stated in the question. i.e. Duplication of the message will not occur and the order of messages will be preserved.  

Ask our Experts
Rate this Question? [WL2]
                     
Question 52
Unattempted                             
                         
Domain :Define Performant Architectures
A company is planning to build an application using the services available on AWS. This application will be stateless in nature, and the service must have the ability to scale according to the demand. Which compute service should be used in this scenario? [WL252]

]                                         
A. AWS DynamoDB
B. AWS Lambda
C. AWS S3
D. AWS SQS

Explanation:
Correct Answer - B

The following content from an AWS Whitepaper supports the usage of AWS Lambda for this requirement:

A stateless application is an application that needs no knowledge of previous interactions and stores no session information. Such an example could be an application that, given the same input, provides the same response to any end-user. A stateless application can scale horizontally since any request can be serviced by any of the available compute resources (e.g., EC2 instances, AWS Lambda functions). 
For more information on AWS Cloud best practices, please visit the following URL:

https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf

Try now labs related to this question
Introduction to Amazon Lambda
This lab walks you through creation and usage of AWS Serverless service called AWS Lambda. In this lab, we will create a sample lambda function which is triggered on S3 Object upload event and makes a copy of that object on another S3 Bucket. 
Credit Needed10
Time 0 : 30                                                                                                        
Try Now
                                         
Ask our Experts
Rate this Question? [WL2]
                     
Question 53
Unattempted                             
                         
Domain :Design Resilient Architectures
A company has a set of EC2 Instances hosted on the AWS Cloud. These instances form a web server farm which services a web application accessed by users on the Internet. What would help make this architecture more fault-tolerant? [WL253] (SELECT TWO)


A. Ensure that the instances are placed in separate Availability Zones. 
 
B. Ensure that the instances are placed in separate regions. 
 
C. Use an AWS Load Balancer to distribute the traffic. 
 
D. Use Auto Scaling to distribute the traffic. 
Explanation:
Correct Answer – A and C

AWS Documentation mentions the following:

A load balancer distributes incoming application traffic across multiple EC2 Instances in multiple Availability Zones. This increases the fault tolerance of your applications. Elastic Load Balancing detects unhealthy instances and routes traffic only to healthy instances. 
For more information on the AWS Classic Load Balancer, please visit the following URL:

https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/introduction.html

 

Note:

 

Autoscaling will not create an ELB automatically, you will need to manually create it in the same region as the AutoScaling group. Once you create an ELB and attach the load balancer to the autoscaling group, it automatically registers the instances in the group and distributes incoming traffic across the instances. 
The following steps provide you information on attaching a load balancer to autoscaling group. 

 

1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/

2. On the navigation pane, under Auto Scaling, choose Auto Scaling Groups. 
3. Select your group. 
4. On the Details tab, choose Edit. 
5. Do one of the following:

a. [Classic Load Balancers] For Load Balancers, select your load balancer. 
b.[Target groups] For Target Groups, select your target group. 
6. Choose Save. 


As per AWS,

You can automatically increase the size of your Auto Scaling group when demand goes up and decrease it when demand goes down. As the Auto Scaling group adds and removes EC2 instances, you must ensure that the traffic for your application is distributed across all of your EC2 instances. The Elastic Load Balancing service automatically routes incoming web traffic across such a dynamically changing number of EC2 instances. Your load balancer acts as a single point of contact for all incoming traffic to the instances in your Auto Scaling group.   

To use a load balancer with your Auto Scaling group, create the load balancer and then attach it to the group. 
For more information, please visit the following URLs:


https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html
https://docs.aws.amazon.com/autoscaling/ec2/userguide/attach-load-balancer-asg.html

 

Try now labs related to this question
Introduction to AWS Elastic Load Balancing
This lab walks you through AWS Elastic Load Balancing. Elastic Load Balancing automatically distributes incoming application traffic across multiple Amazon EC2 instances in the cloud. In this lab, we will demonstrate elastic load balancing with 2 EC2 Instances. 
Credit Needed10
Time 0 : 30                                                                                                        
Try Now
                                         
Ask our Experts
Rate this Question? [WL2]
                     
Question 54
Unattempted                             
                         
Domain :Design Cost-Optimized Architectures
You plan on hosting an application on EC2 Instances which will be used to process logs. The application is not very critical and can resume operation even after an interruption. Which of the following steps could provide a cost-effective solution? [WL254]

]                                         
A. Use Reserved Instances for the underlying EC2 Instances.
B. Use Provisioned IOPS for the underlying EBS Volumes.
C. Use Spot Instances for the underlying EC2 Instances.
D. Use S3 as the underlying data layer. 
Explanation:
Correct Answer – C

One effective solution would be to use Spot Instances in this scenario. 
AWS Documentation mentions the following on Spot Instances:

Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted. For example, Spot Instances are well-suited for data analysis, batch jobs, background processing, and optional tasks. 
For more information on using Spot Instances, please visit the following URL:

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html

 

 

Ask our Experts
Rate this Question? [WL2]
                     
Question 55
Unattempted                             
                         
Domain :Design Resilient Architectures
A company stores its log data in an S3 bucket. There is a current need to have search capabilities available for the data in S3. What could be helpful to achieve this in an efficient manner? [WL255] (SELECT TWO )


A. Use AWS Athena to query the S3 bucket. 
 
B. Create a Lifecycle Policy for the S3 bucket. 
 
C. Load the data into Amazon Elasticsearch. 
 
D. Load the data into Glacier. 
Explanation:
Correct Answer – A and C

Amazon Athena is a service that enables a data analyst to perform interactive queries in the AWS public cloud on data stored in AWS S3.  Since it's a serverless query service, an analyst doesn't need to manage any underlying compute infrastructure to use it. 
For more information on Amazon Athena, please refer to the following URLs:

https://aws.amazon.com/athena/
https://aws.amazon.com/blogs/aws/amazon-athena-interactive-sql-queries-for-data-in-amazon-s3/
Elasticsearch is a highly scalable open-source full-text search and analytics engine. It allows you to store, search, and analyze big volumes of data quickly and in near real time. It is generally used as the underlying engine/technology that powers applications that have complex search features and requirements. 
https://aws.amazon.com/blogs/database/use-amazon-s3-to-store-a-single-amazon-elasticsearch-service-index/

https://aws.amazon.com/blogs/database/analyze-url-paths-to-search-individual-elements-in-amazon-elasticsearch-service/

Ask our Experts
Rate this Question? [WL2]
                     
Question 56
Unattempted                             
                         
Domain :Define Operationally-Excellent Architectures
A company plans to deploy a batch processing application in AWS. Which of the followings would ideally help to host this application? [WL256] (SELECT TWO)


A. Copy the batch processing application to an ECS Container. 
 
B. Create a docker image of your batch processing application. 
 
C. Deploy the image as an Amazon ECS task. 
 
D. Deploy the container behind the ELB. 
Explanation:
Correct Answer – B and C

AWS Documentation mentions the following:

Docker containers are particularly suited for batch job workloads. Batch jobs are often short-lived and embarrassingly parallel. You can package your batch processing application into a Docker image so that you can deploy it anywhere, such as in an Amazon ECS task. 
For more information on the use cases for AWS ECS, please visit the following URL:

https://docs.aws.amazon.com/AmazonECS/latest/developerguide/common_use_cases.html

 

Ask our Experts
Rate this Question? [WL2]
                     
Question 57
Unattempted                             
                         
Domain :Design Resilient Architectures
An architecture consists of the following:

a) An active/passive infrastructure hosted in AWS

b) Both infrastructures comprise of ELB, Auto Scaling, and EC2 resources

How should Route 53 be configured to ensure proper failover in case the primary infrastructure goes down? [WL257]

]                                         
A. Configure a primary routing policy.
B. Configure a weighted routing policy.
C. Configure a Multi-Answer routing policy.
D. Configure a failover routing policy. 
Explanation:
Correct Answer - D

AWS Documentation mentions the following:

You can create an active-passive failover configuration by using failover records. Create a primary and a secondary failover record that has the same name and type, and associate a health check with each. 

The various Route 53 routing policies are as follows:

 

Simple routing policy – Used for a single resource that performs a given function for your domain, for example, a web server that serves content for the example.com website. Failover routing policy – Used when you want to configure active-passive failover. Geolocation routing policy – Used when you want to route traffic based on the location of your users. Geoproximity routing policy – Used when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another. Latency routing policy – Used when you have resources in multiple locations and you want to route traffic to the resource that provides the best latency. Multivalue answer routing policy – Used when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random. Weighted routing policy – Used to route traffic to multiple resources in proportions that you specify.  

For more information on DNS Failover using Route 53, please visit the following URL:

https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring-options.html
Ask our Experts
Rate this Question? [WL2]
                     
Question 58
Unattempted                             
                         
Domain :Specify Secure Applications and Architectures
Your company uses KMS to fully manage the master keys and performing encryption and decryption operations on your data and in your applications.  As an additional level of security, you now recommend AWS rotate your keys. What would happen after enabling this additional feature? [WL258]

]                                         
A. KMS will manage all encrypt/decrypt actions using the appropriate keys
B. Your company must instruct KMS to re-encrypt all data in all services each time a new key is created
C. You have 30 days to delete old keys after a new one is rotated in
D. Your company must create its own keys and import them to KMS to enable key rotation

Explanation:
Correct Answer: A

Option A is correct. KMS will rotate keys annually and use the appropriate keys to perform cryptographic operations. Option B is incorrect. This is not necessary. KMS, as a managed service, will keep old keys and perform operations based on the appropriate key
Option C is incorrect. This is not a requirement of KMS. Option D is incorrect. This is not a requirement of KMS
 

References:

https://aws.amazon.com/kms/faqs/
ttps://docs.aws.amazon.com/general/latest/gr/rande.html#kms_region
https://www.slideshare.net/AmazonWebServices/encryption-and-key-management-in-aws
Ask our Experts
Rate this Question? [WL2]
                     
Question 59
Unattempted                             
                         
Domain :Specify Secure Applications and Architectures
You are a Solutions Architect in a startup company that is releasing the first iteration of its app. Your company doesn’t have a directory service for its intended users but wants the users to be able to sign in and use the app. What would you advice to implement a solution quickly? [WL259]

]                                         
A. Use AWS Cognito although it only supports social identity providers like Facebook
B. Let each user create an AWS user account to be managed via IAM
C. Invest heavily in Microsoft Active Directory as it’s the industry standard
D. Use Cognito Identity along with a User Pool to securely save users’ profile attributes

Explanation:
Correct Answer: D

Option D is correct. Cognito is a managed service that can be used for this app and scale quickly as usage grows. Option A is incorrect. Cognitio supports more than just social identity providers, including OIDC, SAML, and its own identity pools
Option B is incorrect. This isn’t an efficient means of managing user authentication. Option C is incorrect. This isn’t the most efficient means to authenticate and save user information.  

References:

https://aws.amazon.com/cognito/
http://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html
https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html
https://aws.amazon.com/cognito/getting-started/
https://docs.aws.amazon.com/cognito/latest/developerguide/concepts.html
Ask our Experts
Rate this Question? [WL2]
                     
Question 60
Unattempted                             
                         
Domain :Define Performant Architectures
A company is migrating an on-premises 5TB MySQL database to AWS and expects its database size to increase steadily. Which Amazon RDS engine would meet these requirements? [WL260]

 

 

 

]                                         
A. MySQL
B. Microsoft SQL Server
C. Oracle
D. Amazon Aurora

Explanation:
Correct Answer – D

AWS Documentation supports the above requirements with regard to AWS Aurora. 
Amazon Aurora (Aurora) is a fully managed, MySQL and PostgreSQL compatible, relational database engine. It combines the speed and reliability of high-end commercial databases with the simplicity and cost-effectiveness of open-source databases. It delivers up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications. 
All Aurora Replicas return the same data for query results with minimal replica lag—usually, much lesser than 100 milliseconds after the primary instance has written an update. 
For more information on AWS Aurora, please visit the following URL:

http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.Overview.html

NOTE:

On a MySQL DB instance, avoid tables in your database growing too large. Provisioned storage limits restrict the maximum size of a MySQL table file to 16 TB

However, based on database usage, your Amazon Aurora storage will automatically grow, from the minimum of 10 GB up to 64 TB, in 10 GB increments, with no impact on database performance. 
Hence, the best answer would be option D. 

Try now labs related to this question
Introduction to Amazon Aurora
This lab walks you through the creation and testing of an Amazon Aurora database. We will create an Aurora MySQL Database and and test the connection. 
Credit Needed10
Time 1 : 0                                                                                                        
Try Now
                                         
Ask our Experts
Rate this Question? [WL2]
                     
Question 61
Unattempted                             
                         
Domain :Specify Secure Applications and Architectures
You have implemented AWS Cognito services to require users to sign in and sign up to your app through social identity providers like Facebook, Google, etc. Your marketing department wants users to try out the app anonymously as they think that the current log-in requirement is excessive and will reduce demand for products and services offered through the app. What would you suggest to the marketing department in this regard? [WL261]

]                                         
A. It’s too much of a security risk to allow unauthenticated users access to the app
B. Cognito Identity supports guest users for the ability to enter the app and have limited access
C. A second version of the app will need to be offered for unauthenticated users
D. This is possible only if we remove the authentication from everywhere

Explanation:
Correct Answer - B

Option B is correct. Amazon Cognito Identity Pools can support unauthenticated identities by providing a unique identifier and AWS credentials for users who do not authenticate with an identity provider. Unauthenticated users can be associated with a role that has limited access to resources as compared to a role for authenticated users. Option A is incorrect. Cognito will allow unauthenticated users without being a security risk. Option C is incorrect. Cognito supports both authenticated and unauthenticated users.  

References:

https://aws.amazon.com/cognito/
http://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html
https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html
https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html
https://aws.amazon.com/cognito/getting-started/
https://docs.aws.amazon.com/cognito/latest/developerguide/concepts.html
 

Ask our Experts
Rate this Question? [WL2]
                     
Question 62
Unattempted                             
                         
Domain :Specify Secure Applications and Architectures
Your app uses AWS Cognito Identity for authentication and stores user profiles in a User Pool. To expand the availability and ease of signing in to the app, your team is requesting advice on allowing the use of OpenID Connect (OIDC) identity providers as additional means of authenticating users and saving the user profile information. What is your recommendation on OIDC identity providers? [WL262]

]                                         
A. This is supported, along with social and SAML based identity providers.
B. This is not supported, only social identity providers can be integrated into User Pools
C. If you want OIDC identity providers, then you must include SAML and social-based support as well
D. It’s too much effort to add non-Cognito authenticated user information to a User Pool

Explanation:
Correct Answer - A

Option A is correct. OpenID Connect (OIDC) identity providers (IdPs) (like Salesforce or Ping Identity) are supported in Cognito, along with social and SAML based identity providers. You can add an OIDC IdP to your user pool in the AWS Management Console, with the AWS CLI, or by using the user pool API method CreateIdentityProvider. Option B is incorrect. Cognito supports more than just social identity providers, including OIDC, SAML, and its own identity pools. Option C is incorrect. You can add any combination of federated types, you don’t have to add them all. Option D is incorrect. While there is additional coding to develop this, the effort is most likely not too great to add the feature.  

References:

https://aws.amazon.com/cognito/
https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-oidc-idp.html
http://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html
https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html
https://aws.amazon.com/cognito/getting-started/
https://docs.aws.amazon.com/cognito/latest/developerguide/concepts.html
Try now labs related to this question
Introduction to Amazon CloudFront
This lab walks you through to Amazon CloudFront creation and working. In this lab you will create an Amazon CloudFront distribution. It will distribute a publicly accessible image file stored in an Amazon S3 bucket. 
Credit Needed10
Time 1 : 30                                                                                                        
Try Now
                                         
Ask our Experts
Rate this Question? [WL2]
                     
Question 63
Unattempted                             
                         
Domain :Define Performant Architectures
A company is building a two-tier web application to serve dynamic transaction-based content. Which services would you leverage to enable an elastic and scalable Web Tier? [WL263]

]                                         
A. Elastic Load Balancing, Amazon EC2, and Auto Scaling
B. Elastic Load Balancing, Amazon RDS with Multi-AZ, and Amazon S3
C. Amazon RDS with Multi-AZ and Auto Scaling
D. Amazon EC2, Amazon Dynamo DB, and Amazon S3

Explanation:
Correct Answer – A

The question mentions a scalable Web Tier. So Option B, C, and D can be eliminated since they are database related options. 
The below example ( this is a general depiction giving the deployment design of standby architecture having a two tier in them ) shows an Elastic Load Balancer connected to 2 EC2 instances via Auto Scaling. This is an example of an elastic and scalable Web Tier. By scalable, we mean that the Auto Scaling process is able to increase or decrease the number of EC2 Instances as required. 


For more information on the Elastic Load Balancer, please refer to the URL below. 
https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/introduction.html
Try now labs related to this question
Introduction to Amazon Auto Scaling
AWS Auto Scaling will automatically scale resources as needed to align to your selected scaling strategy, This lab walks you through to use Auto Scaling to automatically launch or terminate EC2’s instances based on user defined policies, schedules and health checks. 
Credit Needed10
Time 0 : 55                                                                                                        
Try Now
                                         
Ask our Experts
Rate this Question? [WL2]
                     
Question 64
Unattempted                             
                         
Domain :Specify Secure Applications and Architectures
An instance is launched into a VPC subnet with the network ACL configured to allow all outbound traffic and deny all inbound traffic. The security group of the instance is configured to allow SSH from any IP address. What changes are required to allow SSH access to the instance? [WL264]

]                                         
A. The Outbound Security Group needs to be modified to allow outbound traffic.
B. The Inbound Network ACL needs to be modified to allow inbound traffic
C. Nothing, it can be accessed from any IP address using SSH
D. Both the Outbound Security Group and Outbound Network ACL need to be modified to allow outbound traffic

Explanation:
Correct Answer – B

For an EC2 Instance to allow SSH, you can have the below configurations for the Security and Network ACL for Inbound and Outbound Traffic. 


 

The reason why Network ACL has to have both an Allow for Inbound and Outbound is that network ACLs are stateless. Responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa). Whereas for Security groups, responses are stateful. So if an incoming request is granted, by default an outgoing request will also be granted. 
 

Options A and D are invalid because Security Groups are stateful. Here, any traffic allowed in the Inbound rule is allowed in the Outbound rule too. Option C is also incorrect. 
 

For more information on Network ACLs, please refer to the URL below. 
https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html
Try now labs related to this question
Build Amazon VPC with Public and Private Subnets from Scratch
Learn how to build Public and Private subnets from scratch. 
VPC wizard will not be used. So every component required to build public and private subnets will be created and configured manually. 
This will give an in-depth understanding of internal components of VPC and subnets. 
Credit Needed10
Time 0 : 30                                                                                                        
Try Now
                                         
Ask our Experts
Rate this Question? [WL2]
                     
Question 65
Unattempted                             
                         
Domain :Define Operationally-Excellent Architectures
Your company currently has a web distribution hosted using the AWS CloudFront service. The IT Security department has confirmed that the application using this web distribution now falls under the scope of PCI compliance. What are the possible ways to meet the requirements? [WL265] (SELECT TWO)


A. Enable CloudFront access logs. 
 
B. Enable Cache in CloudFront. 
 
C. Capture requests that are sent to the CloudFront API. 
 
D. Enable VPC Flow Logs

Explanation:
Correct Answer – A and C

AWS Documentation mentions the following:

If you run PCI or HIPAA-compliant workloads based on the AWS Shared Responsibility Model, we recommend that you log your CloudFront usage data for the last 365 days for future auditing purposes. To log usage data, you can do the following:

 

Enable CloudFront access logs. Capture requests that are sent to the CloudFront API.  

For more information on compliance with CloudFront, please visit the following URLs:

https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html
https://aws.amazon.com/blogs/aws/pci-compliance-for-amazon-cloudfront/
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/SERVICENAME-compliance.html
 

Option B is incorrect. It helps to reduce latency. Option D is incorrect. VPC flow logs capture information about the IP traffic going to and from network interfaces in a VPC but not for CloudFront.
